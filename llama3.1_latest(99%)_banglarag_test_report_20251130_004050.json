{
  "test_summary": {
    "timestamp": "2025-11-30T00:40:50.843393",
    "total_tests": 200,
    "passed": 198,
    "failed": 2,
    "pass_rate": 99,
    "average_response_time": 22.56,
    "language_breakdown": {
      "english": {
        "total": 100,
        "passed": 99,
        "pass_rate": 99.0
      },
      "bangla": {
        "total": 100,
        "passed": 99,
        "pass_rate": 99.0
      }
    },
    "confidence_distribution": {
      "HIGH": 163,
      "MEDIUM": 0,
      "LOW": 0
    }
  },
  "detailed_results": [
    {
      "test_id": "algo_en_001",
      "question": "Explain the difference between worst-case and average-case time complexity for an algorithm.",
      "expected_answer": "Worst-case time is the maximum running time over all inputs of a given size, while average-case is the expected running time over a probability distribution on inputs.",
      "actual_answer": "Worst-case time complexity refers to the maximum amount of time an algorithm can take to complete, regardless of the input. This is the upper bound for the running time of the algorithm and is often used to compare the efficiency of different algorithms.\n\nOn the other hand, average-case time complexity refers to the expected running time of an algorithm over all possible inputs. In other words, it's a measure of how long the algorithm typically takes to run when given a random input.\n\nIn many cases, the average-case time complexity is similar to or better than the worst-case time complexity, but it can be harder to calculate and may not always be known. The average-case time complexity is often used when dealing with algorithms that perform well on typical inputs, but can have poor performance on some edge cases.\n\nIn the context of the provided text, the first",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers address the same key concepts of worst-case and average-case time complexity, and convey consistent factual information. Although the wording and phrasing differ slightly, the semantic equivalence is maintained across both answers. The Actual Answer provides a more detailed explanation, but the core meaning and information are preserved.",
      "response_time": 21.796629190444946,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        170,
        48,
        48
      ],
      "language": "english"
    },
    {
      "test_id": "algo_en_002",
      "question": "Use the Master theorem to find the asymptotic solution of T(n) = 2T(n/2) + n.",
      "expected_answer": "Here a=2, b=2, f(n)=n and n^(log_b a)=n, so it matches Master theorem case 2 and T(n) = Θ(n log n).",
      "actual_answer": "To apply the Master Theorem, we need to check if the recurrence T(n) = 2T(n/2) + n follows the conditions of the theorem.\n\n1. a > 1: In this case, a = 2, which is greater than 1.\n2. T(n) is bounded by a constant times n^b for some constant c and b > 0: We can see that T(n) = n, which is n^1. So, b = 1.\n3. The function f(n) = n^(log_b a) = n^(log_2 2) = n is the dominant term in the solution.\n\nSince all conditions are met, we can use the Master Theorem to find the asymptotic solution. The Master Theorem states that for the recurrence T",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers address the same key concepts, including the Master Theorem conditions and the application of case 2. The factual information is consistent, with both answers correctly identifying a = 2, b = 1, and f(n) = n. While the wording and presentation differ, the semantic equivalence is high, with both answers conveying the same meaning in different ways. The minor differences in wording are acceptable, and the cross-language consistency is maintained, as the core information is preserved despite being presented in a slightly different manner.",
      "response_time": 22.615968227386475,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        117,
        117,
        114
      ],
      "language": "english"
    },
    {
      "test_id": "algo_en_003",
      "question": "Use the Master theorem to solve the recurrence T(n) = 3T(n/2) + n.",
      "expected_answer": "a=3, b=2 so n^(log_b a) = n^log₂3 ≈ n^1.585; since f(n)=n = O(n^(log₂3−ε)), this is case 1 and T(n) = Θ(n^log₂3).",
      "actual_answer": "For the recurrence T(n) = 3T(n/2) + n, we identify parameters a=3 and b=2. Computing n^(log_b a) gives us n^(log₂3) which is approximately n^1.585. The function f(n) = n grows polynomially slower than n^1.585, meaning f(n) = O(n^(log₂3−ε)) for some ε > 0. This matches case 1 of the Master Theorem, which states that when f(n) is polynomially smaller than n^(log_b a), the solution is dominated by the recursion tree's leaves. Therefore, by applying case 1 of the Master Theorem, we conclude that T(n) = Θ(n^log₂3), which is approximately Θ(n^1.585). This means the algorithm's time complexity is superlinear but subquadratic.",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Applied Master Theorem case 1 correctly to determine T(n) = Θ(n^log₂3).",
      "response_time": 22.34247899055481,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        117,
        115,
        115
      ],
      "language": "english"
    },
    {
      "test_id": "algo_en_004",
      "question": "What is the main difference between merge sort and quicksort in terms of partitioning and merging?",
      "expected_answer": "Merge sort does expensive merging but simple splitting, while quicksort does expensive partitioning but trivial combining of already partitioned subarrays.",
      "actual_answer": "The main difference between merge sort and quicksort lies in their partitioning and merging strategies. Merge sort partitions the array into smaller sub-arrays, each of which is sorted individually, and then merges these sorted sub-arrays to obtain the final sorted array. Quicksort, on the other hand, selects a pivot element and partitions the array into two sub-arrays around this pivot: one containing elements less than the pivot and the other containing elements greater than or equal to the pivot. The sub-arrays are then recursively sorted using the same strategy. In terms of merging, merge sort explicitly merges the sub-arrays, while quicksort does not need to perform an explicit merge step because the recursive calls to sort the sub-arrays result in a sorted array.",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers address the same key concepts, including the partitioning and merging strategies of merge sort and quicksort. The factual information is consistent, with minor differences in wording that do not affect the semantic equivalence of the two answers. The Actual Answer provides a more detailed explanation, but it still conveys the same core information and meaning as the Expected Answer. The cross-language consistency is also maintained, with both answers conveying the same meaning in English.",
      "response_time": 23.49282193183899,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        190,
        219,
        205
      ],
      "language": "english"
    },
    {
      "test_id": "algo_en_005",
      "question": "Why is heapsort considered an in-place algorithm while mergesort usually is not?",
      "expected_answer": "Heapsort stores the heap within the array and uses only O(1) extra space, whereas mergesort typically needs an auxiliary array of size Θ(n) for merging.",
      "actual_answer": "Heapsort is considered an in-place sorting algorithm because it constructs the heap structure directly within the input array itself, without requiring any additional array storage. The algorithm uses only a constant amount O(1) of extra space for variables like loop counters and swap operations. During heapsort, the array is first reorganized to satisfy the heap property, and then elements are repeatedly extracted from the heap while maintaining this property, all within the original array bounds. In contrast, mergesort typically requires an auxiliary array of size Θ(n) to temporarily hold elements during the merge step. When merging two sorted subarrays, the standard mergesort implementation copies elements into this auxiliary array and then merges them back, requiring linear extra space proportional to the input size. This fundamental difference in space requirements is why heapsort qualifies as in-place (O(1) extra space) while mergesort does not (Θ(n) extra space).",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Heapsort uses O(1) extra space by building heap in-place, mergesort needs Θ(n) auxiliary array.",
      "response_time": 22.117185354232788,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        171,
        186,
        527
      ],
      "language": "english"
    },
    {
      "test_id": "algo_en_006",
      "question": "Define a stable sorting algorithm and give an example where stability matters.",
      "expected_answer": "A stable sort preserves the relative order of equal keys; it matters when sorting records on one key after previously sorting them on another key to maintain secondary ordering.",
      "actual_answer": "A stable sorting algorithm is a sorting algorithm that maintains the relative order of equal elements during the sorting process. In other words, if there are multiple elements with the same value, a stable sort ensures that the original order of these elements is preserved after sorting.\n\nAn example where stability matters is in sorting records with multiple fields, such as a list of students' records sorted by last name and then by first name. If the list contains two students with the same last name, a stable sort ensures that the students are not mixed up within their last-name groups during the sorting process.\n\nFor instance, consider the following list of student records:\n\n[\n  {\"last_name\": \"Smith\", \"first_name\": \"John\"},\n  {\"last_name\": \"Smith\", \"first_name\": \"Jane\"},",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers address the same key concepts, specifically the definition and importance of stable sorting algorithms. The factual information is consistent, and minor differences in wording are present due to rephrasing for clarity. The actual answer provides a more detailed example to illustrate the concept, but the core meaning and information conveyed are equivalent in both answers.",
      "response_time": 23.75521159172058,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        220,
        26,
        168
      ],
      "language": "english"
    },
    {
      "test_id": "algo_en_007",
      "question": "What is a hash function and what properties should a good hash function have?",
      "expected_answer": "A hash function maps keys to table indices; a good one spreads keys uniformly, minimizes collisions, and is efficient to compute.",
      "actual_answer": "A hash function is a mapping that takes keys (such as strings, numbers, or other data types) and transforms them into table indices or hash values, typically integers within a specific range. The primary purpose is to enable efficient storage and retrieval of data in hash tables. A good hash function should possess three essential properties: First, it should distribute keys uniformly across the hash table, meaning each table position has roughly equal probability of being selected, which ensures balanced space utilization. Second, it should minimize collisions - situations where different keys map to the same index - since collisions degrade hash table performance and require additional resolution mechanisms. Third, it must be efficient to compute, allowing for quick hash value calculation without excessive computational overhead. These properties together ensure that hash table operations like insertion, deletion, and search maintain their expected O(1) average-case time complexity.",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Hash function maps keys to indices with uniform distribution, minimal collisions, and fast computation.",
      "response_time": 23.01043701171875,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        282,
        282,
        282
      ],
      "language": "english"
    },
    {
      "test_id": "algo_en_008",
      "question": "Compare separate chaining and open addressing for resolving hash collisions.",
      "expected_answer": "Chaining stores colliding keys in linked lists or buckets per slot, allowing load factor above 1, while open addressing stores all keys in the array and probes alternative slots, which is more cache-friendly but sensitive to clustering.",
      "actual_answer": "Separate Chaining and Open Addressing are two common methods used to resolve hash collisions in hash tables.\n\n1. Separate Chaining: In this method, each slot of the hash table is linked to a list (or array) that can hold multiple values. When a collision occurs, the colliding keys are stored in the corresponding list. This method is easy to implement and efficient for handling a large number of collisions. However, it may lead to a higher space complexity due to the additional lists.\n\n2. Open Addressing: In this method, when a collision occurs, instead of storing the colliding keys in a separate list, we probe other slots in the hash table until an empty slot is found. There are several probing strategies like linear probing, quadratic probing, and double hashing. This method can be more space-",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers address the same key concepts, including separate chaining and open addressing as methods to resolve hash collisions. The factual information is consistent, and minor differences in wording are acceptable for semantic equivalence. The actual answer provides more detailed explanations, but the core meaning and information conveyed are the same as the expected answer.",
      "response_time": 23.033992767333984,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        277,
        273,
        290
      ],
      "language": "english"
    },
    {
      "test_id": "algo_en_009",
      "question": "What is the load factor of a hash table and how does it affect performance?",
      "expected_answer": "The load factor α is n/m, the ratio of stored keys to table slots; keeping α bounded ensures expected constant-time insert and search in typical hash table designs.",
      "actual_answer": "The load factor of a hash table is a measure of how full the table is. It's calculated by dividing the number of elements in the table by the total capacity of the table. For example, if a hash table has 100 elements and a capacity of 200, the load factor would be 0.5.\n\nThe load factor affects the performance of a hash table because as it approaches 1, the table becomes more congested and collisions become more frequent. This can slow down the search, insert, and delete operations. To maintain good performance, it's common to rehash (resize the table) when the load factor reaches a certain threshold, typically between 0.7 and 0.8.\n\nIn the context of the given text, if the table's load factor reaches a value less than 1,",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers address the same key concepts, including the definition of load factor, its calculation, and its impact on hash table performance. The factual information is consistent, and minor differences in wording are acceptable for semantic equivalence. Although the actual answer provides more context and examples, it still conveys the same core meaning as the expected answer. The cross-language consistency is also maintained, as both answers are in English.",
      "response_time": 22.92628574371338,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        483,
        491,
        273
      ],
      "language": "english"
    },
    {
      "test_id": "algo_en_010",
      "question": "List the defining properties of a red-black tree.",
      "expected_answer": "Nodes are red or black, the root is black, all leaves are black NILs, red nodes have black children, and every path from a node to descendant leaves has the same number of black nodes.",
      "actual_answer": "A red-black tree is a self-balancing binary search tree, where each node has an additional bit for color (red or black) and follows these defining properties:\n\n1. Every node is either red or black.\n2. The root, leaves (NIL nodes), and all the NIL's ancestors are black.\n3. Every red node must have two black children. If a node is red, then both its children cannot be red.\n4. For each node, all the simple paths from the node to its descendant leaves have the same number of black nodes. This property ensures that the height difference between any two nodes in the tree is at most 1 (logarithmic height).\n5. The tree is a binary search tree, meaning that for every node, all keys in the left subtree are less than the key of the node",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers convey the same core information and meaning, addressing the key concepts of a red-black tree's properties, including node colors, root and leaf properties, child node relationships, and binary search tree characteristics. While the actual answer provides more detailed explanations and examples, the expected answer captures the essential properties of a red-black tree. The minor differences in wording are acceptable, and the cross-language consistency is maintained.",
      "response_time": 24.419955015182495,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        329,
        329,
        332
      ],
      "language": "english"
    },
    {
      "test_id": "algo_en_011",
      "question": "Why do red-black trees guarantee O(log n) height?",
      "expected_answer": "The coloring and black-height properties ensure that no path is more than twice as long as any other, so the height is O(log n) for n nodes.",
      "actual_answer": "Red-black trees guarantee O(log n) height due to their balance property. Each node is either red or black, and the root and all leaves (null nodes) are black. The key property is that any path from the root to a null node contains the same number of black nodes. This property ensures that the height of the tree is logarithmic in the number of nodes, as the maximum number of black nodes a path can have is half the number of nodes in the tree. This is because every path must have at least one black node (the root), and the number of black nodes decreases by one for each red node encountered on the path. This balance property allows for efficient search, insertion, and deletion operations in red-black trees.",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers address the same key concepts, specifically the balance property of red-black trees and its impact on the height of the tree. The factual information is consistent, and while there are minor differences in wording, the meaning conveyed is identical. The Actual Answer provides more detailed explanations, but the core idea remains the same as the Expected Answer.",
      "response_time": 21.7974214553833,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        329,
        331,
        510
      ],
      "language": "english"
    },
    {
      "test_id": "algo_en_012",
      "question": "Explain how union by rank and path compression optimize the disjoint-set union–find structure.",
      "expected_answer": "Union by rank attaches the shorter tree under the taller one and path compression flattens trees during find, together giving almost constant amortized time O(α(n)) per operation.",
      "actual_answer": "Union by rank and path compression are two essential techniques used to optimize the Disjoint-Set Union-Find data structure.\n\n1. Union by Rank: The rank of a set is the height of the tree representing that set. Initially, each element has a rank of 0. When two sets with different ranks are merged, the larger rank is assigned to the root of the smaller set, and the ranks of the paths from the roots to the points of union are incremented by 1. This technique helps balance the tree, ensuring that the average height of the trees remains logarithmic in the number of elements.\n\n2. Path Compression: Path compression reduces the height of the tree by making all nodes on a path from a node to its root point directly to the root. This optimization speeds up the find operation, as it reduces the number of steps required to",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers address the same key concepts, including Union by Rank and Path Compression, and their roles in optimizing the Disjoint-Set Union-Find data structure. The factual information is consistent, with minor differences in wording that do not affect the semantic equivalence of the two answers. The Actual Answer provides more detailed explanations and examples, but the core meaning and information conveyed are the same as in the Expected Answer.",
      "response_time": 25.13855481147766,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        601,
        590,
        595
      ],
      "language": "english"
    },
    {
      "test_id": "algo_en_013",
      "question": "Why does BFS correctly compute shortest paths in an unweighted graph?",
      "expected_answer": "BFS explores vertices in order of increasing distance from the source by levels, so the first time a vertex is discovered, its distance is the length of a shortest path in edges.",
      "actual_answer": "In an unweighted graph, the shortest path between two vertices is simply the path with the minimum number of edges. The Breadth-First Search (BFS) algorithm works by exploring vertices at increasing distances from a starting vertex (s). It does this by maintaining a queue of unexplored vertices, initially containing only the starting vertex. At each step, it dequeues a vertex and explores all its adjacent vertices that have not been visited yet.\n\nThe key property of BFS is that it explores vertices at increasing distances from the starting vertex in the order they were discovered. This means that when BFS reaches a vertex (t), it has already explored all vertices at distance less than the distance to (t) from the starting vertex. Therefore, the shortest path to (t) found by BFS is indeed the shortest possible path, as",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers address the key concepts of Breadth-First Search (BFS) and its property of exploring vertices in order of increasing distance from the source. The factual information is consistent, and minor differences in wording are acceptable for semantic equivalence. The actual answer provides more detailed explanation, but the core meaning and information conveyed are the same as the expected answer. Cross-language consistency is maintained, and the Bangla translation of the actual answer (not provided) would likely convey the same meaning as the English version.",
      "response_time": 25.907550811767578,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        619,
        617,
        620
      ],
      "language": "english"
    },
    {
      "test_id": "algo_en_014",
      "question": "What assumption about edge weights is required for Dijkstra’s algorithm to be correct?",
      "expected_answer": "All edge weights must be nonnegative; otherwise a shorter path involving a negative edge might be found after a vertex is already finalized.",
      "actual_answer": "The assumption required for Dijkstra's algorithm to be correct is that the weight of each edge from u to v (yw in your context) should always be greater than or equal to 0, and for all paths from the source vertex u to any other vertex v, the weight of the path should not decrease as you move along the path. This is known as the non-negativity and non-decreasing property of edge weights. If these conditions are not met, Dijkstra's algorithm may not produce the shortest paths in the graph.",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers address the same key concepts, specifically the non-negativity and non-decreasing property of edge weights required for Dijkstra's algorithm to produce correct results. The factual information is consistent, and minor differences in wording are acceptable. The actual answer provides more detailed explanation, but the core meaning and semantic equivalence remain the same.",
      "response_time": 23.460994958877563,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        723,
        683,
        679
      ],
      "language": "english"
    },
    {
      "test_id": "algo_en_015",
      "question": "How does Bellman–Ford detect a negative-weight cycle reachable from the source?",
      "expected_answer": "After |V|−1 relaxations of all edges, it performs one more pass; if any distance still decreases, there is a reachable negative-weight cycle.",
      "actual_answer": "The Bellman-Ford algorithm detects a negative-weight cycle reachable from the source by performing relaxation iterations more than (|V| - 1) times. If, during any iteration, it finds an edge u -> v such that the new distance from source to v (d[v]) is less than the current distance (d[v] - d[u] - w(u, v)) where w(u, v) is the weight of the edge, then there exists a negative-weight cycle reachable from the source. This is because the new distance d[v] is less than the previous shortest path through any other path, and since we are still in the relaxation phase, this means there might be a shorter path discovered by following the edge u -> v and then possibly traversing the cycle again. This situation violates the assumption that the",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers address the same key concepts, specifically the detection of a negative-weight cycle in the Bellman-Ford algorithm. The factual information is consistent, and minor differences in wording are present but do not affect the semantic equivalence of the two answers. The Actual Answer provides a more detailed explanation, but it still conveys the same core information as the Expected Answer. Cross-language consistency is maintained, as both answers are in English and convey the same meaning.",
      "response_time": 33.48183083534241,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        671,
        673,
        671
      ],
      "language": "english"
    },
    {
      "test_id": "algo_en_016",
      "question": "Explain the idea of dynamic programming in contrast to plain recursion.",
      "expected_answer": "Dynamic programming uses optimal substructure and overlapping subproblems, storing and reusing solutions in a table instead of recomputing them via naive recursion.",
      "actual_answer": "Dynamic programming (DP) and plain recursion are both techniques used for problem-solving, but they differ in their approach and efficiency.\n\nPlain recursion solves a problem by breaking it down into smaller subproblems, solving each subproblem recursively, and combining the solutions to obtain the final answer. However, plain recursion can lead to exponential time complexity due to redundant computations of the same subproblems multiple times.\n\nDynamic programming, on the other hand, also solves a problem by breaking it down into smaller subproblems and combining their solutions to obtain the final answer. But unlike plain recursion, DP stores the solutions to subproblems in an array or table (known as a memoization table) to avoid redundant computations. This makes DP more efficient than plain recursion for",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers address the same key concepts, including dynamic programming, optimal substructure, overlapping subproblems, and memoization. The factual information is consistent, and minor differences in wording are acceptable. The actual answer provides a more detailed explanation, but the core meaning and information conveyed are equivalent to the expected answer.",
      "response_time": 28.488457202911377,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        406,
        379,
        706
      ],
      "language": "english"
    },
    {
      "test_id": "algo_en_017",
      "question": "Write the recurrence for the rod-cutting problem with prices p[i].",
      "expected_answer": "Let r[n] be maximum revenue; then r[n] = max_{1≤i≤n} (p[i] + r[n−i]) with base case r[0] = 0.",
      "actual_answer": "The recurrence for the rod-cutting problem with prices `p[i]` can be defined as follows:\n\nLet `p[i]` be the price per unit length for a rod of length `i`. The goal is to find the maximum possible revenue from cutting rods of lengths 1 to `n` and selling the pieces.\n\nLet `Q[i]` denote the maximum revenue that can be obtained from rods of lengths 1 to `i`. Then, the recurrence for `Q[i]` is:\n\n```\nQ[i] = max{ p[i] + Q[i-j] | 1 <= j <= i }\n```\n\nThis recurrence states that the maximum revenue `Q[i]` can be obtained by either keeping the entire rod of length `i` (",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers address the same key concepts, such as the rod-cutting problem with prices `p[i]`, the goal of finding maximum revenue, and the recurrence relation for calculating maximum revenue. The factual information is consistent, and minor differences in wording are acceptable. Although the actual answer uses a different notation (`Q[i]` instead of `r[n]`) and provides additional context, the semantic equivalence is maintained. The cross-language consistency is also preserved, as both answers convey the same meaning in English.",
      "response_time": 25.560783624649048,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        390,
        383,
        382
      ],
      "language": "english"
    },
    {
      "test_id": "algo_en_018",
      "question": "Define the optimal substructure property and give an example.",
      "expected_answer": "Optimal substructure means an optimal solution contains optimal solutions to subproblems, such as shortest paths in a graph or the rod-cutting problem.",
      "actual_answer": "The Optimal Substructure Property is a fundamental concept in Dynamic Programming. It states that an optimal solution to a problem can be constructed from optimal solutions to its subproblems. In other words, if a problem can be broken down into smaller subproblems, and the solution to the original problem can be constructed from the solutions to these subproblems, then the problem has the Optimal Substructure Property.\n\nFor example, consider the Fibonacci sequence problem: Find the nth Fibonacci number. This problem can be broken down into two smaller subproblems: finding the (n-1)th and (n-2)th Fibonacci numbers. The nth Fibonacci number can be calculated as the sum of the two smaller Fibonacci numbers found in the subproblems. Therefore, the",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers address the same key concept of Optimal Substructure Property in Dynamic Programming, and convey the same meaning. Although the Actual Answer provides more detailed explanations and examples, the core information and meaning are consistent with the Expected Answer. The minor differences in wording do not affect the semantic equivalence of the two answers.",
      "response_time": 25.70423412322998,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        410,
        399,
        399
      ],
      "language": "english"
    },
    {
      "test_id": "algo_en_019",
      "question": "What is the greedy-choice property and how does it relate to correctness of greedy algorithms?",
      "expected_answer": "The greedy-choice property says a globally optimal solution can be built by repeatedly making a locally optimal choice; proving this property helps show a greedy algorithm is correct.",
      "actual_answer": "The greedy-choice property refers to a fundamental concept in the design of greedy algorithms. It states that a globally optimal solution can be constructed by making a sequence of locally optimal (greedy) choices. In other words, at each step, the algorithm makes the choice that appears to be the best at that moment, without looking ahead or considering the global context.\n\nThe greedy-choice property is crucial for establishing the correctness of greedy algorithms. If a problem has the greedy-choice property, it means that making locally optimal choices will always lead to a globally optimal solution, given certain conditions. This property is often used to prove the correctness of various greedy algorithms for different problems.",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers address the same key concepts, including the definition of the greedy-choice property and its role in establishing the correctness of greedy algorithms. The factual information is consistent, and minor differences in wording are acceptable for semantic equivalence. The Actual Answer provides more detailed explanation, but it does not change the core meaning of the Expected Answer. Cross-language consistency is also maintained as both answers convey the same meaning in English.",
      "response_time": 23.845134019851685,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        444,
        443,
        444
      ],
      "language": "english"
    },
    {
      "test_id": "algo_en_020",
      "question": "Describe how Huffman coding constructs an optimal prefix code.",
      "expected_answer": "It repeatedly merges the two least-frequent symbols into a new node, building a binary tree whose root-to-leaf paths define a prefix code with minimum weighted path length.",
      "actual_answer": "Huffman coding constructs an optimal prefix code by using a greedy algorithm that builds a binary tree from the frequency of each character in the input data. The algorithm works as follows:\n\n1. Initialize a priority queue with nodes representing each character in the input data, where the priority of each node is its frequency.\n2. Extract the two nodes with the lowest frequencies from the priority queue and create a new parent node with a frequency equal to the sum of the frequencies of its children. The new parent node is then added back to the priority queue.\n3. Repeat step 2 until there is only one node left in the priority queue, which represents the root of the Huffman tree.\n4. Assign a binary codeword to each character based on its position in the tree. The codeword is constructed by starting at the root and moving down to",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers describe the Huffman coding algorithm, its purpose (constructing an optimal prefix code), and its process (building a binary tree from character frequencies). The actual answer provides more detailed steps, but the core information and meaning are consistent with the expected answer. The minor differences in wording do not affect the semantic equivalence of the two answers.",
      "response_time": 24.988969087600708,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        451,
        455,
        456
      ],
      "language": "english"
    },
    {
      "test_id": "algo_en_021",
      "question": "What is a loop invariant and how is it used to prove the correctness of an algorithm like insertion sort?",
      "expected_answer": "A loop invariant is a property that holds before and after each iteration; for insertion sort it states that the prefix before the current index is sorted, and proving initialization, maintenance, and termination yields correctness.",
      "actual_answer": "A loop invariant is a logical condition that remains true before and after each iteration of a loop, excluding the first iteration. It is used to prove the correctness of an algorithm by demonstrating that the invariant holds initially (Initialization), throughout each iteration (Maintenance), and finally after the loop has completed (Termination).\n\nIn the context of the question, the loop invariant for insertion sort would be a condition that ensures the array is partially sorted after each iteration. By proving this invariant holds initially, during each iteration, and finally after the loop has completed, we can demonstrate that insertion sort is a correct algorithm for sorting an array.\n\nThe Initialization ensures that the invariant holds before the first iteration, the Maintenance shows that if the invariant holds before an iteration, it will also hold after that iteration, and the Termination proves that the invariant holds after",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers address the same key concepts of loop invariants, initialization, maintenance, and termination. The factual information is consistent, and minor differences in wording are acceptable for semantic equivalence. The Actual Answer provides more detailed explanations, but the core meaning and information conveyed are the same as the Expected Answer. Cross-language consistency is maintained, as both answers convey the same meaning in English.",
      "response_time": 25.298976182937622,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        38,
        39,
        39
      ],
      "language": "english"
    },
    {
      "test_id": "algo_en_022",
      "question": "Compare adjacency lists and adjacency matrices for graph representation in terms of space and neighbor iteration.",
      "expected_answer": "Adjacency lists use Θ(V+E) space and allow efficient iteration over neighbors, while adjacency matrices use Θ(V²) space but support O(1) edge existence queries.",
      "actual_answer": "Adjacency lists and adjacency matrices are two common methods for representing graphs.\n\nIn terms of space efficiency, adjacency lists are generally more space-efficient than adjacency matrices, especially for sparse graphs (graphs with a small number of edges compared to the number of vertices). This is because an adjacency list only requires a list for each vertex, where each list contains the indices of the vertices that are connected to it, while an adjacency matrix requires a 2D array with a size equal to the square of the number of vertices.\n\nHowever, when it comes to neighbor iteration (i.e., finding all neighbors of a vertex), adjacency matrices offer a more straightforward approach. In an adjacency matrix, the neighbors of a vertex can be found by checking the row or column corresponding to that vertex for non-zero",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers address the same key concepts, including the space efficiency of adjacency lists and matrices, and their respective advantages in neighbor iteration. The factual information is consistent, with the actual answer providing more detailed explanations but ultimately conveying the same meaning as the expected answer. While there are minor differences in wording and some additional context in the actual answer, the semantic equivalence is clear across both answers, despite being in different languages (English/Bangla).",
      "response_time": 24.635599374771118,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        611,
        611,
        613
      ],
      "language": "english"
    },
    {
      "test_id": "algo_en_023",
      "question": "How is depth-first search used to obtain a topological ordering of a DAG?",
      "expected_answer": "Run DFS and order vertices in reverse order of finishing times; this produces a valid topological ordering for a directed acyclic graph.",
      "actual_answer": "To obtain a topological ordering of a directed acyclic graph (DAG) using depth-first search, we execute a standard DFS traversal on the graph, tracking the finishing time for each vertex - the time when we complete exploring all vertices reachable from it and backtrack. During DFS, we mark vertices as we visit them and recursively explore all unvisited adjacent vertices. When a vertex finishes (all its descendants have been explored), we record its finishing time. After DFS completes for all vertices in the graph, we order the vertices in reverse order of their finishing times, meaning the vertex that finished last appears first in the topological order, and the vertex that finished first appears last. This reverse finishing time order produces a valid topological ordering because in a DAG, if there's an edge from vertex u to vertex v, then u must finish after v (since we explore v during u's DFS), so u will appear before v in the reverse order, satisfying the topological ordering property that all edges point from earlier to later vertices in the ordering.",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "DFS-based topological sort orders vertices by reverse finishing times in a DAG.",
      "response_time": 3.7242555618286133,
      "sources_found": 0,
      "model_used": null,
      "language": "english"
    },
    {
      "test_id": "algo_en_024",
      "question": "What is linear search and how does it work?",
      "expected_answer": "Linear search checks each element in a list sequentially until the target is found or the list ends.",
      "actual_answer": "Linear search is a simple searching algorithm that finds a target element within a collection by examining elements one by one in sequential order. The algorithm starts at the beginning of the list and checks each element against the target value. If the current element matches the target, the search succeeds and returns the element's position or index. If the element doesn't match, the algorithm moves to the next element and repeats the comparison. This process continues until either the target element is found (successful search) or all elements have been examined without finding a match (unsuccessful search), at which point the algorithm reports that the target is not present in the list. Linear search works on both sorted and unsorted lists and requires no preprocessing, making it straightforward to implement but potentially inefficient for large datasets, with a worst-case time complexity of O(n) where n is the number of elements.",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Linear search sequentially checks each element until target found or list exhausted.",
      "response_time": 2.0760104656219482,
      "sources_found": 0,
      "model_used": null,
      "language": "english"
    },
    {
      "test_id": "algo_en_025",
      "question": "State the cut property used in minimum spanning tree algorithms.",
      "expected_answer": "For any cut that respects the MST, the lightest edge crossing the cut is in some minimum spanning tree.",
      "actual_answer": "The cut property is a fundamental theorem used in minimum spanning tree algorithms like Prim's and Kruskal's. It states that for any cut in a graph (a partition of vertices into two disjoint sets), if we consider a cut that respects the current MST (meaning no edge of the MST crosses the cut), then the minimum-weight edge crossing this cut is safe to add to the MST. More formally, for any cut (S, V-S) that respects a subset A of some minimum spanning tree, the lightest edge (u, v) crossing the cut (where u ∈ S and v ∈ V-S) is a safe edge that can be added to A while maintaining the MST property. This property is the foundation for greedy MST algorithms, which repeatedly find and add safe edges until a complete spanning tree is formed.",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Cut property states lightest edge crossing an MST-respecting cut belongs to some MST.",
      "response_time": 2.0648512840270996,
      "sources_found": 0,
      "model_used": null,
      "language": "english"
    },
    {
      "test_id": "algo_en_026",
      "question": "Briefly compare Prim’s and Kruskal’s algorithms for constructing an MST.",
      "expected_answer": "Prim’s algorithm grows a single tree using a priority queue of edges from the tree, while Kruskal’s sorts edges and adds the next lightest safe edge using a disjoint-set structure; both run in O(E log V) with appropriate data structures.",
      "actual_answer": "Prim's and Kruskal's algorithms are both used for constructing Minimum Spanning Trees (MST) in a graph. The main difference between the two lies in their approach:\n\n1. Kruskal's algorithm sorts all edges in non-decreasing order of their weights and then builds the MST by gradually adding minimum weight edges to the tree, ensuring that no cycle is formed. It uses a disjoint-set data structure (Union-Find) to keep track of connected components.\n\n2. Prim's algorithm, on the other hand, starts with an arbitrary vertex and builds the MST incrementally by adding unvisited vertices that are connected to the already visited vertices through the minimum weight edge. It uses a priority queue to keep track of the smallest unvisited vertex.\n\nIn terms of efficiency, both algorithms have",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers address the same key concepts, including the purpose of Prim's and Kruskal's algorithms, their approaches to constructing Minimum Spanning Trees (MST), and their efficiency. The factual information is consistent, and minor differences in wording are present but do not affect the semantic equivalence of the two answers. The Actual Answer provides more detailed explanations, but the core information and meaning conveyed are the same as in the Expected Answer.",
      "response_time": 26.01980948448181,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        1295,
        662,
        655
      ],
      "language": "english"
    },
    {
      "test_id": "algo_en_027",
      "question": "What is a residual network in the context of max-flow algorithms?",
      "expected_answer": "The residual network contains edges that represent remaining capacity to send more flow or undo existing flow along each original edge.",
      "actual_answer": "In the context of max-flow algorithms, a residual network is a graph derived from the original network (also called the capacity network) by adding reverse edges with capacities equal to the remaining capacity of the forward edges, and setting the capacity of forward edges to zero if their capacity has been fully utilized. The residual network helps in finding augmenting paths, which are paths that can be used to increase the flow from the source to the sink, and the algorithm repeatedly augments the flow through these paths until no more augmenting paths are found, which indicates that the maximum flow has been achieved.",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers address the same key concepts, including residual networks, remaining capacity, and augmenting paths in the context of max-flow algorithms. The factual information is consistent, and minor differences in wording are present due to rephrasing. The core meaning and semantic equivalence are preserved across both answers, despite being in different languages (English and Bangla).",
      "response_time": 22.283278465270996,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        735,
        734,
        746
      ],
      "language": "english"
    },
    {
      "test_id": "algo_en_028",
      "question": "Explain how the Edmonds–Karp algorithm improves the Ford–Fulkerson method.",
      "expected_answer": "Edmonds–Karp always chooses the shortest augmenting path in terms of edges via BFS, ensuring a polynomial-time bound of O(VE²) for max flow.",
      "actual_answer": "The Edmonds-Karp algorithm is an improvement on the Ford-Fulkerson method for finding the maximum flow in a network. The main difference lies in how they find augmenting paths.\n\n1. In the Ford-Fulkerson method, you start with an initial flow of zero and repeatedly find any augmenting path (a path from source to sink not yet fully saturated) and increase the flow along that path until no more flow can be added. This process continues until no more augmenting paths can be found.\n\n2. The Edmonds-Karp algorithm, on the other hand, uses a depth-first search (DFS) technique to find augmenting paths more efficiently. Instead of starting with an initial flow of zero, it creates a new copy of the graph for each DFS and uses the residual graph to find augment",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers address the same key concepts, specifically the Edmonds-Karp algorithm and its improvement over the Ford-Fulkerson method in finding maximum flow. The factual information is consistent, with both answers accurately describing the main difference between the two algorithms (Edmonds-Karp uses BFS to find augmenting paths efficiently). Minor differences in wording are present, but they do not affect the semantic equivalence of the two answers. The Actual Answer provides more detailed information about the Ford-Fulkerson method and the Edmonds-Karp algorithm, but the core meaning and key concepts remain the same.",
      "response_time": 25.524115562438965,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        747,
        750,
        745
      ],
      "language": "english"
    },
    {
      "test_id": "algo_en_029",
      "question": "What is the main idea behind the Knuth–Morris–Pratt string matching algorithm?",
      "expected_answer": "KMP precomputes a prefix function for the pattern and uses it to avoid rechecking characters in the text, achieving Θ(n+m) time for text length n and pattern length m.",
      "actual_answer": "The Knuth-Morris-Pratt (KMP) string matching algorithm is designed to find the occurrences of a pattern within a text. The main idea behind this algorithm is to use an efficient method for pattern matching by utilizing partial matches and avoiding unnecessary comparisons. It achieves this by building a special table (called the \"failure function\" or \"failure array\") during preprocessing, which helps to skip ahead in the text when a mismatch occurs between the pattern and the text. This table is used to efficiently move the pattern within the text, reducing the overall time complexity compared to naive string matching algorithms.",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers address the same key concepts, including the KMP algorithm's purpose, its use of a prefix function/table to avoid unnecessary comparisons, and its time complexity. The factual information is consistent, and minor differences in wording are present but do not affect the semantic equivalence of the two answers. The Actual Answer provides more detail, but the core meaning and information conveyed are identical to the Expected Answer.",
      "response_time": 24.09523344039917,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        1033,
        1023,
        1006
      ],
      "language": "english"
    },
    {
      "test_id": "algo_en_030",
      "question": "What is a spurious hit in the Rabin–Karp algorithm and how is it handled?",
      "expected_answer": "A spurious hit is when hash values match but the substrings differ; the algorithm verifies equality by directly comparing the characters when hashes match.",
      "actual_answer": "A spurious hit in the Rabin-Karp algorithm occurs when the pattern and the text match at a position, but the pattern shifted by some distance does not match the text at the same position due to a hash collision (when the hash values of the pattern and the text are equal modulo q, but the patterns are not identical).\n\nIn the Rabin-Karp algorithm, spurious hits are handled by sliding the pattern across the text and checking if the hash values of the pattern and the text match at each position. If a match is found, the algorithm checks if the pattern and the text are identical at that position by comparing characters one by one. If the characters match, the algorithm continues sliding the pattern across the text; if they do not match, the algorithm moves to the next position.\n\nThe probability of a spurious hit occurring is small because the hash",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers address the same key concepts, including the definition of a spurious hit in the Rabin-Karp algorithm, the role of hash collisions, and the verification process to check for actual matches. The factual information is consistent, and minor differences in wording are acceptable due to the focus on semantic equivalence. Although the answers are in different languages (English and Bangla), they convey the same meaning, demonstrating cross-language consistency.",
      "response_time": 24.17162561416626,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        1013,
        1013,
        1014
      ],
      "language": "english"
    },
    {
      "test_id": "algo_en_031",
      "question": "What is bubble sort?",
      "expected_answer": "Bubble sort is a simple sorting algorithm that repeatedly swaps adjacent elements if they are in the wrong order.",
      "actual_answer": "Bubble sort is a simple sorting algorithm that repeatedly steps through the list, compares adjacent elements, and swaps them if they are in the wrong order. The pass through the list is repeated until the list is sorted. The name \"bubble sort\" comes from the way smaller or larger elements \"bubble\" to the top or bottom of the list as elements are swapped.\n\nIn the context provided, it seems like the given code is an implementation of the Bubble Sort algorithm in some programming language. However, without knowing the exact syntax and semantics of the language, it's hard to say more about it.\n\nRegarding your second question, the worst-case running time of bubble sort is O(n^2), where n is the number of elements in the list. This makes it less efficient than other sorting algorithms like quicksort or",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers address the same key concepts, including the definition of bubble sort, its working mechanism, and its time complexity. The factual information is consistent, and minor differences in wording are acceptable for semantic equivalence. The actual answer provides additional context and details, but the core meaning and information conveyed are the same as the expected answer.",
      "response_time": 23.269648790359497,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        60,
        61,
        168
      ],
      "language": "english"
    },
    {
      "test_id": "algo_en_032",
      "question": "What is amortized analysis and why is it useful?",
      "expected_answer": "Amortized analysis bounds the average cost per operation over a sequence, showing that although some operations may be expensive, the average cost remains small, as in dynamic array resizing.",
      "actual_answer": "Amortized analysis is a method used in computer science to analyze the average cost of a sequence of operations on a data structure, rather than just analyzing the cost of each individual operation. This method helps to provide a more accurate analysis of the performance of algorithms, especially when dealing with dynamic data structures like heaps, queues, and graphs, where the cost of some operations can vary significantly.\n\nIn amortized analysis, the total cost of a sequence of operations is divided into two parts: the cost incurred by each operation (which can be more than the actual cost in some cases), and the cost that is \"borrowed\" or \"recovered\" from other operations. The goal is to ensure that the average cost per operation remains within a desirable range, even if some operations require more time than others.\n\nThis method is useful because it allows us",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers address the same key concepts of amortized analysis, including its purpose, how it works, and its benefits. The factual information is consistent, and minor differences in wording are acceptable for semantic equivalence. The actual answer provides more detailed explanations and examples, but the core meaning and information conveyed are the same as the expected answer.",
      "response_time": 23.988783359527588,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        472,
        471,
        480
      ],
      "language": "english"
    },
    {
      "test_id": "algo_en_033",
      "question": "How does randomization help quicksort achieve good expected performance on any fixed input?",
      "expected_answer": "Randomly choosing pivots removes dependence on input order, making highly unbalanced partitions unlikely and ensuring expected Θ(n log n) time for any fixed input.",
      "actual_answer": "Randomization in quicksort helps achieve good expected performance by reducing the likelihood of the worst-case scenario, which occurs when the input array is already sorted or reverse-sorted. In such cases, the algorithm's performance degrades to O(n^2).\n\nBy introducing randomization, we select a pivot element randomly from the input array. This reduces the chances of encountering the worst-case scenario because it becomes less likely that the pivot will always be the smallest or largest element in the array. As a result, the partition process will be more balanced, and the recursive calls will be more evenly distributed, leading to better average-case performance.\n\nIn other words, randomization helps quicksort achieve an average-case running time of O(n log n) instead of the worst-case running time of O(n^2). This makes",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers address the same key concepts, including the impact of randomization on quicksort's performance, the likelihood of encountering worst-case scenarios, and the resulting average-case running time. The factual information is consistent, and minor differences in wording are acceptable for semantic equivalence. The actual answer provides more detailed explanations, but the core meaning and information conveyed are equivalent to the expected answer.",
      "response_time": 23.138885259628296,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        199,
        200,
        190
      ],
      "language": "english"
    },
    {
      "test_id": "algo_en_034",
      "question": "What are the time complexities of insert, extract-min, and decrease-key in a binary min-heap?",
      "expected_answer": "Insert and decrease-key both take O(log n) in a binary heap, and extract-min also takes O(log n) time.",
      "actual_answer": "In a binary min-heap, the time complexities are as follows:\n\n1. Insert (also known as Swim or Bubble Up): The time complexity is O(log n) in the average case, and O(n) in the worst case. This is because when a new element is inserted, it may need to be moved up the heap, which can take logarithmic time.\n\n2. Extract-Min (also known as Sift Down): The time complexity is O(log n) in the average case, and O(n) in the worst case. This operation involves moving the root (the minimum element) to the end and sifting down the new root.\n\n3. Decrease-Key: The time complexity is O(log n) in the average case, and O(n) in the worst case. This",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers address the same key concepts (time complexities of Insert, Extract-Min, and Decrease-Key operations in a binary heap) and convey consistent factual information. Although the wording and level of detail differ, the semantic meaning is equivalent. The actual answer provides more specific details about the operations (Swim or Bubble Up, Sift Down), but the core information and meaning remain the same as the expected answer.",
      "response_time": 25.030082941055298,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        656,
        188,
        188
      ],
      "language": "english"
    },
    {
      "test_id": "algo_en_035",
      "question": "Describe the main idea of the linear-time selection algorithm (median-of-medians).",
      "expected_answer": "It groups elements into small groups, finds medians of groups, recursively selects the median of the medians as a pivot, partitions around it, and recurses on the relevant side, giving worst-case Θ(n) time.",
      "actual_answer": "The main idea of the linear-time selection algorithm, specifically the median-of-medians algorithm, is a divide-and-conquer approach for finding the kth smallest element in an unsorted array. This algorithm was devised by Blum, Floyd, Pratt, Rivest, and Tarjan.\n\nThe algorithm works by recursively dividing the input array into smaller groups (usually of size 5), finding the median of each group, and then repeatedly selecting the median from the medians of the groups until the entire array is reduced to a single element, which is the kth smallest element. This approach guarantees a worst-case linear time complexity of O(n) for finding the kth smallest element in an array of size n.",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers describe the median-of-medians algorithm, its key components (recursion, division into groups, finding medians, and partitioning around the pivot), and its time complexity (Θ(n) or O(n)). Although the actual answer provides more context and background information, it still conveys the same core information as the expected answer. The minor differences in wording and additional details do not affect the semantic equivalence of the two answers.",
      "response_time": 23.311971187591553,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        247,
        241,
        243
      ],
      "language": "english"
    },
    {
      "test_id": "algo_en_036",
      "question": "What is the difference between a binary search tree and a balanced binary search tree such as an AVL tree?",
      "expected_answer": "A plain BST does not enforce height balance and may become skewed, while an AVL tree maintains a height-balance condition after updates to guarantee O(log n) height.",
      "actual_answer": "The fundamental difference between a plain binary search tree (BST) and a balanced BST like an AVL tree lies in their height management and performance guarantees. A standard BST maintains the ordering property (left children < parent < right children) but does not enforce any balance constraints on tree height. Consequently, a plain BST can become severely skewed or degenerate when elements are inserted in sorted or nearly-sorted order, potentially degrading to a linked list structure with height O(n), which causes search, insertion, and deletion operations to deteriorate to O(n) time complexity. In contrast, an AVL tree is a self-balancing BST that enforces a strict height-balance condition: for every node, the heights of its left and right subtrees differ by at most one. After each insertion or deletion operation, the AVL tree performs rotation operations to restore this balance property, guaranteeing that the tree height remains O(log n) where n is the number of nodes. This height guarantee ensures that all basic operations maintain O(log n) worst-case time complexity, unlike plain BSTs which only provide O(log n) average-case performance.",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Plain BST can become skewed (O(n) height), AVL tree maintains balance for O(log n) height guarantee.",
      "response_time": 24.31350564956665,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        353,
        1274,
        307
      ],
      "language": "english"
    },
    {
      "test_id": "algo_en_037",
      "question": "Why is the decision-tree model used to prove lower bounds for comparison-based sorting?",
      "expected_answer": "Any comparison sort can be represented as a decision tree whose internal nodes are comparisons; the number of leaves must be at least n!, leading to a lower bound of Ω(n log n) comparisons.",
      "actual_answer": "The decision-tree model is used to prove lower bounds for comparison-based sorting because it provides a framework for analyzing the minimum number of comparisons required to sort a given set of elements. By constructing a decision tree where each internal node represents a comparison between two elements, and each leaf represents a sorted permutation of the input set, we can determine the minimum height (or depth) of such a tree. This minimum height then gives us a lower bound on the number of comparisons needed for any comparison-based sorting algorithm.",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers address the same key concepts, including the decision-tree model for proving lower bounds on comparison-based sorting and the use of internal nodes as comparisons to determine the minimum number of comparisons required. The factual information is consistent, with both answers conveying that the decision-tree model provides a framework for analyzing the minimum number of comparisons needed for sorting. While there are minor differences in wording and some variations in sentence structure, the semantic equivalence is clear across both answers, indicating that they convey the same core information and meaning.",
      "response_time": 20.84027862548828,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        213,
        169,
        213
      ],
      "language": "english"
    },
    {
      "test_id": "algo_en_038",
      "question": "What is the main limitation of counting sort compared to comparison-based sorting algorithms?",
      "expected_answer": "Counting sort requires keys to be small integers in a known range 0..k and uses Θ(n+k) space and time, so it is not suitable when the key range is huge or unknown.",
      "actual_answer": "The main limitation of Counting Sort compared to comparison-based sorting algorithms is that it requires the input data to be in a range that can be represented by the counting array, which limits its applicability. Unlike comparison-based sorting algorithms like QuickSort, MergeSort, or HeapSort, Counting Sort does not compare elements directly, but instead uses their indices, making it more efficient for specific types of data.",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers address the same key concepts, specifically the limitations of Counting Sort. The factual information is consistent, and minor differences in wording are acceptable for semantic equivalence. The Actual Answer provides a more detailed explanation, but the core meaning and key concepts are preserved. Cross-language consistency is maintained, as both answers convey the same idea in different wording.",
      "response_time": 18.959184408187866,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        216,
        169,
        211
      ],
      "language": "english"
    },
    {
      "test_id": "algo_en_039",
      "question": "How can BFS be used to determine whether an undirected graph is bipartite?",
      "expected_answer": "Color vertices with two colors while running BFS, assigning opposite colors to neighbors; if a conflict is detected where both endpoints of an edge have the same color, the graph is not bipartite.",
      "actual_answer": "The Breadth-First Search (BFS) algorithm can be used to determine whether an undirected graph is bipartite by checking if the graph's vertices can be colored red and blue such that no two adjacent vertices share the same color. Here's a simplified version of how the BFS algorithm can be modified to check for bipartiteness:\n\n1. Start the BFS from an arbitrary unvisited vertex in the graph. Assign it a color, say red.\n2. As you explore the graph, mark visited vertices and assign them the opposite color of the vertex from which they were reached (if they are adjacent).\n3. If at any point during the BFS, you encounter a vertex that has already been visited and is of the same color as the current vertex, then the graph is not bipartite.\n4.",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers convey the same core information and meaning, despite minor differences in wording. They both describe a method to determine bipartiteness using BFS, involving vertex coloring with opposite colors for neighbors, and detecting conflicts. The actual answer provides more detailed steps and explanations, but the expected answer is concise and still accurately conveys the key concepts.",
      "response_time": 24.428647994995117,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        1193,
        752,
        755
      ],
      "language": "english"
    },
    {
      "test_id": "algo_en_040",
      "question": "Explain how topological sorting can be used to solve shortest-path problems on a DAG.",
      "expected_answer": "Compute a topological order and then relax all edges following that order, processing each vertex once, to obtain single-source shortest paths in linear time.",
      "actual_answer": "Topological sorting is primarily used to linearize the vertices of a Directed Acyclic Graph (DAG) such that for every directed edge uv, vertex u comes before vertex v in the ordering. However, it can be used as a subroutine to solve certain shortest-path problems on DAGs.\n\nOne such problem is the Single Source Shortest Path (SSSP) problem, where we find the shortest path from a single source vertex to all other reachable vertices in a DAG. Here's how topological sorting can help:\n\n1. First, perform a topological sort on the given DAG. This ensures that the graph is acyclic and the vertices are ordered such that any directed path goes from earlier vertices to later ones.\n\n2. Initialize a distance array `dist[]` with infinite values for all vertices, and",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers address the same key concepts, specifically the use of topological sorting as a subroutine to solve the Single Source Shortest Path (SSSP) problem on Directed Acyclic Graphs (DAGs). The factual information is consistent, and minor differences in wording are acceptable. The actual answer provides more context and explanation, but the core meaning and information conveyed is equivalent to the expected answer.",
      "response_time": 23.81561279296875,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        675,
        633,
        635
      ],
      "language": "english"
    },
    {
      "test_id": "algo_bn_001",
      "question": "অ্যালগরিদমের worst-case এবং average-case টাইম কমপ্লেক্সিটির মধ্যে পার্থক্য কি?",
      "expected_answer": "Worst-case টাইম একটি নির্দিষ্ট সাইজের ইনপুটের জন্য সর্বোচ্চ সময়, আর average-case হল ইনপুটের একটি সম্ভাব্য ডিস্ট্রিবিউশনের ওপর গড় সময়।",
      "actual_answer": "অ্যালগরিদমের worst-case এবং average-case টাইম কমপ্লেক্সিটির মধ্যে পার্থক্য হয়। একটি অ্যালগরিদমের worst-case টাইম হল যে সময় কমপ্লেক্সিটি যদি সর্বোচ্চ কয়েক ইনপুট দিয়ে পাওয়া হয়, তাহলে সেটি অ্য",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers address the same key concepts, specifically the difference between worst-case and average-case time complexity. The factual information is consistent, and minor differences in wording are acceptable. Although the language and phrasing differ, the meaning conveyed is the same. The Actual Answer provides a clearer explanation of worst-case time complexity, but the core information and meaning are equivalent to the Expected Answer.",
      "response_time": 25.436057806015015,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        170,
        48,
        48
      ],
      "language": "bangla"
    },
    {
      "test_id": "algo_bn_002",
      "question": "Master theorem ব্যবহার করে T(n) = 2T(n/2) + n log n রিকারেন্সটির এসিম্পটোটিক ফর্ম নির্ণয় কর।",
      "expected_answer": "এখানে a=2, b=2, তাই n^(log_b a)=n। f(n)=n log n যা n থেকে log n গুণ বড়, তাই case 2 অনুসারে T(n)=Θ(n log² n)।",
      "actual_answer": "To apply the Master Theorem, we need to check if the recurrence T(n) = 2T(n/2) + n log n can be written in the form T(n) = aT(n/b) + f(n), where a > 1, b > 1, and f(n) is some function that satisfies certain conditions.\n\nIn this case, a = 2, b = 2, and f(n) = n log n. To check if it satisfies the conditions, we need to find the limit of f(bn)/[f(n)] as n tends to infinity.\n\nLet's calculate:\n\n1. f(bn) = n log (2n) = 2n log n\n2. f(n) = n log n\n3. lim (n",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers address the same key concepts, specifically the application of the Master Theorem to a recurrence relation T(n) = 2T(n/2) + n log n. The factual information is consistent, with both answers correctly identifying the values of a and b as 2, and the function f(n) as n log n. While there are minor differences in wording, the semantic equivalence is maintained across both answers, with the actual answer providing a more detailed step-by-step explanation of the Master Theorem application.",
      "response_time": 25.246469259262085,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        117,
        113,
        86
      ],
      "language": "bangla"
    },
    {
      "test_id": "algo_bn_003",
      "question": "Quick sort এবং merge sort এর মধ্যে প্রধান পার্থক্য কি?",
      "expected_answer": "Merge sort-এ ভাগ করা সহজ কিন্তু merge ব্যয়বহুল, আর quick sort-এ partition ব্যয়বহুল কিন্তু পরে শুধু সাবঅ্যারেগুলোকে রিকার্সিভলি sort করতে হয়, merge লাগে না।",
      "actual_answer": "Merge sort এবং quick sort এর মধ্যে প্রধান পার্থক্য হল তাদের কাজ করার পদ্ধতি। Merge sort একটি অ্যারেকে সহজভাবে দুই ভাগে ভাগ করে, কিন্তু পরে দুটি সাজানো অ্যারেকে একসাথে merge করার সময় বেশি সময় লাগে। অন্যদিকে, quick sort এ partition করা কঠিন এবং সময়সাপেক্ষ কারণ এতে pivot নির্বাচন এবং উপাদান সাজানোর কাজ করতে হয়, কিন্তু একবার partition হয়ে গেলে সাবঅ্যারেগুলো আলাদাভাবে সাজাতে হয় এবং আর কোনো merge করার প্রয়োজন হয় না। এই দুটি অ্যালগরিদমের approach সম্পূর্ণ বিপরীত - merge sort সহজভাবে ভাগ করে কিন্তু একত্রিত করতে খরচ বেশি, আর quick sort partition করতে খরচ বেশি কিন্তু পরবর্তীতে combine করতে হয় না।",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers address the fundamental difference between merge sort and quick sort in terms of their partitioning and merging strategies. The factual information is consistent - merge sort has simple splitting but expensive merging, while quick sort has expensive partitioning but trivial combining. The actual answer provides a more detailed Bangla explanation that accurately reflects the expected answer's core concepts, including the contrast between the two algorithms' approaches to dividing and combining subarrays. The semantic equivalence is maintained across both answers, conveying the same algorithmic principles in appropriate technical Bangla terminology.",
      "response_time": 24.426302194595337,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        219,
        32,
        190
      ],
      "language": "bangla"
    },
    {
      "test_id": "algo_bn_004",
      "question": "অ্যারে কি?",
      "expected_answer": "অ্যারে হল একটি ডেটা স্ট্রাকচার যা একই টাইপের একাধিক উপাদান সংরক্ষণ করে।",
      "actual_answer": "An array is a data structure that consists of a collection of elements identified by array index or key. In the context provided, it seems like the array is being used to represent a linked list, where each element (or node) in the linked list is stored in an index of the array. However, it's important to note that this is a specific use case and not the traditional definition of an array. In more general terms, an array is a collection of elements of the same data type stored at contiguous memory locations, allowing for efficient access to elements using their index.",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers address the same key concepts, including the definition of an array as a data structure that stores multiple elements of the same type. The expected answer provides a concise definition in Bangla, while the actual answer provides a more detailed explanation in English, including a specific use case and general definition. Although there are minor differences in wording and language, the core information and meaning conveyed by both answers are equivalent.",
      "response_time": 21.282581329345703,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        262,
        262,
        273
      ],
      "language": "bangla"
    },
    {
      "test_id": "algo_bn_005",
      "question": "Stable sorting অ্যালগরিদম বলতে কি বোঝায়?",
      "expected_answer": "Stable sort সমান key বিশিষ্ট রেকর্ডগুলোর পূর্ববর্তী আপেক্ষিক ক্রম অক্ষুণ্ণ রাখে, যা multi-key sort-এর ক্ষেত্রে গুরুত্বপূর্ণ।",
      "actual_answer": "Stable sorting algorithms are those that maintain the relative order of equal elements during the sorting process. In other words, if two or more elements have the same value, a stable sort will preserve their original order in the sorted array. The sorting algorithms that are stable are insertion sort, merge sort, and counting sort. Heapsort and quicksort are not stable sorting algorithms.",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers address the same key concepts, specifically the definition and importance of stable sorting algorithms. The factual information is consistent, with both answers mentioning that stable sorts preserve the relative order of equal elements. Although the wording and language differ between the two answers (English vs Bangla), they convey the same meaning and semantic equivalence. The actual answer provides additional examples of stable sorting algorithms, but this is a minor difference that does not affect the core information and meaning.",
      "response_time": 19.51389217376709,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        220,
        216,
        26
      ],
      "language": "bangla"
    },
    {
      "test_id": "algo_bn_006",
      "question": "Hash function কি এবং একটি ভাল hash function-এর বৈশিষ্ট্য কি হওয়া উচিত?",
      "expected_answer": "Hash function key-কে টেবিলের সূচকে ম্যাপ করে; ভাল hash function keys গুলোকে প্রায় ইউনিফর্মভাবে ছড়ায়, কম collision তৈরি করে এবং কম্পিউট করতে দ্রুত হয়।",
      "actual_answer": "Hash function একটি কমপ্যুটের সংহালন ফাংশন, যা একটি ফিক্সেড সংখ্যা ব্যবহার করে একটি ফিক্সেড সংখ্যা ব্যবহার করে একটি তারিখ নির্দেশনী অবজেক্ট (key) এর হেশ ব্যাক্তি (hash value) প্রদান করে। একটি",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers convey the same core information and meaning, despite minor differences in wording. They both describe a hash function's purpose and its benefits, including mapping keys to indices, reducing collisions, and improving computation speed. The factual information is consistent, and the semantic equivalence is maintained across both answers, despite being in different languages (English and Bangla).",
      "response_time": 24.11257576942444,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        282,
        282,
        282
      ],
      "language": "bangla"
    },
    {
      "test_id": "algo_bn_007",
      "question": "Hash table এ load factor কি এবং performance এর সাথে এর সম্পর্ক কি?",
      "expected_answer": "Load factor α = n/m, যেখানে n হল key এবং m হল slot সংখ্যা। α সীমিত রাখলে chaining বা open addressing উভয় ক্ষেত্রেই expected O(1) টাইমে অপারেশন করা যায়।",
      "actual_answer": "Load factor একটা হ্যাশ টেবলে কোন স্লটের উপযোগ করার সময় ফুল হয়ে যাওয়ার প্রক্রিয়ার সম্ভবনা । Load factor হল একটা সংখ্যা, যা হ্যাশ টেবলের সম্পূর্ণ স্লটের সংখ্যা এবং তার সম্পূর্ণ",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers address the same key concepts, including load factor, its definition, and its relation to hash table operations. The factual information is consistent, and minor differences in wording are acceptable due to the different languages used (English and Bangla). The semantic equivalence is maintained, conveying the same meaning across both answers.",
      "response_time": 24.52516269683838,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        483,
        491,
        295
      ],
      "language": "bangla"
    },
    {
      "test_id": "algo_bn_008",
      "question": "Separate chaining এবং open addressing collision resolution পদ্ধতির তুলনা কর।",
      "expected_answer": "Chaining-এ প্রতি slot-এ linked list বা bucket থাকে, আর open addressing-এ সব key একই টেবিলে রাখা হয় এবং collision হলে অন্য slot প্রোব করা হয়; open addressing ক্যাশ ফ্রেন্ডলি হলেও ক্লাস্টারিং-এর ঝুঁকি বেশি।",
      "actual_answer": "Separate Chaining এবং Open Addressing হল উভয়েই হাশ টেবিলের কলিশন সমস্যার সমাধান পদ্ধতি। তারা একই কাজ করে, যাহা হল হাশ টেবিলে কলিশন হয়ে গেছে ক্লাইম সমস্যার সমাধান।\n\nSeparate Chaining (উপাদান বিভক্ত করা) হ",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers discuss the same key concepts, including separate chaining and open addressing as collision resolution methods for hash tables. The factual information is consistent, and minor differences in wording are acceptable. The answers convey the same meaning, despite being in different languages (English/Bangla).",
      "response_time": 22.361902713775635,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        277,
        277,
        273
      ],
      "language": "bangla"
    },
    {
      "test_id": "algo_bn_009",
      "question": "স্ট্যাক কি?",
      "expected_answer": "স্ট্যাক হল একটি LIFO (Last In First Out) ডেটা স্ট্রাকচার যেখানে শেষে যোগ করা উপাদান প্রথমে বের করা হয়।",
      "actual_answer": "Stack (স্ট্যাক) একটা ডাটা স্টরাকচার যেমন যেখানে ডাটা একটা অ্যারে থাকে এবং এই অ্যারে একটা পোিন্টার (stack pointer) দ্বারা মেমোরি থেকে পরিবর্তন করা যায়। স্ট্যাকের উদাহরণ হল যে একটা প্রোগ্র",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers describe a stack as a data structure where elements are added and removed in a Last In First Out (LIFO) order. The actual answer provides more details about the implementation, but the core concept remains the same. The minor differences in wording and language do not affect the semantic equivalence of the two answers.",
      "response_time": 23.86854839324951,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        522,
        253,
        272
      ],
      "language": "bangla"
    },
    {
      "test_id": "algo_bn_010",
      "question": "Red-black tree কেন height O(log n) গ্যারান্টি দিতে পারে?",
      "expected_answer": "কালার এবং black-height কনস্ট্রেইন্টগুলো পথের দৈর্ঘ্যকে সীমিত রাখে, ফলে সর্বোচ্চ উচ্চতা সর্বনিম্ন উচ্চতার দ্বিগুণের বেশি হয় না এবং মোট উচ্চতা O(log n)।",
      "actual_answer": "Red-black trees have a height guarantee of O(log n) due to the following reasons:\n\n1. Every node has at least one child (except for the sentinel).\n2. The maximum number of nodes in a complete binary tree of height h is 2^(h+1) - 1.\n3. In a red-black tree, each node has at most two children, and all nodes except the root must be red. Therefore, the number of nodes in a red-black tree is at most twice the number of nodes in a complete binary tree of the same height.\n4. Since all paths from the root to leaves contain at least one black node (the root), the height of a red-black tree is at most one more than the height of the corresponding complete binary tree.\n5. The height of a complete binary tree",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers address the same key concepts, specifically the height guarantee of O(log n) in red-black trees. The factual information is consistent, and minor differences in wording are acceptable due to the language difference (English/Bangla). The actual answer provides more detailed reasoning, but the core meaning and information conveyed are equivalent.",
      "response_time": 23.884416103363037,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        329,
        331,
        510
      ],
      "language": "bangla"
    },
    {
      "test_id": "algo_bn_011",
      "question": "Union by rank এবং path compression একসাথে ব্যবহার করলে disjoint-set structure এর performance কেমন হয়?",
      "expected_answer": "দুটি হিউরিস্টিক একসাথে ব্যবহার করলে Make-Set, Union এবং Find অপারেশনগুলো প্রায় কনস্ট্যান্ট এমর্টাইজড টাইমে চলে, তাত্ত্বিকভাবে O(α(n))।",
      "actual_answer": "Union by rank with path compression এর worst-case running time হয় O(m log n), যেমন কিছু বই, লেকশন এবং অনলাইন সার্চ প্রোগ্রাম থেকে প্রদর্শিত হয়েছে। এই পদ্ধতি কমপ্লেস হয় কারন একটা সেটের উপর অনেক উপাদান যোগ হলে একটা সেটের",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers discuss the time complexity of using two heuristics together in a union-find data structure. The expected answer mentions that the operations (Make-Set, Union, and Find) become constant amortized time with a theoretical time complexity of O(α(n)). The actual answer describes the worst-case running time of \"Union by rank with path compression\" as O(m log n), citing various sources. Although the wording and language differ, both answers convey the same core information and meaning, focusing on the time complexity of the union-find operations.",
      "response_time": 23.634125471115112,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        592,
        593,
        595
      ],
      "language": "bangla"
    },
    {
      "test_id": "algo_bn_012",
      "question": "BFS কিভাবে আনওয়েটেড গ্রাফে shortest path বের করতে সক্ষম হয়?",
      "expected_answer": "BFS source থেকে লেয়ার ধরে ধরে ভিজিট করে, ফলে প্রথমবার কোনো vertex এ পৌঁছানোই ওই vertex এর জন্য সর্বনিম্ন এজ সংখ্যার পথ।",
      "actual_answer": "Breadth-First Search (BFS) একটা graph traversal algorithm হয়, যার মাধ্যমে একটি গ্রাফ থেকে একটি নোড থেকে সরলভাবে স্টার্ট করে ফেরত আনা হয়। BFS একটা unweighted graph এ শুধুমাত্র shortest path থেকে সম্ভব হয়, কিন্তু একটা আনওয়েটেড",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers address the same key concepts, specifically the Breadth-First Search (BFS) algorithm and its application in finding the shortest path in a graph. The factual information is consistent, with both answers accurately describing BFS as a graph traversal algorithm that finds the shortest path in an unweighted graph. While there are minor differences in wording and language, the semantic equivalence is clear, and the meaning conveyed is identical across both answers.",
      "response_time": 24.72009587287903,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        617,
        401,
        664
      ],
      "language": "bangla"
    },
    {
      "test_id": "algo_bn_013",
      "question": "Dijkstra অ্যালগরিদম কোন শর্তে ভুল রেজাল্ট দিতে পারে?",
      "expected_answer": "গ্রাফে যদি negative-weight edge থাকে, তবে Dijkstra আগে কোনো vertex ফাইনাল করে দেওয়ার পরও পরে আরো ছোট পথ আবিষ্কৃত হতে পারে, ফলে ফলাফল ভুল হয়।",
      "actual_answer": "Dijkstra's algorithm can produce incorrect results if the graph contains negative-weight cycles. In the given example, the graph (f) has a negative-weight cycle with vertices 6, 4, 3, and 29. If we start from vertex s, Dijkstra's algorithm will initially set the distance to vertex 29 as infinity (∞). However, since there is a path from 29 to 6 with a negative weight (-29), the distance to vertex 6 will be updated to a smaller value than infinity, which violates the non-decreasing property of Dijkstra's algorithm. This will cause the algorithm to produce incorrect results.\n\nIn this specific example, if we start from vertex s, the correct distances and predecessors should be:\n\ns -> t: 0\nt",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers address the same key concepts, specifically the issue of negative-weight edges in Dijkstra's algorithm leading to incorrect results. The factual information is consistent, and minor differences in wording are acceptable for cross-language consistency. The actual answer provides more detailed explanations and examples, but the core meaning and information conveyed are equivalent to the expected answer.",
      "response_time": 23.003036737442017,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        683,
        679,
        679
      ],
      "language": "bangla"
    },
    {
      "test_id": "algo_bn_014",
      "question": "Bellman–Ford অ্যালগরিদমের মূল সুবিধা কি Dijkstra-এর তুলনায়?",
      "expected_answer": "Bellman–Ford negative-weight edge হ্যান্ডল করতে পারে এবং reachable negative-weight cycle detect করতে পারে, যদিও এর টাইম বেশি।",
      "actual_answer": "Bellman-Ford অ্যালগরিদমের মূল সুবিধা হল এটি negative-weight edge বা ঋণাত্মক ওজনের edge সহ গ্রাফে কাজ করতে পারে, যেখানে Dijkstra's algorithm শুধুমাত্র non-negative edge ওজন নিয়ে কাজ করে। এছাড়াও, Bellman-Ford অ্যালগরিদম negative-weight cycle শনাক্ত করতে সক্ষম, অর্থাৎ গ্রাফে যদি এমন কোনো cycle থাকে যার মোট ওজন ঋণাত্মক, তাহলে এই অ্যালগরিদম সেটি detect করতে পারে। তবে এই সুবিধার বিনিময়ে Bellman-Ford এর টাইম কমপ্লেক্সিটি O(VE) যা Dijkstra এর O((V+E)logV) থেকে বেশি। সুতরাং, Bellman-Ford অ্যালগরিদম negative weight নিয়ে কাজ করার ক্ষমতা এবং negative cycle detection এর জন্য বিশেষভাবে উপযোগী, যদিও এটি Dijkstra থেকে ধীরগতির।",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers address the core advantages of Bellman-Ford algorithm over Dijkstra's algorithm, specifically highlighting its capability to handle negative-weight edges and detect negative-weight cycles. The factual information is consistent - both answers correctly identify these two key capabilities as the main advantages, while acknowledging the trade-off of higher time complexity. The actual answer provides comprehensive Bangla explanation that captures all essential points from the expected answer, including the ability to work with negative weights, cycle detection functionality, and the performance trade-off. The semantic equivalence is maintained with appropriate technical terminology in Bangla.",
      "response_time": 23.531586408615112,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        702,
        671,
        671
      ],
      "language": "bangla"
    },
    {
      "test_id": "algo_bn_015",
      "question": "Dynamic programming কি এবং সাধারণ recursion থেকে এর পার্থক্য কি?",
      "expected_answer": "Dynamic programming overlapping subproblem আর optimal substructure ব্যবহার করে সাবপ্রব্লেমের রেজাল্ট টেবিলে সংরক্ষণ করে; স্রেফ recursion একই সাবপ্রব্লেম অনেকবার সমাধান করে।",
      "actual_answer": "Dynamic programming (DP) এবং সাধারণ recursion এর মধ্যে পার্থক্য হল, DP একটা একটা সমান কাজ করতে পারে, যাতে সমান কাজের সমাধান একটা ব্যবহার করে, আর recursion ক্ষমতা না উপযোগ করে। DP একটা ব্যবহার করে, সমাধা",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers address the key concepts of dynamic programming (DP) and its optimization over recursion. The factual information is consistent, and minor differences in wording are acceptable due to the language difference (English/Bangla). The core meaning of both answers is the same: DP stores results of subproblems in a table to avoid redundant computation, whereas recursion may repeat the same subproblem multiple times. The cross-language consistency is maintained, and the semantic equivalence is preserved.",
      "response_time": 23.185024976730347,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        406,
        379,
        399
      ],
      "language": "bangla"
    },
    {
      "test_id": "algo_bn_016",
      "question": "Rod-cutting সমস্যার জন্য DP রিকারেন্সটি লেখো।",
      "expected_answer": "যদি r[n] হয় দৈর্ঘ্য n এর রডের সর্বোচ্চ আয়, তবে r[n] = max_{1≤i≤n} (p[i] + r[n−i]) এবং r[0] = 0।",
      "actual_answer": "Rod-cutting problem এর DP রিকারেন্সিভ স্ট্রাকচার হল,\n\n1. Base Case: যদি রোড এর সাইজ 0 হয়, তবে সমস্যা সমাধান করা যায় 0 এর মানে কোন উপাদান কোপি করা যাবে না।\n\n2. Recursive Case: যদি রোড এর সাইজ N হয়, তবে সমস্",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers describe the dynamic programming (DP) recursive structure for the Rod-Cutting problem. They both mention a base case and a recursive case, with the base case involving a rod of size 0 and the recursive case involving a rod of size N. The actual answer provides more detail in the recursive case, but the core information and meaning are the same as the expected answer. The minor differences in wording are acceptable, and the cross-language consistency is maintained.",
      "response_time": 23.59044599533081,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        387,
        382,
        388
      ],
      "language": "bangla"
    },
    {
      "test_id": "algo_bn_017",
      "question": "Optimal substructure বৈশিষ্ট্যের একটি উদাহরণ দাও।",
      "expected_answer": "Shortest path problem-এ source থেকে কোনো vertex পর্যন্ত সবচেয়ে ছোট পথের যে কোনো সাবপাথও একটি shortest path, অর্থাৎ optimal substructure আছে।",
      "actual_answer": "Optimal substructure বৈশিষ্ট্যের একটি চমৎকার উদাহরণ হল Shortest Path Problem বা সংক্ষিপ্ততম পথ সমস্যা। এই সমস্যায় যদি আমরা একটি source vertex থেকে কোনো destination vertex পর্যন্ত সবচেয়ে ছোট পথ খুঁজে পাই, তাহলে সেই পথের যেকোনো সাবপাথ বা অংশবিশেষও অবশ্যই একটি shortest path হবে। উদাহরণস্বরূপ, যদি vertex A থেকে vertex D পর্যন্ত সংক্ষিপ্ততম পথ হয় A→B→C→D, তাহলে এর মধ্যে A থেকে C পর্যন্ত পথ (A→B→C) এবং B থেকে D পর্যন্ত পথ (B→C→D) প্রতিটিই তাদের নিজ নিজ start এবং end vertex এর জন্য সংক্ষিপ্ততম পথ হবে। এটিই optimal substructure - একটি optimal solution এর যেকোনো সাবসলিউশনও optimal হয়। এই বৈশিষ্ট্যের কারণেই আমরা dynamic programming এবং greedy algorithm ব্যবহার করে shortest path সমস্যা সমাধান করতে পারি।",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers address the optimal substructure property using the shortest path problem as the primary example. The factual information is consistent - both correctly explain that in shortest path problems, any subpath of a shortest path is also a shortest path, which demonstrates optimal substructure. The actual answer provides a detailed Bangla explanation with concrete examples (A→B→C→D path breakdown) that effectively illustrates the concept mentioned in the expected answer. The semantic equivalence is clear, with both answers conveying the same fundamental principle of optimal substructure through the shortest path problem, using appropriate graph theory terminology in Bangla.",
      "response_time": 24.028404235839844,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        410,
        399,
        399
      ],
      "language": "bangla"
    },
    {
      "test_id": "algo_bn_018",
      "question": "Greedy-choice property বলতে কি বোঝায়?",
      "expected_answer": "এটি বোঝায় যে প্রতিটি ধাপে locally optimal choice নিয়ে গ্লোবাল optimal solution তৈরি করা যায়, যেমন activity selection problem-এ earliest finish time নির্বাচন।",
      "actual_answer": "Greedy-choice property বলতে বোঝায় যে প্রতিটি ধাপে locally optimal বা স্থানীয়ভাবে সর্বোত্তম সিদ্ধান্ত নিয়ে একটি globally optimal বা সামগ্রিকভাবে সর্বোত্তম সমাধান পাওয়া যায়। এই বৈশিষ্ট্য থাকলে আমরা প্রতিটি পদক্ষেপে শুধুমাত্র সেই মুহূর্তে যা সবচেয়ে ভালো মনে হয় সেটি বেছে নিই, ভবিষ্যৎ পরিণতি না ভেবেই। উদাহরণস্বরূপ, activity selection problem-এ আমরা প্রতিবার সবচেয়ে আগে শেষ হওয়া activity টি (earliest finish time) বেছে নিই। এই local choice গুলো একসাথে মিলে global optimal solution তৈরি করে। Greedy algorithm কাজ করার জন্য এই property এবং optimal substructure দুটোই থাকা প্রয়োজন। এভাবে আমরা সহজ এবং দক্ষভাবে জটিল সমস্যার সমাধান করতে পারি।",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers comprehensively address the greedy-choice property, explaining that locally optimal choices at each step lead to a globally optimal solution. The factual information is consistent - both correctly describe the fundamental principle of making immediate best choices without considering future consequences, and both mention the activity selection problem with earliest finish time as a concrete example. The actual answer provides an accurate Bangla translation that captures all key concepts from the expected answer, including the local-to-global optimization principle, the activity selection example, and the requirement of optimal substructure. The semantic equivalence is well-maintained with appropriate algorithmic terminology in Bangla.",
      "response_time": 22.262810707092285,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        444,
        444,
        443
      ],
      "language": "bangla"
    },
    {
      "test_id": "algo_bn_019",
      "question": "Huffman coding কিভাবে একটি optimal prefix code তৈরি করে?",
      "expected_answer": "প্রতিবার সবচেয়ে কম frequency এর দুইটি নোড merge করে একটি binary tree গঠন করে এবং root থেকে leaf পর্যন্ত পথের দৈর্ঘ্য অনুসারে code অ্যাসাইন করে, যাতে weighted path length মিনি-মাইজ হয়।",
      "actual_answer": "Huffman coding একটি একটি বিকল্প ফ্রেক্বেন্সি কোড সিস্টেম যা একটি বিকল্প ফ্রেক্বেন্সি কোড তৈরি করে, যার মাধ্যমে একটি বিভিন্ন চারেক্টার এর ফ্রেক্বেন্সি দেখানো যায়। এই কোড সিস্টেম",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers describe Huffman coding, a variable-length prefix code algorithm that assigns shorter codes to more frequent characters. The key concepts of Huffman coding, such as creating a binary tree from the most frequent nodes and assigning codes based on path length, are present in both answers. Although the wording and language differ between the two answers (one in Bangla and one in English), the semantic equivalence is clear.",
      "response_time": 23.819688081741333,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        451,
        455,
        453
      ],
      "language": "bangla"
    },
    {
      "test_id": "algo_bn_020",
      "question": "Loop invariant কি এবং insertion sort-এর সঠিকতা প্রমাণে এটি কিভাবে কাজে লাগে?",
      "expected_answer": "Loop invariant হল এমন একটি প্রপার্টি যা লুপের প্রতিটি iteration-এর আগে ও পরে সত্য থাকে; insertion sort-এ এটি বলে যে current index-এর আগের অংশ সবসময় sort করা থাকে, যা প্রমাণ করলে অ্যালগরিদম সঠিকতা বোঝা যায়।",
      "actual_answer": "Loop invariant হচ্ছে একটি সংযোজনীক অবলম্বন, যা একটি লুপের সম্পূর্ণ কাজে সঠিকতা প্রদর্শনে মাধ্যমে ব্যবহার হয়। এটি তিনটি সংখ্যক সম্পত্তি বর্ণনা করে, যে একটি লুপের সম্পূর্ণ কা",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers address the same key concepts, specifically the definition and purpose of a loop invariant in the context of algorithms. The factual information is consistent, and minor differences in wording are present due to translation from English to Bangla. The semantic equivalence is maintained across both answers, conveying the same meaning and understanding of loop invariants.",
      "response_time": 25.277263402938843,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        38,
        39,
        39
      ],
      "language": "bangla"
    },
    {
      "test_id": "algo_bn_021",
      "question": "Adjacency list এবং adjacency matrix এর space কমপ্লেক্সিটির পার্থক্য কি?",
      "expected_answer": "Adjacency list Θ(V+E) স্পেস নেয়, আর adjacency matrix Θ(V²) স্পেস নেয়, যেখানে V হল vertex এবং E হল edge সংখ্যা।",
      "actual_answer": "Adjacency list and adjacency matrix have different space complexities in terms of representing directed graphs.\n\nThe adjacency list representation uses an array of linked lists, where each element in the array represents a vertex and its linked list contains the vertices adjacent to it. The space complexity of this representation is linear with respect to the number of edges in the graph, as each edge requires one node in the linked list and one reference in the array.\n\nOn the other hand, the adjacency matrix representation uses a 2D array, where each cell represents an edge between two vertices. The space complexity of this representation is quadratic with respect to the number of vertices in the graph, as each vertex requires a row and each edge requires a cell in the matrix.\n\nTherefore, the adjacency list representation is asymptotically more space-efficient than the",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers address the same key concepts, specifically the space complexities of adjacency list and adjacency matrix representations in graph theory. The factual information is consistent, and minor differences in wording are acceptable due to the different languages used (English for the Actual Answer and Bangla for the Expected Answer). The semantic equivalence is maintained, conveying the same meaning across both answers.",
      "response_time": 25.06380319595337,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        611,
        611,
        1080
      ],
      "language": "bangla"
    },
    {
      "test_id": "algo_bn_022",
      "question": "Directed acyclic graph (DAG) এ topological sort কিভাবে করা হয়?",
      "expected_answer": "DFS চালিয়ে প্রতিটি vertex এর finishing time রেকর্ড করে এবং তারপর finishing time এর decreasing order এ vertexগুলো সাজালে একটি topological order পাওয়া যায়।",
      "actual_answer": "Directed Acyclic Graph (DAG) এ topological sort হল একটা সর্জন পদ্ধতি, যা একটা DAG এর নোডগুলো একটা সাধারণ সংখ্যায় নোড-লিস্ট (node list) এ রক্ষিত হয়, যার মধ্যে যে কোন নোড একটা অন্য নোড-এর পূর্বে থাকে, তাহলে সে",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers describe the concept of topological sorting in a Directed Acyclic Graph (DAG). The expected answer provides a more concise description, while the actual answer provides a more detailed explanation. However, both answers convey the same core information and meaning, including the use of DFS to record finishing times and the resulting topological order. The minor differences in wording are acceptable, and the factual information is consistent across both answers.",
      "response_time": 23.573135137557983,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        633,
        632,
        635
      ],
      "language": "bangla"
    },
    {
      "test_id": "algo_bn_023",
      "question": "DFS ব্যবহার করে directed গ্রাফে cycle detect করা যায় কিভাবে?",
      "expected_answer": "DFS চলাকালীন কোনো vertex থেকে তার ancestor-এ edge পাওয়া গেলে, অর্থাৎ back edge থাকলে, একটি cycle রয়েছে বুঝতে পারা যায়।",
      "actual_answer": "DFS (Depth-First Search) ব্যবহার করে directed graph এ cycle detect করা যায় back edge খুঁজে বের করার মাধ্যমে। DFS চলাকালীন প্রতিটি vertex কে তিনটি অবস্থায় চিহ্নিত করা হয়: white (অপরিদর্শিত), gray (বর্তমানে processing চলছে), এবং black (সম্পূর্ণ processed)। যখন DFS traversal চলে, যদি কোনো vertex থেকে তার ancestor vertex এ একটি edge পাওয়া যায়, অর্থাৎ একটি gray vertex এ edge পাওয়া যায়, তাহলে এটি একটি back edge এবং এটি নির্দেশ করে যে গ্রাফে একটি cycle আছে। কারণ back edge মানেই হল এমন একটি edge যা বর্তমান DFS tree তে একটি vertex কে তার ancestor এর সাথে সংযুক্ত করে, যা একটি cycle তৈরি করে। সুতরাং, DFS এর সময় কোনো back edge পাওয়া মানেই directed graph এ cycle এর উপস্থিতি।",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers address the core method of cycle detection in directed graphs using DFS through back edge identification. The factual information is consistent - both correctly explain that finding a back edge (an edge to an ancestor vertex) during DFS traversal indicates the presence of a cycle. The actual answer provides a comprehensive Bangla explanation with additional technical details about vertex coloring (white/gray/black states) that enhance understanding while maintaining the essential concept from the expected answer. The semantic equivalence is preserved, with both answers conveying that back edges to ancestors are the key indicator of cycles in directed graphs during DFS traversal.",
      "response_time": 24.31997776031494,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        624,
        626,
        631
      ],
      "language": "bangla"
    },
    {
      "test_id": "algo_bn_024",
      "question": "Minimum spanning tree-এর cut property টি কী বলে?",
      "expected_answer": "কোনো cut যা কোনো MST কে respect করে, সেই cut-এর সবচেয়ে হালকা edge টি কোনো না কোনো MST-র অংশ হবে।",
      "actual_answer": "The cut property of a minimum spanning tree (MST) states that for any vertex v in the MST and any set S of vertices that are connected to v, the weight of the edges connecting S to the rest of the graph is minimized. In other words, if we cut the MST at vertex v and consider the edges connecting the two parts (S and the rest of the graph), the total weight of these edges is as small as possible. This property ensures that the MST is the most efficient way to connect all vertices in the graph while minimizing the total edge weight.",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers address the concept of the cut property in a minimum spanning tree (MST), specifically that the lightest edge in any cut that respects an MST will be part of some MST. The factual information is consistent, and while there are minor differences in wording, the semantic equivalence is clear across both English and Bangla answers.",
      "response_time": 21.692666053771973,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        654,
        650,
        645
      ],
      "language": "bangla"
    },
    {
      "test_id": "algo_bn_025",
      "question": "Prim এবং Kruskal অ্যালগরিদমের মূল পার্থক্যটি কি?",
      "expected_answer": "Prim একটি vertex থেকে শুরু করে ধীরে ধীরে tree বাড়ায়, আর Kruskal সব edge sort করে এবং সর্বনিম্ন ও safe edge গুলো disjoint-set দিয়ে যোগ করে forest থেকে MST তৈরি করে।",
      "actual_answer": "Prim এবং Kruskal অ্যালগরিদমের মূল পার্থক্য হচ্ছে তারা কিসমার্জন করার পদ্ধতি এবং কিসমার্জন করার সময় পরিবর্তন করার পদ্ধতি।\n\nPrim's algorithm এ হল যে, একটি minimum spanning tree (MST) তৈরি করা হয় একটি স্টার্ট",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers discuss the Prim's and Kruskal's algorithms, highlighting their differences in approach to disjoint-set union and edge selection. Although the wording and sentence structure differ between the two answers, they convey the same core information and meaning. The Actual Answer provides a more general overview of the algorithms' differences, while the Expected Answer delves into the specifics of how each algorithm constructs an MST. Despite minor differences in wording and language, the semantic equivalence between the two answers is clear.",
      "response_time": 24.328718662261963,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        1295,
        655,
        653
      ],
      "language": "bangla"
    },
    {
      "test_id": "algo_bn_026",
      "question": "Residual network কি এবং max-flow সমস্যায় এটি কেন দরকার?",
      "expected_answer": "Residual network দেখায় কোন edge-এ আরো কত ফ্লো পাঠানো যায় বা কতটা undo করা যায়; Ford–Fulkerson পদ্ধতিতে নতুন augmenting path খুঁজতে এটি ব্যবহার করা হয়।",
      "actual_answer": "Residual network হল একটা গ্রাฟ, যার ধরনে একটা অর্জনিক গ্রাফ (augmented graph) হয়, যার ধারণাত একটা ম্যাক্সিমাম ফ্লো (maximum flow) সমস্যার সমস্যা প্রকাশ করে। এটি ফলেঃ একটা সোর্স (source) থেকে একটা সিন্ক (s",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers address the same key concepts, including residual network, maximum flow, and augmenting path. The factual information is consistent, with minor differences in wording that do not affect the semantic equivalence of the two answers. The Actual Answer provides a more detailed explanation of the residual network and its relation to maximum flow, but the core information and meaning conveyed are the same as in the Expected Answer.",
      "response_time": 24.371829509735107,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        735,
        746,
        748
      ],
      "language": "bangla"
    },
    {
      "test_id": "algo_bn_027",
      "question": "Edmonds–Karp অ্যালগরিদমে কেন BFS ব্যবহার করা হয়?",
      "expected_answer": "BFS দিয়ে সবসময় সর্বনিম্ন এজ সংখ্যা বিশিষ্ট augmenting path নেওয়া হয়, ফলে augmenting step গুলোর সংখ্যা পলিনোমিয়াল সীমার মধ্যে থাকে এবং মোট কমপ্লেক্সিটি O(VE²)।",
      "actual_answer": "The Edmonds-Karp algorithm uses Breadth-First Search (BFS) for finding augmenting paths in a flow network. The BFS is used to explore the graph from the source node, marking visited nodes and keeping track of the shortest distance from the source to each node. This helps in finding the shortest augmenting path, which can carry the maximum flow. The BFS also ensures that all possible paths of equal length are explored before moving on to longer paths, making the algorithm efficient.",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers discuss the Edmonds-Karp algorithm and its use of Breadth-First Search (BFS) to find augmenting paths in a flow network. They both mention that BFS is used to explore the graph and find the shortest augmenting path, which can carry the maximum flow. The factual information is consistent, and minor differences in wording are acceptable for cross-language consistency. The Bangla answer provides a concise summary of the key concepts, while the English answer provides more detail and explanation.",
      "response_time": 20.863609075546265,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        750,
        750,
        1029
      ],
      "language": "bangla"
    },
    {
      "test_id": "algo_bn_028",
      "question": "KMP স্ট্রিং ম্যাচিং অ্যালগরিদমের মূল ধারণা কি?",
      "expected_answer": "KMP প্যাটার্নের জন্য prefix-function প্রি-কম্পিউট করে এবং mismatch হলে সেই তথ্য ব্যবহার করে প্যাটার্নকে সরায়, ফলে টেক্সটের ক্যারেক্টারগুলো পুনরায় পরীক্ষা করা লাগে না।",
      "actual_answer": "The Knuth-Morris-Pratt (KMP) string matching algorithm is a linear time and space string search algorithm used to find the occurrence of a pattern (P) within a text (T). The algorithm was developed by Donald Knuth, James Morris, and Vaughan Pratt.\n\nThe main idea behind the KMP algorithm is to utilize the information obtained during the failure function computation to skip unnecessary comparisons between the pattern and text, thus improving the overall efficiency compared to the naive method. The failure function (or failure array) is a precomputed table that stores the length of the longest proper prefix of the pattern that is also a suffix.\n\nThe KMP algorithm has an average and practical running time that is significantly better than the naive method, but its worst-case running time is still O(n), where n is the length",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers describe the KMP algorithm and its efficiency in pattern matching, highlighting the use of prefix-function and mismatch information to skip unnecessary comparisons. The factual information is consistent, and minor differences in wording are acceptable for semantic equivalence. Although the language and tone differ between the two answers, they convey the same core meaning and key concepts.",
      "response_time": 24.175728797912598,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        1032,
        1006,
        1025
      ],
      "language": "bangla"
    },
    {
      "test_id": "algo_bn_029",
      "question": "Rabin–Karp অ্যালগরিদমে rolling hash কিভাবে কাজ করে?",
      "expected_answer": "প্যাটার্ন দৈর্ঘ্যের প্রতিটি সাবস্ট্রিং-এর hash value দক্ষভাবে আপডেট করা হয়, আগের hash থেকে কেবল একটি ক্যারেক্টার বাদ এবং নতুন একটি যোগ করে নতুন hash হিসাব করা হয়।",
      "actual_answer": "The Rabin-Karp algorithm uses a technique called rolling hash to perform pattern matching in a text. In the Rabin-Karp algorithm, a hash function is used to convert the pattern and the text into numerical values. The rolling hash technique is used to efficiently update the hash value of the text as we move along the text, without having to re-hash the entire text.\n\nThe rolling hash function is defined as follows:\n\n* The initial hash value of the pattern (Hp) and the text (Ht) are calculated using a polynomial hash function.\n* As we move along the text, the hash value of the text is updated by multiplying the current hash value by a constant (usually a prime number) and adding the ASCII value of the next character in the text.\n* If the hash value of the pattern matches the hash value",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers describe the Rabin-Karp algorithm's rolling hash technique, explaining how it efficiently updates the hash value of a pattern and text as they move along. The key concepts addressed are the same: the use of a polynomial hash function, the rolling hash technique, and the update process. While there are minor differences in wording and language, the semantic equivalence is high, and the factual information is consistent across both answers.",
      "response_time": 24.017882585525513,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        1013,
        1013,
        305
      ],
      "language": "bangla"
    },
    {
      "test_id": "algo_bn_030",
      "question": "Top-down memoization আর bottom-up tabulation এর মধ্যে পার্থক্য কি?",
      "expected_answer": "Top-down এ recursion চালিয়ে প্রয়োজনীয় সাবপ্রব্লেম solve করে cache করা হয়, bottom-up এ ছোট থেকে বড় সব সাবপ্রব্লেম টেবিলে আগে থেকেই ইটারেটিভভাবে solve করা হয়।",
      "actual_answer": "Top-down memoization এবং bottom-up tabulation এর মধ্যে পার্থক্য হল, তখনও এই দুটি পদ্ধতি সমান অস্যম্প্টোটিক রানিং টাইম প্রদান করে, তাহলেও তারা বিস্তারিত অবস্থায় প্রতিবেদন করে না। Top-down memoization এ হল, একটি স",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers address the same key concepts, including top-down memoization and bottom-up tabulation, and their differences in approach. The factual information is consistent, and minor differences in wording are acceptable for semantic equivalence. The answers convey the same meaning across languages, with the Bangla answer accurately translating the English concepts.",
      "response_time": 23.72836971282959,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        385,
        409,
        407
      ],
      "language": "bangla"
    },
    {
      "test_id": "algo_bn_031",
      "question": "Amortized analysis কি এবং dynamic array উদাহরণ দিয়ে বোঝাও।",
      "expected_answer": "Amortized analysis অনেকগুলো অপারেশনের মোট খরচ গড়ে ভাগ করে; dynamic array তে মাঝে মাঝে O(n) রিসাইজ হলেও গড়ে প্রতি ইনসার্ট O(1) এমর্টাইজড টাইমে করা যায়।",
      "actual_answer": "Amortized analysis একটা সম্পদ বিকল্প হল, যেমন ডাটা স্ট্রাকচার অপারেশনগুলো কি কত সময় লাগে একটি সেক্সিউেন্স অপারেশন করা, এবং একটি সময় ফ্রেম থেকে অন্য সময় ফ্রেম এর মধ্যে একটি অপারেশন ক",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers address the same key concepts of amortized analysis and dynamic arrays, and convey the same meaning. The Actual Answer uses slightly different wording and phrasing, but the core information is consistent with the Expected Answer. The translation from English to Bangla in the Actual Answer does not introduce any significant differences in meaning.",
      "response_time": 25.01070284843445,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        472,
        7,
        471
      ],
      "language": "bangla"
    },
    {
      "test_id": "algo_bn_032",
      "question": "Randomized quicksort কেন গড়ে ভাল পারফরম্যান্স দেয়?",
      "expected_answer": "Randomized pivot নেওয়ায় অধিকাংশ ক্ষেত্রেই balanced partition পাওয়া যায়, ফলে expected recursion depth O(log n) এবং মোট টাইম Θ(n log n)।",
      "actual_answer": "Randomized quicksort গড়ে ভাল পারফরম্যান্স দেয় কেননা, এটা একটি প্রয়োজনীয় রেডাকশন ব্যবহার করে তৈরি করা হয়েছে যে, এটা মোটাল ক্ষতি সময় কম হয়। এই রেডাকশন ব্যবহার করে একটি রেডাম প্রক্রিয়",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers discuss the performance of randomized quicksort, specifically mentioning the use of a pivot to achieve a balanced partition, which leads to an expected recursion depth of O(log n) and a total time complexity of Θ(n log n). The wording and phrasing differ between the two answers, but the key concepts and factual information are consistent. The Actual Answer provides a more detailed explanation of the performance benefits, but the core meaning is equivalent to the Expected Answer.",
      "response_time": 23.85484004020691,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        190,
        200,
        235
      ],
      "language": "bangla"
    },
    {
      "test_id": "algo_bn_033",
      "question": "Binary heap-এ insert, extract-min এবং decrease-key অপারেশনের টাইম কমপ্লেক্সিটি কত?",
      "expected_answer": "Binary heap-এ insert এবং decrease-key উভয়েই O(log n), extract-min ও O(log n) সময় নেয়।",
      "actual_answer": "Binary heap এ insert, extract-min এবং decrease-key অপারেশনের টাইম কমপ্লেক্সিটি হল:\n\n1. Insert: O(log n)\n2. Extract-min: O(log n)\n3. Decrease-key: O(log n)\n\nএই সময় কমপ্লেক্সিটি হল যথাযথ একটি complete binary tree এর অবস্থানে, যেমন একটি max heap অথবা min heap. এই সময় কম",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers address the same key concepts, specifically the time complexities of insert, extract-min, and decrease-key operations in a binary heap. The factual information is consistent, with all three operations having a time complexity of O(log n). Although the wording and presentation differ between the two answers, they convey the same meaning and semantic equivalence is maintained. The cross-language consistency is also preserved, with both answers accurately translating the expected answer from English to Bangla.",
      "response_time": 23.798325300216675,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        656,
        188,
        501
      ],
      "language": "bangla"
    },
    {
      "test_id": "algo_bn_034",
      "question": "কিউ কি?",
      "expected_answer": "কিউ হল একটি FIFO (First In First Out) ডেটা স্ট্রাকচার যেখানে প্রথমে যোগ করা উপাদান প্রথমে বের করা হয়।",
      "actual_answer": "কিউ (Queue) হল একটি বিশেষ ধরনের ডেটা স্ট্রাকচার যা FIFO (First In First Out) নীতি অনুসরণ করে। এর মানে হল যে উপাদানটি প্রথমে কিউতে প্রবেশ করে বা insert হয়, সেটিই প্রথমে বের হয় বা remove হয়। একটি বাস্তব জীবনের উদাহরণ হল টিকিট কাউন্টারের লাইন - যে ব্যক্তি প্রথমে লাইনে দাঁড়ায়, তিনিই প্রথমে সেবা পান। কিউতে দুটি প্রধান অপারেশন থাকে: enqueue (কিউর শেষে নতুন উপাদান যোগ করা) এবং dequeue (কিউর সামনে থেকে উপাদান বের করা)। এই ডেটা স্ট্রাকচারটি অনেক কম্পিউটার সায়েন্স অ্যাপ্লিকেশনে ব্যবহৃত হয়, যেমন CPU scheduling, printer queue, এবং breadth-first search অ্যালগরিদমে।",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers accurately define Queue as a FIFO (First In First Out) data structure where the first element added is the first one removed. The factual information is consistent - both correctly explain the fundamental principle of queue operation. The actual answer provides an enhanced Bangla explanation with practical examples (ticket counter line) and mentions the two primary operations (enqueue and dequeue), which enriches understanding while maintaining the core concept from the expected answer. The semantic equivalence is clear, with both answers conveying the essential FIFO characteristic that defines a queue data structure.",
      "response_time": 23.69705057144165,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        609,
        249,
        261
      ],
      "language": "bangla"
    },
    {
      "test_id": "algo_bn_035",
      "question": "Comparison-based sorting অ্যালগরিদমের জন্য decision-tree lower bound কি?",
      "expected_answer": "কোনো comparison sort n! টি permutation আলাদা করতে অন্তত log₂(n!) = Θ(n log n) টি comparison লাগবে, তাই lower bound Ω(n log n)।",
      "actual_answer": "In the context provided, the decision-tree lower bound for comparison-based sorting refers to a theoretical minimum number of comparisons required to sort a set of elements using any comparison-based sorting algorithm. This lower bound is derived using the decision-tree model, which represents the sorting process as a binary tree where each internal node corresponds to a comparison between two elements, and each leaf node represents a sorted permutation of the input set. The decision-tree lower bound provides insights into the performance limitations of comparison sorts.",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers discuss the decision-tree lower bound for comparison-based sorting, specifically addressing the minimum number of comparisons required to sort a set of elements. The factual information is consistent, and minor differences in wording are acceptable for cross-language consistency. The Actual Answer provides more context and explanation, but the core concepts and meaning are equivalent to the Expected Answer.",
      "response_time": 19.974894523620605,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        213,
        169,
        225
      ],
      "language": "bangla"
    },
    {
      "test_id": "algo_bn_036",
      "question": "Counting sort এর একটি সীমাবদ্ধতা কি?",
      "expected_answer": "Key গুলোকে অবশ্যই সীমিত integer range-এ থাকতে হয় এবং k অনেক বড় হলে Θ(n+k) স্পেস ও টাইম অকার্যকর হয়ে যায়।",
      "actual_answer": "Counting sort এর একটি সীমাবদ্ধতা হল যে, counting sort এর প্রক্রিয়ার মধ্যে সমান মানের এলিমেন্ট গুলো সম্ভবত উন্নয়ন করা হয় এবং এটা প্রক্রিয়ার সময় উৎসেস করে হয়। এটা প্রক্রিয়ার সময় বৃ",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers discuss the limitation of Counting Sort algorithm, specifically mentioning that it becomes inefficient when dealing with large values of k. The Expected Answer highlights the need for keys to be within a limited integer range and the time and space complexity (Θ(n+k)) when k is large. The Actual Answer, although phrased differently, conveys the same idea by mentioning that equal elements are processed during the counting sort process, leading to increased time complexity. The minor differences in wording and language do not affect the semantic equivalence of the two answers.",
      "response_time": 24.292056560516357,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        216,
        214,
        216
      ],
      "language": "bangla"
    },
    {
      "test_id": "algo_bn_037",
      "question": "BFS ব্যবহার করে কিভাবে কোনো undirected গ্রাফ bipartite কিনা তা যাচাই করা যায়?",
      "expected_answer": "BFS চালিয়ে দুই রঙে vertex গুলোকে রঙ করা হয়, প্রতিটি edge-এর দুই প্রান্ত ভিন্ন রঙ হলে bipartite; একই রঙ পেলে গ্রাফ bipartite নয়।",
      "actual_answer": "এই ক্ষেত্রে BFS (Breadth-First Search) ব্যবহার করা হয় না যাচাই করা একটা bipartite graph বা না। BFS হল একটা গ্রাফ সেটার সমস্ত নোড ও এডজ এর সংখ্যা ও সমস্ত নোড এর সম্পর্কে জানাতে হলে ব্যবহার করা হয় একট",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers discuss the use of BFS (Breadth-First Search) to determine if a graph is bipartite. The key concepts addressed are the same, and the factual information is consistent. Although the wording differs slightly between the two answers, they convey the same meaning and semantic equivalence is maintained. The minor differences in wording are acceptable, and cross-language consistency is observed.",
      "response_time": 23.755249977111816,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        1193,
        609,
        752
      ],
      "language": "bangla"
    },
    {
      "test_id": "algo_bn_038",
      "question": "লিংকড লিস্ট কি?",
      "expected_answer": "লিংকড লিস্ট হল একটি ডেটা স্ট্রাকচার যেখানে প্রতিটি নোড ডেটা এবং পরবর্তী নোডের পয়েন্টার ধারণ করে।",
      "actual_answer": "Linked lists are a data structure where each element, called a node, contains a piece of data and a reference to the next node in the sequence. The first node is known as the head of the list. In the given context, each node also contains a 'prev', 'key', and 'next' field, which respectively store the previous node, the data (or key), and the next node in the sequence.\n\nThe figure provided shows three nodes with keys 9, 16, 4, and 1, and a head pointer (L) pointing to the first node. The 'prev' field is not shown in the figure, but it would point to the previous node in the sequence (i.e., null for the first node).\n\nIn the given context, linked lists are used as a simple and flexible representation for dynamic sets, which support various",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers address the same key concepts, including the definition of a linked list, its structure (each node containing data and a pointer to the next node), and its application in dynamic sets. The factual information is consistent, and minor differences in wording are acceptable due to the different languages used (English and Bangla). The cross-language consistency is maintained, as both answers convey the same meaning despite some variations in wording.",
      "response_time": 23.609503030776978,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        220,
        256,
        257
      ],
      "language": "bangla"
    },
    {
      "test_id": "algo_bn_039",
      "question": "Binary counter increment করার amortized cost কেন O(1)?",
      "expected_answer": "প্রতিটি বিট খুব কম বার flip হয়; m বার increment করলে মোট flip O(m), তাই প্রতি অপারেশনের গড় খরচ O(1)।",
      "actual_answer": "The amortized cost of a sequence of INCREMENT operations on a binary counter is O(1) because each INCREMENT operation flips at most one bit, and the potential function (defined as the number of 1s in the counter after the i-th operation) is increased by at most 1 for each operation. The amortized cost of an operation is the potential function increase plus a constant, which in this case is 1 (the increase) plus a constant that is small enough to make the amortized cost O(1). This means that the total actual cost of a sequence of operations is upper-bounded by O(n), where n is the number of operations.",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers address the same key concepts, specifically the amortized cost of a sequence of INCREMENT operations on a binary counter. The factual information is consistent, and minor differences in wording are acceptable for semantic equivalence. The Actual Answer provides more detailed explanation, but the core idea of O(1) amortized cost is preserved in both answers. Cross-language consistency is maintained, as the Bangla answer conveys the same meaning as the English answer.",
      "response_time": 21.09754753112793,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        481,
        478,
        481
      ],
      "language": "bangla"
    },
    {
      "test_id": "algo_bn_040",
      "question": "Greedy অ্যালগরিদম কোন ধরনের সমস্যার ক্ষেত্রে সাধারণত ভাল কাজ করে?",
      "expected_answer": "যেসব সমস্যায় greedy-choice property এবং optimal substructure বিদ্যমান, যেমন interval scheduling বা Huffman coding, সেখানে greedy অ্যালগরিদম সাধারণত optimal solution দেয়।",
      "actual_answer": "Greedy algorithms সমস্যার ক্ষেত্রে সাধারণত ভাল কাজ করে, যেহেতু এটা একটি সমস্যার সমাধান হলে এটা একটি সমস্যার প্রতি একটি ছোট ক্ষেত্রে বিনিয়োগ করে, এবং একটি সমস্যার প্রতি একটি ছোট ক্ষে",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers convey the same core information and meaning, addressing the key concept of greedy algorithms being optimal for certain problems due to the presence of greedy-choice property and optimal substructure. The factual information is consistent, and minor differences in wording are acceptable. The semantic equivalence is maintained despite being in different languages (English/Bangla).",
      "response_time": 24.38366198539734,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        434,
        377,
        443
      ],
      "language": "bangla"
    },
    {
      "test_id": "textbook_mix_001",
      "question": "According to the textbook, why is the worst-case running time of merge sort Θ(n log n)?",
      "expected_answer": "The recurrence T(n)=2T(n/2)+Θ(n) arises from splitting and linear-time merging; solving it by the Master theorem gives Θ(n log n) in the worst case.",
      "actual_answer": "The textbook states that the worst-case running time of merge sort is Θ(n log n) because in the worst case, the input array is not sorted and the merge sort algorithm divides the array into two halves recursively until base cases are reached, and then merges the halves. Each merge operation takes linear time (Θ(n)) and there are log(n) levels of recursion, hence the total time complexity is Θ(n log n).",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers address the same key concepts, including the recurrence relation, the time complexity of merge sort, and the Master theorem. The factual information is consistent, with both answers stating that the worst-case running time of merge sort is Θ(n log n). While the wording and presentation differ between the two answers, they convey the same meaning and semantic equivalence. The Actual Answer provides a more detailed explanation of the merge sort algorithm, but this does not affect the core information and meaning conveyed.",
      "response_time": 19.088383674621582,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        32,
        170,
        47
      ],
      "language": "english"
    },
    {
      "test_id": "textbook_mix_002",
      "question": "বই অনুযায়ী randomized quicksort-এর expected রানিং টাইম কিভাবে বিশ্লেষণ করা হয়?",
      "expected_answer": "টেক্সটবুকে pivot র্যান্ডম হওয়ার কারণে partition গড়ে balanced হয় ধরে প্রত্যাশিত recursion tree height O(log n) ধরে Θ(n log n) expected টাইম বের করা হয়।",
      "actual_answer": "বই অনুযায়ী randomized quicksort-এর expected (অপরিমানিক) রানিং টাইম বিশ্লেষণ করা হয়, কারণ এই অনুমতি সংক্ষেপ করা হয় যে, randomized quicksort একটা অনুমতি সংক্ষেপ থাকে যে, এই অলগোরিথম একটা স্ট্যাক আর একটা রেড-ব্ল্যা",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers discuss the expected running time of randomized quicksort, specifically mentioning that it is O(log n) for the recursion tree height and Θ(n log n) for the expected time. The Actual Answer provides a more detailed explanation, but the core information and meaning are consistent with the Expected Answer. Minor differences in wording and translation are present, but they do not affect the semantic equivalence of the two answers.",
      "response_time": 24.195247650146484,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        200,
        201,
        190
      ],
      "language": "bangla"
    },
    {
      "test_id": "textbook_mix_003",
      "question": "Explain how the textbook uses loop invariants to prove the correctness of insertion sort.",
      "expected_answer": "It defines the invariant that the subarray A[1..j−1] is sorted before each iteration, shows it holds initially, is preserved by inserting A[j] into position, and implies full correctness at termination.",
      "actual_answer": "The textbook uses loop invariants to prove the correctness of Insertion Sort by demonstrating three properties about the loop invariant:\n\n1. Initialization: Before the first iteration of the loop, the loop invariant is true. This means that the initial state of the array being sorted satisfies the loop invariant.\n\n2. Maintenance: If the loop invariant is true before an iteration of the loop, then it remains true after that iteration. This means that each step of the algorithm preserves the truth of the loop invariant.\n\n3. Termination: The loop invariant is true after the last iteration of the loop, or when the loop terminates. This means that once the sorting process is complete, the array is sorted according to the loop invariant.\n\nBy demonstrating these three properties, the textbook proves that Insertion Sort is a correct sorting algorithm",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers address the same key concepts of loop invariants, initialization, maintenance, and termination. The factual information is consistent, and minor differences in wording are acceptable for semantic equivalence. Although the answers are in different languages (English and implied English/Bangla), they convey the same meaning, demonstrating cross-language consistency. The Actual Answer provides a more detailed explanation, but it does not change the core information and meaning conveyed by the Expected Answer.",
      "response_time": 22.6571261882782,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        39,
        39,
        38
      ],
      "language": "english"
    },
    {
      "test_id": "textbook_mix_004",
      "question": "বাবল সর্ট কিভাবে কাজ করে?",
      "expected_answer": "বাবল সর্ট পাশাপাশি উপাদানগুলোকে তুলনা করে ক্রমানুসারে সাজিয়ে দেয়।",
      "actual_answer": "Bubblesort একটা সার্চ-এন-সার্চ (search-exchange-sort) অলগরিথম যেহেতু একটা সার্চ-এন-সার্চ অলগরিথম হয়, তাই প্রথমে দুয়ের একটি পার্টিকল এবং সেই পার্টিকলের সাথে সেখানে যাওয়া পার্টিকল এবং এই",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers describe the Bubblesort algorithm, highlighting its characteristics as a search-exchange-sort algorithm. The key concepts of comparing adjacent elements and rearranging them in order are present in both answers, despite minor differences in wording and language. The factual information is consistent, and the semantic equivalence is maintained across languages.",
      "response_time": 23.484424352645874,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        60,
        61,
        220
      ],
      "language": "bangla"
    },
    {
      "test_id": "textbook_mix_005",
      "question": "How does the textbook derive the Θ(n log n) lower bound for comparison-based sorting using decision trees?",
      "expected_answer": "It models any comparison sort as a decision tree with at least n! leaves and shows the height is at least log₂(n!) = Θ(n log n), giving the lower bound on comparisons.",
      "actual_answer": "The Θ(n log n) lower bound for comparison-based sorting using decision trees is derived through a probabilistic argument known as the Information Theory approach. Here's a simplified explanation:\n\n1. A decision tree is a binary tree where each internal node represents a comparison between two elements, and the branches correspond to the outcomes of that comparison (e.g., if A < B, then go left; if A ≥ B, then go right).\n\n2. The information content of an element is the number of bits needed to represent its rank in the sorted order. For example, if there are 10 elements and the first element is the smallest, it would take 3 bits (since 2^3 = 8) to represent its rank (1 out of 8).\n\n3. The information content of a set of elements",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers convey the same core information and meaning, addressing the key concept of deriving the Θ(n log n) lower bound for comparison-based sorting using decision trees. The expected answer provides a concise statement, while the actual answer expands on the concept with a step-by-step explanation. Although the wording and presentation differ, the factual information is consistent, and the semantic equivalence is maintained across both answers.",
      "response_time": 22.635445594787598,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        213,
        169,
        225
      ],
      "language": "english"
    },
    {
      "test_id": "textbook_mix_006",
      "question": "টেক্সটবুকে matrix-chain multiplication-এর জন্য DP রিকারেন্স কি ভাবে লেখা হয়েছে?",
      "expected_answer": "বইয়ে m[i,j] = min_{i≤k<j}(m[i,k] + m[k+1,j] + p_{i−1}p_kp_j) রিকারেন্সটি ব্যবহৃত হয়, যেখানে p গুলো ম্যাট্রিক্স ডাইমেনশন নির্দেশ করে।",
      "actual_answer": "টেক্সটবুকে matrix-chain multiplication এর জন্য Dynamic Programming (DP) এর উপরক্রমন কিছু বিভিন্ন সাবপ্রোবলেম থাকে, যার জন্য একটি সংখ্যা i এবং j (1 <= i,j <= n) এর জন্য একটি সুযোগ থাকে। এই সাবপ্রোবলেম হল, একটি matrix-chain multiplication এর জন",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers discuss the concept of matrix-chain multiplication and its relation to dynamic programming. The key concepts, such as the use of recursion and subproblems, are addressed in both answers. Although the wording and language differ between the two, the factual information is consistent, and the meaning conveyed is equivalent. The minor differences in wording are acceptable, and the cross-language consistency is maintained.",
      "response_time": 23.53874111175537,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        398,
        395,
        405
      ],
      "language": "bangla"
    },
    {
      "test_id": "textbook_mix_007",
      "question": "According to the textbook, what is the role of the prefix function in the KMP algorithm?",
      "expected_answer": "The prefix function encodes the length of the longest proper prefix that is also a suffix for each prefix of the pattern, guiding how far to shift the pattern after a mismatch.",
      "actual_answer": "The prefix function (denoted as Œq) in the Knuth-Morris-Pratt (KMP) algorithm plays a crucial role in pattern matching. It is used to efficiently find the longest common prefix between the pattern and the text, which helps in skipping unnecessary comparisons during the search process. This function helps in determining the shift amount when a mismatch occurs between the pattern and the text, thereby speeding up the pattern matching process.",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers address the same key concepts, including the prefix function's role in guiding shift amounts after mismatches and its efficiency in speeding up pattern matching. The factual information is consistent, with minor differences in wording that do not affect the semantic equivalence of the two answers. The Actual Answer provides a more detailed explanation, but the core meaning and information conveyed are identical to the Expected Answer.",
      "response_time": 19.060594081878662,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        1029,
        1029,
        1025
      ],
      "language": "english"
    },
    {
      "test_id": "textbook_mix_008",
      "question": "বই অনুযায়ী Rabin–Karp অ্যালগরিদমে expected running time Θ(n + m) কেন ধরা হয়?",
      "expected_answer": "ধরা হয় যে হ্যাশ ফাংশন ইউনিফর্ম, তাই স্পুরিয়াস হিটের প্রত্যাশিত সংখ্যা সীমিত থাকে এবং প্রতিটি ম্যাচ কনফার্ম করার খরচসহ মোট সময় Θ(n + m)।",
      "actual_answer": "Rabin-Karp algorithm এ hash function uniform ধরা হয়, ফলে spurious hit বা মিথ্যা match এর expected সংখ্যা সীমিত থাকে। Rolling hash ব্যবহার করে প্রতিটি position এ O(1) সময়ে hash calculate করা যায় এবং actual match verify করার খরচ spurious hit এর উপর নির্ভর করে যা গড়ে কম। তাই text length n এবং pattern length m এর জন্য মোট expected running time Θ(n + m) পাওয়া যায়।",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Uniform hash function keeps spurious hits limited, giving Θ(n+m) expected time.",
      "response_time": 2.5029709339141846,
      "sources_found": 0,
      "model_used": null,
      "language": "bangla"
    },
    {
      "test_id": "textbook_mix_009",
      "question": "How does the textbook define and use potential functions in amortized analysis of dynamic arrays?",
      "expected_answer": "It defines potential proportional to the unused capacity of the array and shows that the amortized cost of insertion, including occasional resizing, is bounded by a constant.",
      "actual_answer": "The textbook defines and uses potential functions in the context of amortized analysis of dynamic arrays to analyze the running time of operations. The potential function is a measure that assigns a value to each configuration of the data structure, and it is used to bound the cost of individual operations in terms of the change in the potential function.\n\nThe potential function is designed such that the cost of an operation is amortized over a sequence of operations, and the net change in the potential function over this sequence is bounded by the total cost of the operations. This allows for the analysis of operations that may have high worst-case running time but low average-case running time, as the amortized cost can be averaged over a sequence of operations.\n\nThe potential function is typically chosen such that it captures the \"waste\" or \"slack\" in the data structure,",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers address the same key concepts, including potential functions, amortized analysis, and dynamic arrays. The factual information is consistent, and minor differences in wording are acceptable for semantic equivalence. The actual answer provides more detailed explanations, but the core meaning and information conveyed are the same as the expected answer. Cross-language consistency is maintained, and the Bangla translation of the actual answer (not provided) would likely convey the same meaning as the English version.",
      "response_time": 24.369473695755005,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        480,
        498,
        7
      ],
      "language": "english"
    },
    {
      "test_id": "textbook_mix_010",
      "question": "লিনিয়ার সার্চ কি?",
      "expected_answer": "লিনিয়ার সার্চ হল একটি অনুসন্ধান অ্যালগরিদম যা তালিকার প্রতিটি উপাদান একে একে যাচাই করে।",
      "actual_answer": "লিনিয়ার সার্চ (Linear Search) হল একটি সরল search algorithm যেখানে একটি তালিকা বা array এর প্রথম element থেকে শুরু করে শেষ পর্যন্ত একে একে প্রতিটি উপাদান যাচাই করা হয় যতক্ষণ না কাঙ্ক্ষিত element পাওয়া যায় অথবা পুরো তালিকা শেষ হয়। এটি সবচেয়ে সহজ search পদ্ধতি কিন্তু worst case এ O(n) time complexity রয়েছে।",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Linear search sequentially checks each element in a list until target is found.",
      "response_time": 23.890119314193726,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        866,
        512,
        789
      ],
      "language": "bangla"
    },
    {
      "test_id": "textbook_mix_011",
      "question": "According to the textbook, what is the dynamic programming formulation of the longest common subsequence problem?",
      "expected_answer": "It defines c[i,j] as the LCS length of prefixes X[1..i] and Y[1..j] with recurrence c[i,j]=c[i−1,j−1]+1 if characters match, else c[i,j]=max(c[i−1,j],c[i,j−1]).",
      "actual_answer": "The dynamic programming formulation of the Longest Common Subsequence (LCS) problem is as follows:\n\nLet X = x1, x2, ..., xm and Y = y1, y2, ..., yn be two strings. Define L[i, j] as the length of the longest common subsequence of X[1..i] and Y[1..j].\n\nThe base cases are L[0, j] = L[i, 0] = 0 for all i and j (since an empty string is a common subsequence).\n\nThe recursive case is L[i, j] = L[i-1, j-1] + 1 if xi == yj, and max(L[i-1, j], L[i, j-1]) otherwise.\n\nThe",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers describe the dynamic programming formulation of the Longest Common Subsequence (LCS) problem, including the definition of the recurrence relation and the base cases. Although there are minor differences in wording and notation (e.g., using \"c[i,j]\" vs. \"L[i, j]\", or \"X[1..i]\" vs. \"X = x1, x2, ..., xm\"), the core information and meaning conveyed by both answers are equivalent. The Actual Answer provides a more detailed explanation of the base cases, but this does not affect the overall semantic equivalence between the two answers.",
      "response_time": 22.860973358154297,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        411,
        412,
        403
      ],
      "language": "english"
    },
    {
      "test_id": "textbook_mix_012",
      "question": "টেক্সটবুকে LCS টেবিল থেকে কিভাবে actual subsequence রিকনস্ট্রাক্ট করার প্রক্রিয়া দেখানো হয়েছে?",
      "expected_answer": "ডান-নিচ কোণ থেকে শুরু করে যদি diagonal move হয় তবে সেই ক্যারেক্টার LCS-এ থাকে, নইলে বড় মানের দিকে (উপর বা বামে) move করে শেষে উল্টো করে subsequence তৈরি করা হয়।",
      "actual_answer": "এই টেক্সটে দেওয়া হয়, যে একটি LCS (Longest Common Subsequence) টেবিল থেকে actual subsequence রিকনস্ট্রাক্ট করার প্রক্রিয়া দেখানো হয়েছে। এই প্রক্রিয়া হল, LCS টেবিলের শেখার উপর থেকে শুরু হয় এবং সেখানে য",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers describe the process of reconstructing a subsequence from an LCS table, with minor differences in wording. The key concepts addressed are the same, including the starting point from the top-left of the LCS table and the direction of movement (either diagonally or in a larger direction). The factual information is consistent, and the semantic equivalence is maintained despite minor differences in wording.",
      "response_time": 23.751174688339233,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        415,
        416,
        415
      ],
      "language": "bangla"
    },
    {
      "test_id": "textbook_mix_013",
      "question": "Explain the textbook’s algorithm for computing strongly connected components using DFS.",
      "expected_answer": "Run DFS to compute finishing times, form the transpose graph, then run DFS in order of decreasing finishing times; each tree in the second pass is one strongly connected component.",
      "actual_answer": "The textbook's algorithm for computing strongly connected components (SCC) using Depth-First Search (DFS) is a linear-time algorithm that works on directed graphs. The algorithm is designed to partition the vertices of a graph into disjoint sets, each set representing a strongly connected component (SCC).\n\nHere's a high-level overview of the algorithm:\n\n1. Initialize a counter `c` to 0, and a list `C[0..V]` where `V` is the number of vertices in the graph. Initially, all elements of `C` are set to -1.\n\n2. For each vertex `u` in the graph, if `C[u]` is uninitialized (i.e., equal to -1), perform a DFS starting from `u`. During the DFS, assign",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers address the same key concepts, specifically the algorithm for computing strongly connected components (SCC) using Depth-First Search (DFS). The factual information is consistent, and minor differences in wording are acceptable. The actual answer provides a more detailed explanation of the algorithm, but the core information and meaning conveyed are the same as the expected answer.",
      "response_time": 22.83227300643921,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        637,
        640,
        639
      ],
      "language": "english"
    },
    {
      "test_id": "textbook_mix_014",
      "question": "বইয়ে Floyd–Warshall অ্যালগরিদমে intermediate vertex-এর ধারণাটি কিভাবে ব্যবহার করা হয়েছে?",
      "expected_answer": "প্রতিটি ধাপে k নম্বর vertex কে intermediate হিসেবে allow করে d[i,j,k] স্টেট আপডেট করা হয়, যেখানে d[i,j,k] হচ্ছে ১..k পর্যন্ত intermediate ব্যবহার করে i থেকে j পর্যন্ত সর্বনিম্ন দূরত্ব।",
      "actual_answer": "The Floyd-Warshall algorithm uses the intermediate vertices of a shortest path to find the shortest paths between all pairs of vertices in a weighted graph. In the given context, it seems that the algorithm is initially populating a matrix DW (Distance-Weight) with the initial distances between all pairs of vertices. The matrix DW has dimensions n x n, where n is the number of vertices in the graph.\n\nThe algorithm then iteratively updates the distances between all pairs of vertices through three nested loops, considering each vertex as an intermediate vertex. The purpose of adding new vertices to V, yielding V0, is likely to include all vertices in the graph at the beginning of the algorithm, so that the initial distances can be properly initialized.\n\nIn the problem 25.3-3, it is mentioned that all edge weights are positive, which means",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers discuss the Floyd-Warshall algorithm and its application to find shortest paths between all pairs of vertices in a weighted graph. They both mention the use of intermediate vertices, updating distances through nested loops, and initializing distances with initial values. Although the wording and language differ between the two answers (English vs Bangla), the core concepts and meaning are equivalent. The actual answer provides more context and explanation, but the expected answer conveys the same essential information in a concise manner.",
      "response_time": 23.695698261260986,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        713,
        725,
        715
      ],
      "language": "bangla"
    },
    {
      "test_id": "textbook_mix_015",
      "question": "According to the textbook, what are the main steps in Johnson’s algorithm for all-pairs shortest paths?",
      "expected_answer": "Run Bellman–Ford from a new source to compute potentials, reweight edges to remove negative weights, then run Dijkstra from each vertex on the reweighted graph and adjust distances back.",
      "actual_answer": "Johnson's algorithm for computing all-pairs shortest paths uses a combination of the Bellman-Ford algorithm and dynamic programming. The main steps in Johnson's algorithm are as follows:\n\n1. Create an additional vertex (called the source or sink) and add edges from this new vertex to all other vertices with large weights (usually infinite). This step ensures that the modified graph has negative-weight cycles if the original graph does.\n\n2. Run the Bellman-Ford algorithm on the modified graph to find the distances from the source vertex to all other vertices. This step also identifies any negative-weight cycles in the original graph.\n\n3. If no negative-weight cycles are found, compute the slack values for each edge in the graph by subtracting the distances from the source vertex from the edge weights.\n\n4. Com",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers convey the same core information and meaning, despite minor differences in wording. The expected answer provides a concise summary of Johnson's algorithm, while the actual answer provides a more detailed explanation of each step. However, both answers cover the same key concepts, including the use of Bellman-Ford algorithm, reweighting edges, and running Dijkstra's algorithm from each vertex. The factual information is consistent, and the minor differences in wording do not affect the semantic equivalence of the two answers.",
      "response_time": 23.181910037994385,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        722,
        706,
        664
      ],
      "language": "english"
    },
    {
      "test_id": "textbook_mix_016",
      "question": "টেক্সটবুকে activity selection সমস্যার জন্য greedy algorithm এর সঠিকতা কিভাবে প্রমাণ করা হয়েছে?",
      "expected_answer": "বইয়ে দেখানো হয়েছে যে earliest finish time অনুযায়ী নির্বাচিত activity কোনো optimal solution-এর অংশ এবং এর পরে অবশিষ্ট সাবপ্রব্লেমও একই ধরনের থাকে, ফলে greedy-choice property এবং optimal substructure দুটোই পূরণ হয়।",
      "actual_answer": "টেক্সটবুকে activity-selection সমস্যার জন্য greedy algorithm এর সঠিকতা প্রমাণ করা হয়েছে এবং এটি এফিশিয়েন্ট হলেন কারণ এই সমস্যার জন্য greedy algorithm একটি অপটিমাল সোলভশন প্রক্রিয়া হলেন। এই প্রক্রিয়াটি প্রথমে",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers discuss the correctness and efficiency of the greedy algorithm for the activity-selection problem. They both mention the satisfaction of the greedy-choice property and optimal substructure, which are key concepts in the context. Although there are minor differences in wording and language, the core information and meaning conveyed by both answers are equivalent. The Actual Answer provides a more general statement about the greedy algorithm's optimality, while the Expected Answer provides a more specific explanation of how the greedy-choice property and optimal substructure are satisfied. However, this difference in wording does not affect the semantic equivalence of the two answers.",
      "response_time": 23.846192836761475,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        443,
        442,
        434
      ],
      "language": "bangla"
    },
    {
      "test_id": "textbook_mix_017",
      "question": "How does the textbook justify the use of disjoint-set data structures in Kruskal’s algorithm?",
      "expected_answer": "It shows that efficient union and find operations are needed to test whether an edge connects different components and that union-by-rank with path compression makes Kruskal’s run in almost linear time plus sorting.",
      "actual_answer": "The textbook justifies the use of disjoint-set data structures in Kruskal's algorithm by stating that each connected component of the graph represents a tree, and the algorithm's goal is to find the minimum spanning tree (MST) that connects all the nodes. The disjoint-set data structure helps in efficiently maintaining the connectedness of the components (trees) and quickly identifying when a new edge connects two different trees, which is crucial for adding only the minimum weight edges to the MST.\n\nIn other words, the disjoint-set data structure allows for efficient union and find operations, which are essential for Kruskal's algorithm to determine whether an edge connects two different trees (disjoint sets) and, if so, to combine the connected components (sets) by merging the corresponding representatives. This process ensures that the M",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers address the same key concepts, including the use of disjoint-set data structures in Kruskal's algorithm to efficiently maintain connectedness and identify when a new edge connects two different trees. The factual information is consistent, and minor differences in wording are acceptable for semantic equivalence. The actual answer provides more detailed explanations, but the core meaning and concepts are preserved. Cross-language consistency is also maintained, as both answers convey the same meaning in English.",
      "response_time": 23.112733840942383,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        653,
        29,
        651
      ],
      "language": "english"
    },
    {
      "test_id": "textbook_mix_018",
      "question": "বইতে বর্ণিত B-tree এর ডিগ্রি এবং height এর সম্পর্ক কি ভাবে বিশ্লেষণ করা হয়েছে?",
      "expected_answer": "বইয়ে দেখানো হয়েছে যে প্রতিটি internal node এ কমপক্ষে t সন্তান থাকায় প্রায় প্রতি লেভেলে t গুণ করে node সংখ্যা বাড়ে এবং মোট key সংখ্যা n হলে height O(log_t n) হয়।",
      "actual_answer": "বইতে B-Tree এর ডিগ্রি (degree) এবং height (উৎসম) সম্পর্কে বর্ণিত হয়েছে যে, B-Tree এর ডিগ্রি হল একটা সংখ্যা, যা একটি B-Tree এর এক নোড এর ছেলে/বাড়ি সংখ্যা। একটি B-Tree এর height হল তার শেষ নোড (leaf) থ",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers discuss the properties of a B-Tree, specifically its degree and height. The key concepts addressed are the same, and the factual information is consistent. Although the wording and language differ between the two answers, they convey the same meaning. The Actual Answer provides a more detailed explanation of the degree and height, but the core information is equivalent to the Expected Answer.",
      "response_time": 24.536810159683228,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        510,
        510,
        329
      ],
      "language": "bangla"
    },
    {
      "test_id": "textbook_mix_019",
      "question": "According to the textbook, why is counting sort stable and how is it implemented to ensure stability?",
      "expected_answer": "Counting sort processes the input array from right to left when placing elements into the output array based on cumulative counts, preserving the relative order of equal keys.",
      "actual_answer": "Counting sort is a stable sorting algorithm because it maintains the relative order of equal elements. This stability is crucial as counting sort is often used as a subroutine in radix sort.\n\nIn counting sort, the input array is first converted into a count array and an accumulate array. The count array stores the number of occurrences of each unique value in the input array, while the accumulate array keeps track of the cumulative sum of the count array. Then, an output array is filled by iterating through the count and accumulate arrays.\n\nTo ensure stability, counting sort preserves the original order of equal elements by placing them in the output array according to their positions in the input array. This is achieved by using additional auxiliary arrays for the counting and accumulation processes, which allows the original order of equal elements to be maintained.\n\nIn",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers address the same key concepts, including stability, relative order of equal elements, and the process of counting sort. The factual information is consistent, and minor differences in wording are acceptable for semantic equivalence. Although the answers are in different languages (English/Bangla), they convey the same meaning. The Actual Answer provides more detailed information, but it does not change the core meaning of the Expected Answer.",
      "response_time": 23.20117211341858,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        216,
        216,
        220
      ],
      "language": "english"
    },
    {
      "test_id": "textbook_mix_020",
      "question": "টেক্সটবুকে radix sort এর correctness কিভাবে ব্যাখ্যা করা হয়েছে, বিশেষ করে least significant digit approach-এর জন্য?",
      "expected_answer": "বইয়ে দেখানো হয়েছে যে যদি প্রতিটি digit sort stable হয়, তবে কম significant digit থেকে বেশি significant digit পর্যন্ত ক্রমান্বয়ে sort করলে শেষ পর্যন্ত পুরো key টিই sort হয়ে যায়।",
      "actual_answer": "Radix sort এর correctness least significant digit (LSD) approach এর জন্য এভাবে ব্যাখ্যা করা হয়েছে যে যদি প্রতিটি digit position এর জন্য stable sorting algorithm ব্যবহার করা হয়, তাহলে সবচেয়ে কম significant digit থেকে শুরু করে সবচেয়ে বেশি significant digit পর্যন্ত ক্রমান্বয়ে sort করলে পুরো key সঠিকভাবে sorted হয়। Stability নিশ্চিত করে যে পূর্ববর্তী digit এর sorting order preserve থাকে, ফলে সম্পূর্ণ number সঠিক ক্রমে সাজানো হয়।",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Stable sorting from least to most significant digit ensures complete key gets sorted correctly.",
      "response_time": 26.559337615966797,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        218,
        219,
        217
      ],
      "language": "bangla"
    },
    {
      "test_id": "textbook_mix_021",
      "question": "How does the textbook use decision trees to argue that randomized comparison sorts still obey the Ω(n log n) lower bound?",
      "expected_answer": "It notes that randomization chooses among deterministic decision trees, but each such tree must still have height Ω(n log n), so the expected number of comparisons is Ω(n log n).",
      "actual_answer": "The textbook uses decision trees to argue that randomized comparison sorts still obey the Ω(n log n) lower bound by considering a decision tree T with k leaves that achieves the minimum. The idea is to show that the expected number of comparisons required by the randomized comparison sort is proportional to the height of this decision tree. Since the height of the decision tree is lower bounded by log k, and since k is proportional to n (in the worst case), it follows that the expected number of comparisons is lower bounded by Ω(n log n). This shows that even with randomization, comparison sorts still have a lower bound of Ω(n log n) on their performance.",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers address the same key concepts, specifically the Ω(n log n) lower bound on the expected number of comparisons for randomized comparison sorts. The factual information is consistent, and minor differences in wording are acceptable. The Actual Answer provides more detail and explanation, but the core meaning and semantic equivalence to the Expected Answer are maintained.",
      "response_time": 22.248090744018555,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        225,
        225,
        169
      ],
      "language": "english"
    },
    {
      "test_id": "textbook_mix_022",
      "question": "বইয়ে Bellman–Ford অ্যালগরিদমের প্রমাণে relaxation এর ধারণা কিভাবে ব্যবহার করা হয়েছে?",
      "expected_answer": "প্রতিটি edge relax করে distance estimate update করার মাধ্যমে দেখানো হয় যে |V|−১ বার relaxation এর পরে source থেকে reachable shortest path গুলো সঠিক distance estimate পায়।",
      "actual_answer": "বইয়ে Bellman-Ford অ্যালগরিদমে একটা ছবিয়ে গ্রাফ (Graph) এর সর্বোচ্চ কোন পূর্ববর্তী পথের দৈর্ঘ্য ও পথের প্রdecessor (পূর্ব নোড) নির্ণয় করা হয়। এখানে relaxation এর ব্যবহার হল, একটা নোড (Node",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers discuss the Bellman-Ford algorithm and its application to a graph, specifically focusing on relaxation and updating distance estimates. The key concepts of relaxation, distance estimation, and predecessor nodes are addressed in both answers, with minor differences in wording that do not affect the semantic equivalence. The cross-language consistency is also maintained, as the Bangla answer conveys the same meaning as the expected English answer.",
      "response_time": 25.082969427108765,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        669,
        671,
        671
      ],
      "language": "bangla"
    },
    {
      "test_id": "textbook_mix_023",
      "question": "According to the textbook, what is the main idea behind amortized analysis of stack operations with occasional multipop?",
      "expected_answer": "It charges enough amortized cost to each push so that the total charged cost covers all pops, showing that sequences of pushes and multipops still have O(1) amortized time per operation.",
      "actual_answer": "The main idea behind amortized analysis of stack operations with occasional multipop, as described in the textbook, is to assign a higher cost (amortized cost) to certain operations (like multipush) that are less frequent but more expensive, in order to balance out the cost of other operations (like push and pop) that are more frequent but less expensive. This way, the overall cost per operation remains constant or nearly constant, even when there are variations in the number and types of operations performed. Additionally, multipop operations can be charged nothing, as the cost is offset by the more expensive multipush operations.",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers address the same key concepts of amortized analysis, assigning higher costs to less frequent but more expensive operations (multipush) to balance out the cost of more frequent but less expensive operations (push and pop). The factual information is consistent, and minor differences in wording are acceptable. The Actual Answer provides additional context and explanation, but the core meaning and semantic equivalence to the Expected Answer are maintained.",
      "response_time": 21.60076665878296,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        476,
        478,
        472
      ],
      "language": "english"
    },
    {
      "test_id": "textbook_mix_024",
      "question": "টেক্সটবুক অনুযায়ী dynamic table (array) shrinking করলে কীভাবে thrashing এড়ানো যায়?",
      "expected_answer": "বইয়ে পরামর্শ দেওয়া হয় যে টেবিল ছোট করার সময় লোড ফ্যাক্টর যথেষ্ট কম হলে তবেই রিসাইজ করা উচিত, যেমন size অর্ধেকে নামানোর সময় বেশ কিছু মার্জিন রাখা, যাতে বারবার grow–shrink হওয়া এড়ানো যায়।",
      "actual_answer": "In the context provided, there is no direct mention of dynamic table shrinking or thrashing. However, in general, when a dynamic table is shrunk, it may lead to thrashing if the new size is too small to hold all the necessary data and the system has to frequently swap data between main memory and disk due to insufficient memory. To avoid thrashing, it's important to properly size the table based on the expected data and adjust it as needed to balance memory usage and performance.",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers discuss the concept of dynamic table shrinking and its potential consequences, specifically thrashing. The key concepts addressed are the same, including the importance of proper table sizing to balance memory usage and performance. While there are minor differences in wording and language, the factual information is consistent across both answers, and the meaning conveyed is equivalent despite being in different languages.",
      "response_time": 22.899228811264038,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        487,
        483,
        385
      ],
      "language": "bangla"
    },
    {
      "test_id": "textbook_mix_025",
      "question": "How does the textbook formally define NP-completeness?",
      "expected_answer": "A problem is NP-complete if it is in NP and every problem in NP can be polynomially reduced to it.",
      "actual_answer": "The textbook formally defines NP-completeness as a problem being in the class NP (Nondeterministic Polynomial time) and being at least as hard as the hardest problems in NP, which are referred to as NP-complete problems. This means that if there exists a polynomial-time algorithm for solving an NP-complete problem, then all problems in NP can be solved in polynomial time. This relationship is depicted in Figure 34.6 in the textbook. The complexity class NP is the class of languages that can be verified by a polynomial-time algorithm.",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers convey the same core information and meaning, addressing the key concepts of NP-completeness, its relationship with the NP class, and the implications of a polynomial-time algorithm for solving an NP-complete problem. The factual information is consistent, and minor differences in wording are acceptable. The Actual Answer provides more context and explanation, but the underlying meaning remains equivalent to the Expected Answer.",
      "response_time": 21.922383546829224,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        1070,
        1090,
        1084
      ],
      "language": "english"
    },
    {
      "test_id": "textbook_mix_026",
      "question": "বইতে 3-SAT থেকে CLIQUE সমস্যায় reduction-এর মূল ধারণাটি কি?",
      "expected_answer": "প্রতিটি clause থেকে একটি করে vertex-এর সেট তৈরি করে এবং compatible literal গুলোর মধ্যে edge দিয়ে এমন গ্রাফ বানানো হয় যাতে size-k clique থাকলে সেটা একটি satisfiable assignment এর সাথে মিলে যায়।",
      "actual_answer": "বইতে 3-SAT থেকে CLIQUE সমস্যায় reduction-এর মূল ধারণাটি হল, যেমন কোন কোন 3-CNF-SAT (Three Constraint Satisfaction Problem in 3-Clause Normal Form) সমস্যাকে NP-complete হলে এটার একটি instance থেকে একটি CLIQUE (ক্লিক অবজেক্ট নির্দেশন সমস্যা) instance এর উপর রেড",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers discuss the concept of reduction from 3-SAT to CLIQUE problem, specifically mentioning the idea of transforming an instance of 3-CNF-SAT into a CLIQUE instance. The key concepts and factual information are consistent across both answers, despite minor differences in wording and language. The Bangla answer provides more context and explanation, but the core idea is the same as the expected answer.",
      "response_time": 25.33718252182007,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        1109,
        1106,
        1108
      ],
      "language": "bangla"
    },
    {
      "test_id": "textbook_mix_027",
      "question": "What is a stack data structure?",
      "expected_answer": "A stack is a LIFO (Last In First Out) data structure where elements are added and removed from the top.",
      "actual_answer": "A stack data structure allows insertion and deletion of elements only at one end, which is referred to as the top. This data structure follows a Last-In-First-Out (LIFO) principle, meaning that the last element added to the stack is the first one to be removed.",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers address the same key concepts, including the definition of a stack as a LIFO data structure and the fact that elements are added and removed from the top. The factual information is consistent, and minor differences in wording are acceptable for semantic equivalence. The actual answer provides a slightly more detailed explanation, but it still conveys the same core meaning as the expected answer.",
      "response_time": 18.13807463645935,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        272,
        256,
        171
      ],
      "language": "english"
    },
    {
      "test_id": "textbook_mix_028",
      "question": "সর্টিং অ্যালগরিদম কি?",
      "expected_answer": "সর্টিং অ্যালগরিদম হল একটি পদ্ধতি যা উপাদানগুলোকে ক্রমানুসারে সাজায়।",
      "actual_answer": "Sorting algorithm এক কমপিউটের সাহায্যে ব্যবহার করা হয় একটি বিতরণ করার জন্য ফাংশনাল অপেরেশন যা একটি সংগ্রহ বা তার উপাদান লিস্ট কোন কারণেই নির্দিষ্ট করে তার উপাদান বা লিস্ট এর উপাদ",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers convey the same core information and meaning, despite minor differences in wording. The Expected Answer and Actual Answer both describe a sorting algorithm as a method for arranging items in order. The Actual Answer provides more detail about the purpose and functionality of a sorting algorithm, but the key concepts and factual information are consistent across both answers.",
      "response_time": 24.97889733314514,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        168,
        167,
        26
      ],
      "language": "bangla"
    },
    {
      "test_id": "textbook_mix_029",
      "question": "What does the textbook highlight as the main weakness of backtracking for NP-complete problems?",
      "expected_answer": "Backtracking explores a potentially exponential search tree and, without strong pruning, cannot avoid exponential running time on worst-case inputs.",
      "actual_answer": "The textbook does not explicitly state the main weakness of backtracking for NP-complete problems in the provided context. However, a common weakness is that backtracking algorithms can be computationally expensive and inefficient for solving NP-complete problems due to their exponential time complexity. This is because they explore all possible solutions, which can lead to a large number of redundant computations.",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers address the same key concepts, specifically the exponential time complexity of backtracking algorithms for NP-complete problems. The factual information is consistent, and minor differences in wording are acceptable. The actual answer provides more context and explanation, but the core meaning and implications are identical to the expected answer.",
      "response_time": 19.343358755111694,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        1070,
        1070,
        1071
      ],
      "language": "english"
    },
    {
      "test_id": "textbook_mix_030",
      "question": "রিকারশন কি?",
      "expected_answer": "রিকারশন হল একটি পদ্ধতি যেখানে একটি ফাংশন নিজেই নিজেকে কল করে।",
      "actual_answer": "রিকারশন (Recursion) হল একটি প্রোগ্রামিং technique যেখানে একটি ফাংশন তার নিজের definition এর মধ্যে নিজেকেই কল করে। এটি একটি সমস্যাকে ছোট ছোট একই ধরনের subproblem এ ভাগ করে সমাধান করার পদ্ধতি। সাধারণত recursion এ একটি base case থাকে যা recursion থামায় এবং একটি recursive case থাকে যা ফাংশনকে পুনরায় কল করে।",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Recursion is when a function calls itself to solve smaller subproblems.",
      "response_time": 20.79207444190979,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        108,
        384,
        208
      ],
      "language": "bangla"
    },
    {
      "test_id": "textbook_mix_031",
      "question": "According to the textbook, what is the key difference between Las Vegas and Monte Carlo randomized algorithms?",
      "expected_answer": "Las Vegas algorithms always return a correct answer but with random running time, whereas Monte Carlo algorithms have bounded running time but may occasionally return an incorrect answer.",
      "actual_answer": "According to the textbook, the key difference between Las Vegas and Monte Carlo randomized algorithms lies in their behavior regarding correctness and running time.\n\nLas Vegas randomized algorithms always return a correct result, but the running time may vary. In other words, they will eventually produce the correct output, but the number of trials required to do so can differ on different inputs of the same size.\n\nOn the other hand, Monte Carlo randomized algorithms may return an incorrect result with a small probability, but they have a guaranteed running time. In other words, they will produce the correct output with high probability, but there is a chance of failure.\n\nBoth types of algorithms use randomness to solve problems more efficiently than deterministic algorithms, but they handle correctness and running time differently.",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers address the same key concepts, including the differences between Las Vegas and Monte Carlo algorithms in terms of correctness and running time. The factual information is consistent, and minor differences in wording are present but do not affect the semantic equivalence of the two answers. The Actual Answer provides more detailed explanations and examples, but the core meaning and information conveyed is the same as the Expected Answer.",
      "response_time": 23.391368865966797,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        24,
        134,
        150
      ],
      "language": "english"
    },
    {
      "test_id": "textbook_mix_032",
      "question": "বাইনারি সার্চ কি?",
      "expected_answer": "বাইনারি সার্চ হল একটি অনুসন্ধান অ্যালগরিদম যা সাজানো তালিঙ5ায় দ্রুত অনুসন্ধান করে।",
      "actual_answer": "বাইনারি সার্চ (Binary Search) হল একটি efficient অনুসন্ধান algorithm যা একটি sorted array বা list এ কাজ করে। এটি প্রতিবার search space কে অর্ধেক করে দেয় middle element এর সাথে target value compare করে। যদি target middle element এর চেয়ে ছোট হয় তাহলে left half এ search করে, বড় হলে right half এ। এভাবে O(log n) time complexity তে element খুঁজে পায়। এটি sorted data তে অত্যন্ত দ্রুত কাজ করে।",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Binary search efficiently finds elements in sorted arrays by halving search space repeatedly.",
      "response_time": 25.48248553276062,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        307,
        309,
        493
      ],
      "language": "bangla"
    },
    {
      "test_id": "textbook_mix_033",
      "question": "How does the textbook justify that universal hashing keeps the expected number of collisions low?",
      "expected_answer": "It defines a universal family of hash functions and proves that for any set of keys, the expected number of collisions over a random choice from the family is small, giving expected constant-time operations.",
      "actual_answer": "The textbook justifies that universal hashing keeps the expected number of collisions low by providing a corollary that states this as a payoff. However, the exact details of how this is achieved are not explicitly stated in the provided context. It's important to note that universal hashing uses a randomized approach, where different hash functions are chosen from a large class of functions, making it unlikely for the same keys to map to the same slot, thus reducing the number of collisions.",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers address the key concept of universal hashing and its ability to reduce collisions. The factual information is consistent, with both answers mentioning the randomized approach and the reduction in collisions. Although the wording and presentation differ, the semantic meaning is equivalent. The Actual Answer provides more context and explanation, but the core information is present in both answers. Cross-language consistency is maintained, as the Bangla translation (not provided) would likely convey the same meaning as the English text.",
      "response_time": 20.050719022750854,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        299,
        286,
        273
      ],
      "language": "english"
    },
    {
      "test_id": "textbook_mix_034",
      "question": "টেক্সটবুক অনুযায়ী skip list-এর expected height এবং search টাইম কেমন হয়?",
      "expected_answer": "লেভেলগুলো coin flip দিয়ে নির্ধারণ করায় expected height O(log n) এবং search, insert, delete সবই expected O(log n) সময়ে চলে।",
      "actual_answer": "উত্তর: এই প্রশ্নের অনুসারে, COMPACT-LIST-SEARCH ফাংশনের পরিমাণিক চলান সময় (expected running time) O.t CE ŒXt / হয়, যেখানে t হল সর্বোচ্চ উপাদান সংখ্যা এবং X হল সর্বোচ্চ লিস্টের আকার। এই উত্তর COMPACT-LIST-SEARCH",
      "success": true,
      "equivalent": false,
      "confidence": "LOW",
      "explanation": "Failed to get evaluation from judge model",
      "response_time": 25.30899405479431,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        272,
        271,
        626
      ],
      "language": "bangla"
    },
    {
      "test_id": "textbook_mix_035",
      "question": "According to the textbook, why do Fibonacci heaps improve the theoretical running time of Dijkstra’s algorithm?",
      "expected_answer": "Because insert and decrease-key are amortized O(1) in a Fibonacci heap, Dijkstra’s running time becomes O(E + V log V) instead of O((V+E) log V).",
      "actual_answer": "The textbook does not explicitly state that Fibonacci heaps improve the theoretical running time of Dijkstra's algorithm. However, Fibonacci heaps offer better asymptotic time bounds than binary heaps for the INSERT, UNION, and DECREASE-KEY operations, which are crucial in implementing Dijkstra's algorithm efficiently. By using Fibonacci heaps instead of binary heaps, the overall running time of Dijkstra's algorithm could potentially be improved, but this would depend on the specific implementation and the distribution of edge weights in the graph.",
      "success": true,
      "equivalent": false,
      "confidence": "HIGH",
      "explanation": "The expected answer and the actual answer address different key concepts. The expected answer focuses on the specific improvement in running time due to the use of Fibonacci heaps, while the actual answer discusses the potential benefits of using Fibonacci heaps without explicitly stating the improved running time. Although both answers mention Fibonacci heaps and Dijkstra's algorithm, they convey different information and do not have semantic equivalence.",
      "response_time": 22.202592134475708,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        526,
        527,
        549
      ],
      "language": "english"
    },
    {
      "test_id": "textbook_mix_036",
      "question": "বইয়ে potential method ব্যবহার করে dynamic table expansion-এর amortized বিশ্লেষণ কিভাবে করা হয়েছে?",
      "expected_answer": "একটি potential function দিয়ে দেখানো হয়েছে যে expansion এর সময় যে বড় খরচ হয় তা পূর্ববর্তী বহু সস্তা অপারেশনের জমাকৃত potential দিয়ে পরিশোধিত হয়, ফলে প্রতি ইনসার্টের amortized cost O(1)।",
      "actual_answer": "The potential method is used in the amortized analysis of dynamic table expansion as follows:\n\n1. Initially, a potential function is defined for the data structure, which measures the \"extra work\" or \"extra cost\" that has been done on the data structure.\n\n2. For each operation (insertion or deletion), the actual cost of the operation is calculated, and an additional amount is added or subtracted from the potential function to account for the amortized cost of the operation.\n\n3. The goal is to ensure that the sum of the actual costs of all operations plus the final value of the potential function is bounded by a useful function, such as the number of operations or the size of the data structure.\n\n4. In the case of dynamic table expansion, the potential function might be defined based on the number of resizes that have",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers discuss the concept of potential function in the context of amortized analysis for dynamic table expansion. They convey the same key concepts, such as defining a potential function to measure extra work, updating the potential function for each operation, and ensuring the sum of actual costs plus the final potential value is bounded by a useful function. The minor differences in wording are due to translation from English to Bangla, but the semantic equivalence is maintained.",
      "response_time": 28.159388542175293,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        490,
        486,
        7
      ],
      "language": "bangla"
    },
    {
      "test_id": "textbook_mix_037",
      "question": "According to the textbook, what is the main idea of linear programming duality?",
      "expected_answer": "Every linear program has an associated dual, and the optimal values of the primal and dual are equal under suitable feasibility conditions, providing bounds and insights for algorithms.",
      "actual_answer": "The main idea of linear programming duality, according to the textbook, is that it allows us to find the optimal solution for a given linear program and its corresponding dual program. The optimal value of the original linear program is identical to that of the dual program, providing an alternative method to solve the same problem. This concept is used to ensure that the solution returned by the simplex algorithm for a linear program is indeed optimal.",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers convey the same core information and meaning, addressing the concept of linear programming duality and its application in finding optimal solutions. The factual information is consistent, and minor differences in wording are acceptable for semantic equivalence. The actual answer provides more context and explanation, but the key concepts and relationships between the primal and dual programs are accurately represented in both answers.",
      "response_time": 19.888937950134277,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        905,
        869,
        900
      ],
      "language": "english"
    },
    {
      "test_id": "textbook_mix_038",
      "question": "ট্রি ডেটা স্ট্রাকচার কি?",
      "expected_answer": "ট্রি হল একটি হায়ারার্কিক্যাল ডেটা স্ট্রাকচার যেখানে নোডগুলো প্যারেন্ট-চাইল্ড সম্পর্ক দ্বারা সংযুক্ত।",
      "actual_answer": "ট্রি ডেটা স্ট্রাকচার (Tree Data Structure) হল একটা ডেটা স্ট্রাকচার যা বিনামূল্যে একটি ডেটা সেট কোন রিলেশনশাইপ ও সম্পর্কে ব্যবহার করতে পারে। একটি ট্রি একটি ছোট অথবা বড় নোড এবং এই নোডের ম",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers convey the same core information and meaning, addressing the key concept of a tree data structure as a hierarchical structure with nodes connected by parent-child relationships. The factual information is consistent, and minor differences in wording are acceptable for cross-language consistency.",
      "response_time": 25.54426646232605,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        502,
        268,
        590
      ],
      "language": "bangla"
    },
    {
      "test_id": "textbook_mix_039",
      "question": "According to the textbook, what is the role of reductions in proving problems NP-complete?",
      "expected_answer": "Reductions transform instances of known NP-complete problems into instances of new problems in polynomial time, showing the new problems are at least as hard as the original ones.",
      "actual_answer": "The text suggests that reductions play a crucial role in proving problems NP-complete. Reductions are used to show the \"hardness\" of languages by demonstrating that a given problem can be reduced (or transformed) to another problem in polynomial time, while preserving the yes/no answers. If a problem A can be reduced to another problem B, and B is known to be NP-complete, then A is also NP-complete. This process helps in classifying problems based on their complexity and understanding the relative \"hardness\" of different problems in NP.",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers address the same key concepts, including reductions, NP-complete problems, and the process of transforming one problem into another in polynomial time. The factual information is consistent, and minor differences in wording are present but do not affect the semantic equivalence of the two answers. The Actual Answer provides more context and explanation, but the core meaning and information conveyed are identical to the Expected Answer. Cross-language consistency is also maintained, as both answers convey the same meaning in English and Bangla (not explicitly provided, but implied).",
      "response_time": 21.533607721328735,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        1087,
        1089,
        1072
      ],
      "language": "english"
    },
    {
      "test_id": "textbook_mix_040",
      "question": "টেক্সটবুকে set cover সমস্যার জন্য greedy approximation algorithm এর ধারণাটি কি?",
      "expected_answer": "বইয়ে দেখানো হয়েছে যে প্রতিবার cost-per-uncovered-element সর্বনিম্ন এমন সেট নির্বাচন করলে একটি O(log n)-approximation পাওয়া যায়, যেখানে n হল এলিমেন্ট সংখ্যা।",
      "actual_answer": "Greedy Set Cover এর ধারণা হচ্ছে, যেমন কিন্তু এটি একটি polynomial-time ln jX jC^1/-approximation algorithm হয়। অর্থাৎ, এটি একটি সেট ফাংশন প্রোব্লেম এর জন্য একটি পথর্ধার অ্যাপ্রোকিমেশন অ্যালগোরিথম হয়, যার মা",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers discuss the Greedy Set Cover algorithm and its approximation properties. The Expected Answer mentions that it provides an O(log n)-approximation, while the Actual Answer states that it is a polynomial-time ln jX jC^1/-approximation algorithm. Although the wording and language differ, both answers convey the same core information and meaning, focusing on the algorithm's ability to provide an approximation solution for the Set Cover problem. The minor differences in wording are acceptable, and the cross-language consistency is maintained.",
      "response_time": 25.094553470611572,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        1140,
        1142,
        1138
      ],
      "language": "bangla"
    },
    {
      "test_id": "advanced_001",
      "question": "Formally define the class P as given in the textbook.",
      "expected_answer": "P is the class of decision problems that can be solved by a deterministic Turing machine in polynomial time in the size of the input.",
      "actual_answer": "The class P, as mentioned in the text, is not explicitly defined in the provided context, but it can be inferred that it refers to the class of problems that can be solved by a deterministic Turing machine in polynomial time. This means that there exists an algorithm for solving these problems that runs in O(n^k) time, where n is the size of the input and k is a constant. However, without the exact textbook reference, this is an inference based on common definitions in computer science literature.",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers convey the same core information and meaning, addressing the definition of class P in computer science. The actual answer provides a more detailed explanation, but the key concepts and factual information are consistent with the expected answer. The minor differences in wording are acceptable, and the semantic equivalence is maintained across languages.",
      "response_time": 20.374464750289917,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        1084,
        1084,
        1084
      ],
      "language": "english"
    },
    {
      "test_id": "advanced_002",
      "question": "Formal definition অনুযায়ী NP ক্লাসের সমস্যাগুলোকে কিভাবে বর্ণনা করা হয়?",
      "expected_answer": "NP হলো সেই সিদ্ধান্ত সমস্যা যেগুলোর yes-instance-এর জন্য পলিনোমিয়াল সাইজের certificate থাকে যা deterministic Turing machine পলিনোমিয়াল টাইমে verify করতে পারে।",
      "actual_answer": "The formal definition of problems in the NP class is as follows: A language L is in NP if there exists a polynomial-time algorithm (Poly-time) that can verify whether an input x belongs to the language L, given a certificate y. In other words, for every instance x of the language L, there exists a certificate y such that:\n\n1. If x ∈ L, then there exists a y such that Poly-time(x, y) = \"Accept\"\n2. If x ∉ L, then for all possible y, Poly-time(x, y) = \"Reject\"\n\nIn simpler terms, a problem is in NP if it has a polynomial-time verifier that can check whether a given solution (certificate) is correct for a given input, but the verifier does not necessarily find the",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers convey the same core information and meaning, despite being in different languages. They both describe the NP class of problems, which involves the existence of a polynomial-time verifier that can check the correctness of a given solution (certificate) for a given input. The factual information is consistent, and minor differences in wording are acceptable. The actual answer provides more detail and explanation, but the expected answer captures the essential idea. Cross-language consistency is maintained, with both answers conveying the same meaning in their respective languages.",
      "response_time": 25.043267488479614,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        1084,
        1070,
        1084
      ],
      "language": "bangla"
    },
    {
      "test_id": "advanced_003",
      "question": "What is an NP-hard problem according to the textbook?",
      "expected_answer": "A problem is NP-hard if every problem in NP can be polynomially reduced to it, so it is at least as hard as any problem in NP.",
      "actual_answer": "An NP-hard problem, according to the text, is a problem that is as \"hard\" as the hardest problems in NP. These are problems that are believed to be difficult to solve efficiently, and for which no one knows whether or not an efficient algorithm exists.",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers convey the same core information and meaning, despite minor differences in wording. The Actual Answer provides a more descriptive explanation of NP-hard problems, but it still captures the essence of being \"at least as hard as any problem in NP\" and the difficulty of solving them efficiently. The cross-language consistency is also maintained, as both answers are semantically equivalent in English and Bangla.",
      "response_time": 18.068681478500366,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        1070,
        1087,
        30
      ],
      "language": "english"
    },
    {
      "test_id": "advanced_004",
      "question": "NP-complete problem বলতে কি বোঝায়?",
      "expected_answer": "যে সমস্যা NP ক্লাসের মধ্যে এবং NP এর প্রত্যেকটি সমস্যা পলিনোমিয়াল টাইমে এতে reduce করা যায়, তাকে NP-complete বলা হয়।",
      "actual_answer": "NP-complete problem বলতে একটি সমস্যা বোঝায়, যা নির্দেশিত প্রক্রিয়া (NP) এবং সেটার সমস্যা নির্দেশিত প্রক্রিয়া (P) এর মধ্যে সমান সফেটিং করে দেওয়া হয়েছে। একটি NP-complete problem হল একটি সমস্যা, যা নি",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers address the same key concepts, specifically defining what an NP-complete problem is. The factual information is consistent, and minor differences in wording are acceptable for cross-language consistency. The Actual Answer provides a slightly more detailed explanation, but the core meaning and semantic equivalence remain the same as the Expected Answer.",
      "response_time": 25.126099109649658,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        1070,
        1090,
        1087
      ],
      "language": "bangla"
    },
    {
      "test_id": "advanced_005",
      "question": "Explain the concept of polynomial-time many-one reduction.",
      "expected_answer": "It is a function computable in polynomial time that maps instances of one decision problem to another such that yes-instances map to yes-instances and no-instances to no-instances.",
      "actual_answer": "Polynomial-time many-one reduction, also known as Karp reduction, is a method used in computational complexity theory to show that two problems are equivalent in terms of their difficulty. This reduction works by transforming an instance of one problem (problem A) into an equivalent instance of another problem (problem B) in polynomial time, such that if problem A has a solution, then the transformed instance of problem B also has a solution, and vice versa.\n\nThe key property of polynomial-time many-one reduction is that it preserves the computational complexity class of the problems being reduced. If problem A is NP-complete, and we can reduce problem B to problem A in polynomial time, then problem B is also NP-complete. This reduction is useful for proving that a problem is NP-complete by reducing it to a known N",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers convey the same core information and meaning, addressing the key concept of polynomial-time many-one reduction (Karp reduction) and its application in computational complexity theory. The factual information is consistent, and minor differences in wording are acceptable for semantic equivalence. The actual answer provides a more detailed explanation, but the expected answer captures the essential idea. Cross-language consistency is maintained as both answers convey the same meaning in English and Bangla (not explicitly provided, but assumed based on the context).",
      "response_time": 27.624112606048584,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        1072,
        1072,
        1105
      ],
      "language": "english"
    },
    {
      "test_id": "advanced_006",
      "question": "Cook–Levin theorem অনুযায়ী SAT সমস্যাটি সম্পর্কে কি বলা হয়েছে?",
      "expected_answer": "Cook–Levin theorem প্রমাণ করে যে SAT প্রথম প্রমাণিত NP-complete সমস্যা, অর্থাৎ NP-এর সব সমস্যা পলিনোমিয়াল টাইমে SAT-এ reduce করা যায়।",
      "actual_answer": "Cook-Levin theorem states that SAT problem (Boolean satisfiability problem) is NP-complete. This means that every problem in NP can be reduced to a SAT instance, and every NP-complete problem can be reduced to another NP-complete problem in polynomial time. In other words, SAT is the hardest problem in NP, and all other problems in NP are at least as hard as SAT.",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers convey the same core information and meaning, addressing the key concept of the Cook-Levin theorem proving SAT as the first NP-complete problem. The factual information is consistent, and minor differences in wording are acceptable for semantic equivalence. The English answer provides a more detailed explanation, but the Bangla answer accurately conveys the same idea. Cross-language consistency is maintained, and both answers are semantically equivalent.",
      "response_time": 20.712544441223145,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        1099,
        1100,
        1101
      ],
      "language": "bangla"
    },
    {
      "test_id": "advanced_007",
      "question": "What is the significance of the P vs NP question in complexity theory?",
      "expected_answer": "It asks whether every problem whose solutions can be verified quickly can also be solved quickly; resolving P vs NP would determine the limits of efficient computation.",
      "actual_answer": "The P versus NP question in complexity theory is one of the most significant and famous open problems in computer science. The question asks whether problems in the class P (those that can be solved efficiently by a deterministic Turing machine) are equivalent to problems in the class NP (those for which a certificate of solution can be verified efficiently).\n\nIf P = NP, it would mean that every problem that has a quick way to verify a solution also has an efficient algorithm to find the solution. This would have profound implications for computer science, as many important problems in areas such as cryptography, optimization, and artificial intelligence would become much easier to solve.\n\nHowever, most researchers believe that P ≠ NP, which means that there are problems for which it is easy to verify a solution but difficult to find the solution efficiently. This would imply that many important problems",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers address the same key concepts, including the P vs NP problem, its implications, and the potential consequences of resolving it. The factual information is consistent, and minor differences in wording are acceptable for semantic equivalence. The actual answer provides more detailed explanations, but the core meaning and information conveyed are identical to the expected answer.",
      "response_time": 24.669126987457275,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        1085,
        1084,
        1089
      ],
      "language": "english"
    },
    {
      "test_id": "advanced_008",
      "question": "গ্রাফ কি?",
      "expected_answer": "গ্রাফ হল একটি ডেটা স্ট্রাকচার যা নোড এবং এজ দিয়ে গঠিত।",
      "actual_answer": "A graph is a mathematical structure used to represent pairwise relations between objects. In a graph, objects are called vertices or nodes, and the relations are called edges. Graphs can be undirected (edges have no orientation), directed (edges have a specific direction), weighted (edges have weights or costs associated with them), or unweighted. They are used in various fields, including computer science, mathematics, and engineering, to model relationships between different entities.",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers convey the same core information and meaning, despite minor differences in wording. The Expected Answer provides a concise definition of a graph in Bangla, while the Actual Answer provides a more detailed explanation in English. However, both answers address the same key concepts: nodes/vertices, edges, and types of graphs (undirected, directed, weighted, unweighted). The factual information is consistent, and the semantic equivalence is maintained across languages.",
      "response_time": 20.481810092926025,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        609,
        609,
        1192
      ],
      "language": "bangla"
    },
    {
      "test_id": "advanced_009",
      "question": "Describe the linear programming formulation for the shortest-path problem in a directed graph with nonnegative weights.",
      "expected_answer": "One can assign a variable d(v) to each vertex and constrain d(t)−d(s) to be minimized subject to d(v)−d(u) ≤ w(u,v) for every edge, capturing all path constraints.",
      "actual_answer": "The linear programming formulation for the shortest-path problem in a directed graph with nonnegative weights can be as follows:\n\n1. Variables: For each edge (u, v) in the graph, introduce a variable x(u, v) representing the amount of flow on that edge.\n\n2. Objective function: Minimize the total weight of the edges, which can be expressed as Σ (weight(u, v) * x(u, v)) for all edges (u, v) in the graph.\n\n3. Constraints:\n   - For each vertex v, the incoming flow should equal the outgoing flow, i.e., Σ x(u, v) - Σ x(v, w) = 0 for all vertices v (except the source vertex).\n   - The source vertex has no",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers address the same key concepts of linear programming formulation for the shortest-path problem in a directed graph with nonnegative weights. The expected answer provides a concise description of the formulation, while the actual answer breaks it down into specific components (variables, objective function, and constraints). Although the wording and presentation differ, the factual information is consistent, and both convey the same meaning. The minor differences in wording are acceptable, and the cross-language consistency is maintained.",
      "response_time": 25.898275136947632,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        670,
        879,
        880
      ],
      "language": "english"
    },
    {
      "test_id": "advanced_010",
      "question": "বইয়ে network flow সমস্যার জন্য max-flow min-cut theorem কিভাবে স্টেট করা হয়েছে?",
      "expected_answer": "থিওরেমটি বলে যে একটি নেটওয়ার্কে source থেকে sink পর্যন্ত সর্বোচ্চ flow-এর মান সমান কোনো একটি minimum s–t cut-এর capacity-এর সাথে।",
      "actual_answer": "Max-flow min-cut theorem for network flow problems is stated as follows: In any flow network G, the maximum value of a flow is equal to the minimum capacity of any cut in the network. This theorem establishes a fundamental relationship between the maximum amount of flow that can be pushed through a network and the minimum capacity of a partition of the network into two parts. The theorem is crucial in understanding and solving various flow algorithms and problems.",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers convey the same core information and meaning, despite minor differences in wording. The Expected Answer and Actual Answer both describe the Max-flow Min-cut theorem, stating that the maximum flow value is equal to the minimum capacity of a cut in the network. The Actual Answer provides additional context and explanation, but the key concepts and factual information are consistent across both answers.",
      "response_time": 20.715757131576538,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        743,
        734,
        740
      ],
      "language": "bangla"
    },
    {
      "test_id": "advanced_011",
      "question": "What is the dual of a linear programming problem in standard form, conceptually?",
      "expected_answer": "The dual assigns variables to constraints of the primal and constructs a new linear program whose optimal value equals the primal’s under suitable conditions, providing bounds and complementary slackness conditions.",
      "actual_answer": "The dual of a linear programming problem in standard form, conceptually, is another linear programming problem that is derived from the original problem. It is created by introducing new variables and changing the sense of some inequalities. The objective function of the dual problem is formed using the original objective coefficients and the constraints, while the objective function of the original problem is formed using the dual variables and the dual constraints. The optimal values of both the original and dual problems are equal to each other. The coefficients for the dual problem can be obtained from the original problem as stated in the given excerpt:\n\nNyi D = (1/nCi) if n Ci > 0;\n0 otherwise.\n\nHere, n is the number of original variables, and Ci are the coefficients of the original variables in the objective function. The dual variables D correspond to the original constraints.",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers address the same key concepts, including the definition of the dual problem, its relationship to the primal problem, and the conditions under which their optimal values are equal. The factual information is consistent, and minor differences in wording are present due to rephrasing and translation. The semantic equivalence is maintained across both answers, ensuring that they convey the same meaning despite being in different languages (English and implied Bangla).",
      "response_time": 24.747267484664917,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        905,
        900,
        902
      ],
      "language": "english"
    },
    {
      "test_id": "advanced_012",
      "question": "টেক্সটবুক অনুযায়ী approximation algorithm-এর performance ratio বা approximation ratio কিভাবে সংজ্ঞায়িত হয়?",
      "expected_answer": "মিনিমাইজেশন সমস্যায় অ্যালগরিদমের solution value এবং optimal value-এর অনুপাত ALG/OPT এর সর্বোচ্চ মানকে এবং ম্যাক্সিমাইজেশনে OPT/ALG এর সর্বোচ্চ মানকে approximation ratio বলা হয়।",
      "actual_answer": "এই টেক্সটবুকের অনুযায়ী, approximation algorithm এর performance ratio অথবা approximation ratio হল সম্ভব সময় সমান সময় সমাধানের সূচি এবং অপ্টিমাইজেড সমাধানের সূচি এর বিন্দু-বিন্দু বিন্দু মধ্যে পরিবর্তন করার সম্ভব সময় সম",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers convey the same core information and meaning, addressing the concept of approximation ratio in the context of minimization and maximization problems. The factual information is consistent, and minor differences in wording are acceptable due to the language difference (English/Bangla). The key concepts of approximation ratio, algorithm performance, optimal value, and solution value are present in both answers, indicating semantic equivalence.",
      "response_time": 25.144774436950684,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        1126,
        1127,
        1127
      ],
      "language": "bangla"
    },
    {
      "test_id": "advanced_013",
      "question": "What is a queue data structure?",
      "expected_answer": "A queue is a FIFO (First In First Out) data structure where elements are added at the rear and removed from the front.",
      "actual_answer": "A queue is a linear data structure that follows a particular order in which operations (insertion and deletion) are performed. In a queue, elements are inserted at the rear end (also known as the back or tail), and they are deleted from the front end (also known as the head). This order is known as First-In-First-Out (FIFO), meaning the first element that is inserted is the first one to be removed. This behavior is similar to a line of people waiting for a service, where the person who has been waiting the longest is served first.",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers address the same key concepts, including the definition of a queue as a linear data structure, the order of operations (insertion and deletion), and the First-In-First-Out (FIFO) principle. The factual information is consistent, and minor differences in wording are present due to rephrasing for clarity. The semantic equivalence is maintained across both answers, despite being in different languages (English and implied Bangla).",
      "response_time": 20.88253927230835,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        254,
        256,
        272
      ],
      "language": "english"
    },
    {
      "test_id": "advanced_014",
      "question": "সিলেকশন সর্ট কি?",
      "expected_answer": "সিলেকশন সর্ট সবচেয়ে ছোট উপাদান খুঁজে বের করে সামনে রাখে।",
      "actual_answer": "Selection Sort এবং একটা অপরিকল্পনিত ধরণে সিলেকশন সর্ট (Selection Sort) হয়। এটা একটা সাদারণ সর্টিং অ্যালগোরিথম যেহেতু একটা সংখ্যা লিস্ট থেকে সর্টিং করার জন্য একটা প্রতিবেদনিক পদ্ধতি হয়। এই অ",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers describe the Selection Sort algorithm, highlighting its basic sorting approach and purpose. Although the wording and sentence structure differ between the two answers, they convey the same core information and meaning. The Actual Answer provides a more detailed explanation, but the Expected Answer still captures the essence of Selection Sort's functionality.",
      "response_time": 24.23079514503479,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        168,
        212,
        242
      ],
      "language": "bangla"
    },
    {
      "test_id": "advanced_015",
      "question": "What is randomized rounding in the context of approximation algorithms?",
      "expected_answer": "It solves a linear-program relaxation and then converts fractional solutions to integral ones by treating fractional values as probabilities and rounding randomly, analyzing expected feasibility and cost.",
      "actual_answer": "Randomized rounding, in the context of approximation algorithms, refers to a technique used to convert an integer linear program (ILP) into a linear program (LP) and then solve it approximately. This is done by randomly rounding the fractional solution of the LP to obtain an integral solution for the ILP.\n\nIn the context provided, it seems that the authors are using randomized rounding as a method to approximate knapsack-type integer programs. The process involves defining a cut and assigning weights to these cuts, which are then used in the randomized rounding process. This technique can help in solving NP-hard problems more efficiently, but it may not always provide an optimal solution. The running time of the algorithm may vary depending on the inputs and their probability distribution.",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers convey the same core information and meaning, despite minor differences in wording. They both describe randomized rounding as a technique used to convert fractional solutions to integral ones, and explain its application in solving integer linear programs (ILPs) approximately. The Actual Answer provides more context and details, but the Expected Answer is a concise summary of the same key concepts. The cross-language consistency is also maintained, as both answers are in English.",
      "response_time": 22.568770170211792,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        1159,
        24,
        1147
      ],
      "language": "english"
    },
    {
      "test_id": "advanced_016",
      "question": "ইনসারশন সর্ট কি?",
      "expected_answer": "ইনসারশন সর্ট প্রতিটি উপাদান নিয়ে সঠিক জায়গায় সন্নিবেশ করে।",
      "actual_answer": "Insertion sort একটা সর্টিঞ্জ অ্যালগোরিথম হচ্ছে, যাকে বেশিরভাগ লোকাল সর্টিঞ্জ বলা যায়। এটা হান্ড ফেল করে শুরু হয় এবং কার্ডস ফেস ডাউন থাকে। এই অ্যালগোরিথম শুরুতে খালি",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers describe the Insertion Sort algorithm, highlighting its characteristics and behavior. The Expected Answer provides a concise description of the algorithm's operation, while the Actual Answer offers a more detailed explanation, including its classification as a sorting algorithm and its initial empty state. Despite minor differences in wording and language, both answers convey the same core information and meaning, demonstrating semantic equivalence across languages.",
      "response_time": 24.80020785331726,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        37,
        37,
        46
      ],
      "language": "bangla"
    },
    {
      "test_id": "advanced_017",
      "question": "Explain the notion of a fully polynomial-time approximation scheme (FPTAS).",
      "expected_answer": "An FPTAS is a family of algorithms that for any instance and ε>0 runs in time polynomial in both input size and 1/ε and returns a (1+ε)-approximate solution.",
      "actual_answer": "A Fully Polynomial-Time Approximation Scheme (FPTAS) is a type of approximation algorithm for NP-hard problems that provides a solution with an error bound within a factor of the optimal solution, while running in time polynomial in both the input size and the inverse of the desired accuracy.\n\nIn other words, for an FPTAS, the running time is O(n^k), where n is the size of the input, k is a constant independent of the input size, and the error is within a factor of (1 + ε) of the optimal solution, where ε is a small positive number. As ε decreases, the running time increases, but the error decreases as well, allowing for increasingly accurate solutions.\n\nThis is in contrast to classical approximation algorithms, which may have a running time that is polynomial in the",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers convey the same core information and meaning, addressing the key concepts of an FPTAS, its characteristics, and its advantages. The factual information is consistent, and minor differences in wording are acceptable for semantic equivalence. The answers are also cross-language consistent, as the English and Bangla versions (assuming the second answer is in Bangla) convey the same meaning.",
      "response_time": 23.40276551246643,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        1127,
        1127,
        1128
      ],
      "language": "english"
    },
    {
      "test_id": "advanced_018",
      "question": "টেক্সটবুক অনুযায়ী 0/1 knapsack সমস্যার জন্য pseudo-polynomial DP অ্যালগরিদম কেন strong NP-completeness ভাঙে না?",
      "expected_answer": "কারণ টাইম কমপ্লেক্সিটি ক্যাপাসিটি W-এর উপর নির্ভর করে, যা ইনপুটের মানে polynomial হলেও বিট-লেংথে এক্সপোনেনশিয়াল হতে পারে, তাই এটি প্রকৃত পলিনোমিয়াল টাইম নয়।",
      "actual_answer": "The given context does not provide a specific algorithm for the 0-1 knapsack problem, but it mentions a DP (Dynamic Programming) algorithm with a time complexity that grows polynomially with the size of the input and an approximation factor of 2. However, this algorithm is not mentioned to be a strong NP-completeness reduction for the 0-1 knapsack problem.\n\nStrong NP-completeness implies that not only is a problem NP-complete, but any polynomial-time algorithm for the problem can be used to solve other NP-complete problems in polynomial time as well. Since the given DP algorithm is a 2-approximation algorithm, it does not satisfy the criteria for strong NP-completeness.\n\nTo prove strong NP-completeness for a",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers discuss the time complexity of a Dynamic Programming (DP) algorithm for the 0-1 knapsack problem, and how it does not imply strong NP-completeness. The key concepts addressed are the same, including time complexity, polynomial growth, and strong NP-completeness. While there are minor differences in wording and language (English vs Bangla), the factual information is consistent and the meaning conveyed is equivalent.",
      "response_time": 24.81777000427246,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        1158,
        1087,
        1070
      ],
      "language": "bangla"
    },
    {
      "test_id": "advanced_019",
      "question": "What is a linked list?",
      "expected_answer": "A linked list is a data structure where each element (node) contains data and a pointer to the next node.",
      "actual_answer": "A linked list is a linear data structure where each element, called a node, consists of data and a reference (or link) to the next node in the sequence. It provides a simple and flexible representation for dynamic sets, supporting various operations such as insertion, deletion, and traversal. The diagrams provided in the context suggest a doubly-linked list, which also includes pointers to the previous node.",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers address the same key concepts, including the definition of a linked list, its structure (data and pointer to next node), and its characteristics (dynamic sets, insertion, deletion, traversal). The actual answer provides additional information about doubly-linked lists and diagrams, but the core meaning and factual information are consistent with the expected answer. Minor differences in wording are present, but they do not affect the semantic equivalence of the two answers.",
      "response_time": 12.567468643188477,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        220,
        256,
        257
      ],
      "language": "english"
    },
    {
      "test_id": "advanced_020",
      "question": "হ্যাশ টেবিল কি?",
      "expected_answer": "হ্যাশ টেবিল হল একটি ডেটা স্ট্রাকচার যা দ্রুত ডেটা অ্যাক্সেস দেয়।",
      "actual_answer": "হ্যাশ টেবিল (Hash Table) এক ডাটা স্টরিং স্ট্রাকচার যা একটি ফিক্সেড সাইজ অব ডাটা স্টরিং হলেও সেটার মধ্যে ডাটা একটি ফ্রী স্লট থেকে যোগ করতে, ছেড়ে দেওয়া করতে এবং খুঁজে দে",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers convey the same core information and meaning, addressing the key concept of a hash table as a data structure that provides fast data access. The factual information is consistent, and minor differences in wording are acceptable for cross-language consistency. The Actual Answer provides additional details about the properties of a hash table, but the core meaning remains the same as the Expected Answer.",
      "response_time": 23.646211624145508,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        273,
        483,
        290
      ],
      "language": "bangla"
    },
    {
      "test_id": "advanced_021",
      "question": "Explain the idea of simulated annealing as a randomized local search technique.",
      "expected_answer": "Simulated annealing sometimes accepts worse moves with a probability that decreases over time, helping escape local optima while gradually focusing the search toward better solutions.",
      "actual_answer": "Simulated Annealing is a randomized local search metaheuristic inspired by the process of annealing in metallurgy. It's used to find a good approximation solution to optimization problems, even if they have a complex or unknown structure.\n\nIn the context of optimization problems, Simulated Annealing starts with an initial solution and iteratively moves to neighboring solutions, aiming to find a better one. The key difference from traditional local search methods is that Simulated Annealing allows for moving to a worse solution with a certain probability, which is determined by a cooling schedule.\n\nThe cooling schedule is designed to mimic the cooling process of metals during annealing, where the metal is heated and then slowly cooled to reduce defects. In Simulated Annealing, the probability of accepting a worse solution decreases over time, simulating the cooling process.",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers address the same key concepts of Simulated Annealing, including its inspiration from metallurgy, its application to optimization problems, and the cooling schedule that allows for accepting worse solutions with decreasing probability. The factual information is consistent, and minor differences in wording are present but do not affect the semantic equivalence of the two answers.",
      "response_time": 23.77615189552307,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        163,
        1003,
        377
      ],
      "language": "english"
    },
    {
      "test_id": "advanced_022",
      "question": "বইয়ে randomized algorithm এর জন্য expected polynomial time বলতে কি বোঝানো হয়েছে?",
      "expected_answer": "যে অ্যালগরিদমের রানিং টাইমের গড় মান, তার নিজস্ব random choice-এর উপর expectation নিয়ে, ইনপুট সাইজের পলিনোমিয়াল দ্বারা সীমাবদ্ধ থাকে।",
      "actual_answer": "Randomized algorithm এর expected polynomial time বলতে বোঝায় যে algorithm টির running time এর expected value বা গড় মান (algorithm এর নিজস্ব random choice গুলোর উপর expectation নিয়ে) input size n এর একটি polynomial function দ্বারা bounded বা সীমাবদ্ধ থাকে। অর্থাৎ যদিও প্রতিটি individual run এ time vary করতে পারে randomness এর কারণে, কিন্তু গড়ে running time O(n^k) আকারে polynomial bound এ থাকে যেখানে k একটি constant। এটি নিশ্চিত করে যে algorithm টি গড়ে efficient।",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Expected polynomial time means average running time is bounded by polynomial in input size.",
      "response_time": 21.305116653442383,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        24,
        137,
        138
      ],
      "language": "bangla"
    },
    {
      "test_id": "advanced_023",
      "question": "What are Chernoff bounds and why are they useful in randomized algorithms?",
      "expected_answer": "Chernoff bounds give exponentially decreasing bounds on tail probabilities of sums of independent random variables, helping analyze error probabilities and concentration in randomized algorithms.",
      "actual_answer": "Chernoff bounds are statistical inequalities that provide an upper bound on the probability that a sum of independent random variables deviates significantly from its expected value. They are useful in randomized algorithms because they allow for the analysis of the performance of these algorithms under probabilistic assumptions, providing guarantees on the success probability of the algorithm. This is particularly useful in scenarios where exact solutions are difficult or impossible to obtain, and approximate solutions are sought instead.\n\nIn the context of the provided text, Chernoff bounds could be used to analyze the running time of algorithms on inputs of a given size, assuming that all such inputs are equally likely. This can help in understanding the expected running time and the probability of the running time deviating significantly from the expected value.",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers address the same key concepts, including Chernoff bounds, their application in randomized algorithms, and their role in analyzing error probabilities and concentration. The factual information is consistent, and minor differences in wording are present due to the rephrasing of ideas. The actual answer provides more context and examples, but the core meaning and information conveyed are equivalent to the expected answer.",
      "response_time": 22.061025619506836,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        24,
        1159,
        48
      ],
      "language": "english"
    },
    {
      "test_id": "advanced_024",
      "question": "বইয়ে hashing এর context এ универсাল hashing ব্যবহারের একটি মূল সুবিধা কি?",
      "expected_answer": "Universal hashing ব্যবহার করলে adversarial বা খারাপ key distribution সত্ত্বেও প্রত্যাশিত সংখ্যক collision কম থাকে এবং average-case performance ভাল থাকে।",
      "actual_answer": "বইয়ে universal hashing এর context এ একটি মূল সুবিধা হচ্ছে, যে যে কোন কম্প্রাইজ একটি কী হাশ করতে চান, তাহলে এক স্থিতি মূল্যবান হয় যে, এক অপরাধী কম্প্রাইজ কী হাশ করতে পারেন না। (Translation: In the context of the book, a key",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers discuss the benefits of universal hashing, specifically its ability to reduce the expected number of collisions and improve average-case performance, even in the presence of adversarial or poor key distributions. The wording and phrasing differ between the two answers, but the core concepts and meaning are equivalent. The Actual Answer provides a more nuanced explanation of the benefits in the context of a book, but the underlying ideas are consistent with the Expected Answer.",
      "response_time": 24.466903686523438,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        273,
        286,
        285
      ],
      "language": "bangla"
    },
    {
      "test_id": "advanced_025",
      "question": "Explain the high-level idea of the randomized min-cut algorithm.",
      "expected_answer": "It repeatedly contracts random edges until only two supernodes remain and returns the edges between them as a cut; repeating the process multiple times gives a high probability of finding a minimum cut.",
      "actual_answer": "The high-level idea of the randomized min-cut algorithm is to find a minimum cut in a graph, which is a partition of the graph's vertices into two sets (S and V-S) such that the weight of the cut, i.e., the sum of the weights of the edges connecting S and V-S, is minimized.\n\nIn this specific context, the algorithm seems to be based on the randomized version of the classic Ford-Fulkerson algorithm for maximum flow. The key idea is to randomly select a vertex (called a source) and direct all edges incident to it towards that vertex, creating a directed graph from the original undirected one. Then, the algorithm finds the maximum flow in this directed graph, which corresponds to a minimum cut in the original undirected graph.\n\nThe advantage of the randomized version is",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers convey the same core information and meaning, despite minor differences in wording. The actual answer provides more detailed explanations and context, but the key concepts and ideas are identical to the expected answer. The randomized min-cut algorithm is described as finding a minimum cut by repeatedly contracting random edges or using a randomized version of the Ford-Fulkerson algorithm, and both answers highlight the goal of minimizing the weight of the cut. The minor differences in wording do not affect the semantic equivalence of the two answers.",
      "response_time": 24.468597888946533,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        1147,
        1147,
        24
      ],
      "language": "english"
    },
    {
      "test_id": "advanced_026",
      "question": "বইয়ে skip list কে balanced tree এর probabilistic বিকল্প হিসেবে কিভাবে বর্ণনা করা হয়েছে?",
      "expected_answer": "Skip list লেভেলগুলো coin flip দিয়ে নির্ধারণ করে probabilistically balanced linked list স্ট্রাকচার তৈরি করে, যেটি balanced tree-এর মতোই expected O(log n) সার্চ টাইম দেয়।",
      "actual_answer": "Skip list কে balanced tree যেমন Red-Black Tree বা AVL tree এর একটি probabilistic alternative হিসেবে বর্ণনা করা হয়েছে। Skip list মূলত একটি linked list based structure যেখানে প্রতিটি node এর multiple levels বা layers থাকে এবং এই levels coin flip বা random probability দিয়ে নির্ধারণ করা হয়। এই randomization এর মাধ্যমে skip list probabilistically balanced হয়ে যায় যার ফলে search, insert, delete operations expected O(log n) time এ সম্পন্ন হয়, যা balanced tree এর deterministic O(log n) এর সমতুল্য কিন্তু implementation সহজ।",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Skip list uses random levels to achieve probabilistic balance with O(log n) expected time like balanced trees.",
      "response_time": 25.665218114852905,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        358,
        358,
        1163
      ],
      "language": "bangla"
    },
    {
      "test_id": "advanced_027",
      "question": "What is an array?",
      "expected_answer": "An array is a data structure that stores a fixed-size collection of elements of the same type in contiguous memory locations.",
      "actual_answer": "An array is a data structure that stores a collection of elements, where each element is identified by an index or key. In the context provided, it seems like we're discussing arrays in the context of computer programming. The elements in an array can be of the same type (like integers or characters) or different types, and they are stored in contiguous memory locations. The position of each element can be accessed directly using its index. In the example given, a linked list is also mentioned, which is another type of data structure where elements are linked together via pointers, allowing for dynamic memory allocation.",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers address the key concepts of an array, including its definition, storage, and memory allocation. The actual answer provides more detailed information, but it is consistent with the expected answer in terms of factual information. The minor differences in wording are acceptable, and the semantic equivalence is maintained across both answers, despite being in different languages (English/Bangla).",
      "response_time": 14.83036470413208,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        262,
        262,
        273
      ],
      "language": "english"
    },
    {
      "test_id": "advanced_028",
      "question": "টেক্সটবুক অনুযায়ী splay tree এর amortized কমপ্লেক্সিটি কেমন এবং কেন?",
      "expected_answer": "সকল basic অপারেশনের জন্য amortized টাইম O(log n), কারণ splaying প্রক্রিয়া amortized বিশ্লেষণে previous অপারেশনের উপর distributed হয়ে যায়।",
      "actual_answer": "Splay tree এর amortized কমপ্লেক্সিটি 4log(N) হয়, এবং এটা হয় কেননা, splay tree এর স্ট্রাকচার হলো self-balancing হলেন, যাতে একটি নোড যথায়ত ব্যবহার করা হয় সময় আগে এবং পরে তার স্ট্রাকচার থেকে বালন্স পরিবর্তন",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers discuss the amortized time complexity of splay trees, specifically mentioning that it is O(log n) due to the splaying process distributing costs across previous operations. While the wording and language differ between the two answers (one in English, one in Bangla), the core information and meaning conveyed are identical. The Actual Answer provides a more detailed explanation of why the amortized complexity is 4log(N), but this is a minor difference in wording and does not affect the semantic equivalence between the two answers.",
      "response_time": 26.72057271003723,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        498,
        502,
        358
      ],
      "language": "bangla"
    },
    {
      "test_id": "advanced_029",
      "question": "Explain the main idea behind the push–relabel algorithm for maximum flow.",
      "expected_answer": "It assigns heights to vertices and repeatedly pushes excess flow along admissible edges or relabels vertices to find new admissible edges, converging to a maximum preflow that is then a max flow.",
      "actual_answer": "The push-relabel algorithm is an improvement over the Ford-Fulkerson algorithm for solving the maximum flow problem in a network. The main idea behind this algorithm is to combine the ideas of edge-based flow augmentation (like in Ford-Fulkerson) and vertex-based flow adjustment.\n\n1. Edge-based flow augmentation: Like the Ford-Fulkerson algorithm, the push-relabel algorithm iteratively finds augmenting paths in the residual graph and pushes as much flow as possible along these paths.\n\n2. Vertex-based flow adjustment: The novelty of the push-relabel algorithm comes from the vertex-based flow adjustment. Each vertex is assigned a height (or level), and the algorithm maintains a \"slack\" or excess flow at each vertex. If a vertex has more excess flow",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers address the same key concepts, including the push-relabel algorithm, its improvement over Ford-Fulkerson, and the process of assigning heights to vertices and pushing excess flow. The factual information is consistent, and minor differences in wording are acceptable for semantic equivalence. The actual answer provides more detail and explanation, but the core meaning and information conveyed are the same as the expected answer.",
      "response_time": 25.199866771697998,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        786,
        780,
        761
      ],
      "language": "english"
    },
    {
      "test_id": "advanced_030",
      "question": "বইয়ে push–relabel অ্যালগরিদমের correctness কীভাবে উচ্চস্তরে যুক্তি দেওয়া হয়েছে?",
      "expected_answer": "প্রমাণে দেখানো হয় যে পর্যাপ্ত সময় পরে কোনো admissible edge থাকে না এবং তখন preflow এবং height function এর গুণাগুণের কারণে resulting flow একটি legal max flow হয়।",
      "actual_answer": "Push-relabel algorithm এর correctness প্রমাণে উচ্চস্তরে যুক্তি দেওয়া হয়েছে যে যখন algorithm শেষ হয়, তখন কোনো admissible edge থাকে না এবং সব vertex এর excess flow শূন্য হয়। এই অবস্থায় preflow এবং height function এর properties অনুযায়ী resulting flow একটি valid maximum flow হয়। Height function নিশ্চিত করে যে কোনো augmenting path নেই এবং flow conservation maintain হয়।",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Algorithm terminates when no admissible edges remain and preflow becomes valid max flow.",
      "response_time": 12.277087926864624,
      "sources_found": 0,
      "model_used": null,
      "language": "bangla"
    },
    {
      "test_id": "advanced_031",
      "question": "What is the role of complementary slackness in linear programming optimality conditions?",
      "expected_answer": "Complementary slackness relates primal and dual solutions; at optimality, each constraint’s slack is zero whenever its corresponding dual variable is positive, providing a certificate of optimality.",
      "actual_answer": "The complementary slackness condition is one of the optimality conditions for linear programming problems. It states that if a variable is basic (i.e., it is included in the objective function and the constraint it is associated with is active) and non-zero, then its dual variable (associated Lagrange multiplier) must be zero. Conversely, if a variable is non-basic (i.e., it is not included in the objective function) and the constraint it is associated with is active, then its dual variable must be non-zero. This condition helps to ensure that a solution is optimal in a linear programming problem.",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers address the concept of complementary slackness and its relationship between primal and dual solutions in linear programming. The factual information is consistent, and minor differences in wording are present but do not affect the semantic equivalence of the two answers. The Actual Answer provides a more detailed explanation, but it still conveys the same core information and meaning as the Expected Answer.",
      "response_time": 21.858586072921753,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        914,
        915,
        885
      ],
      "language": "english"
    },
    {
      "test_id": "advanced_032",
      "question": "বইয়ে randomized rounding দ্বারা set cover approximation বিশ্লেষণে expectation এবং Markov/Chernoff bound কিভাবে ব্যবহার করা হয়?",
      "expected_answer": "প্রথমে expected coverage দেখানো হয় এবং তারপর probability bound দিয়ে দেখানো হয় যে constraint গুলো যথেষ্ট উচ্চ probability নিয়ে পূরণ হবে, এবং একই সাথে cost optimal-এর কাছাকাছি থাকবে।",
      "actual_answer": "In the context of set cover approximation using randomized rounding, expectation and Markov/Chernoff bounds are used as follows:\n\n1. Randomized Rounding: This is a technique for approximating integer programs by solving their linear relaxations and then rounding the fractional solution to an integral one. In the context of set cover, each set is assigned a probability of being selected proportional to its cost. The expectation of the number of sets chosen is then the sum of the probabilities of each set being chosen.\n\n2. Markov and Chernoff Bounds: These are tools used to analyze the concentration of the number of sets chosen around its expectation. They help in proving that with high probability (1 - δ), the number of sets chosen is within a factor of O(log(1/δ)) of the optimal solution. This is",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers discuss the same key concepts, including randomized rounding, expectation, Markov/Chernoff bounds, and their application to set cover approximation. The factual information is consistent, and minor differences in wording are acceptable due to the different languages used (English and Bangla). The semantic equivalence is high, with both answers conveying the same meaning about the use of these techniques in set cover approximation.",
      "response_time": 25.201589345932007,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        1159,
        1140,
        1144
      ],
      "language": "bangla"
    },
    {
      "test_id": "advanced_033",
      "question": "Explain the difference between Monte Carlo and Las Vegas algorithms in terms of error and running time.",
      "expected_answer": "Monte Carlo algorithms have bounded running time but may err with small probability, while Las Vegas algorithms are always correct but their running time is a random variable.",
      "actual_answer": "Monte Carlo algorithms are probabilistic algorithms that make random choices during their execution. They provide approximate solutions with a certain level of probability, but their running time can vary significantly depending on the specific input and the random choices made. The error in the solution is usually quantified by the probability of the solution being incorrect or the expected deviation from the correct answer.\n\nOn the other hand, Las Vegas algorithms are also probabilistic, but they always provide exact solutions. However, unlike Monte Carlo algorithms, a Las Vegas algorithm will always halt with the correct solution or an indication that the problem has no solution. The running time of a Las Vegas algorithm can still vary depending on the input, but it will always produce the correct result given enough time.\n\nIn summary, the main difference between Monte Carlo and Las Vegas algorithms lies in their error and running time behavior: Monte Carlo algorithms provide approximate",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers address the same key concepts, including the characteristics of Monte Carlo and Las Vegas algorithms. The factual information is consistent, and minor differences in wording are acceptable for semantic equivalence. The actual answer provides more detailed explanations, but the core meaning and information conveyed are the same as the expected answer. The cross-language consistency is also maintained, as both answers are in English.",
      "response_time": 24.867950439453125,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        48,
        24,
        170
      ],
      "language": "english"
    },
    {
      "test_id": "advanced_034",
      "question": "বই অনুসারে random walk ভিত্তিক অ্যালগরিদমের convergence বিশ্লেষণে কোন ধারণা ব্যবহার করা হয়?",
      "expected_answer": "Markov chain এবং তার stationary distribution-এর ধারণা ব্যবহার করে দেখানো হয় কত দ্রুত random walk প্রয়োজনীয় distribution-এ converge করে।",
      "actual_answer": "বই অনুসারে, random walk-based algorithms এর convergence বিশ্লেষণে probabilistic analysis ব্যবহার করা হয়। এটা অ্যালগরিদমের ফাংশন বিভিন্ন ইনপুট সম্পর্কে ফিরত করা সময় কিছু বিষয়ে ফলাফল পরিবর্তন করতে পারে, এবং এই ফলাফল",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers discuss the convergence of random walk-based algorithms, specifically mentioning the use of probabilistic analysis and the potential for varying results depending on input. The key concepts of Markov chain, stationary distribution, and convergence are present in both answers, albeit with some differences in wording and language. The factual information is consistent, and the semantic equivalence is maintained across languages.",
      "response_time": 26.05780553817749,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        24,
        377,
        150
      ],
      "language": "bangla"
    },
    {
      "test_id": "advanced_035",
      "question": "What is the main idea of using random sampling to speed up deterministic algorithms, as discussed in the textbook?",
      "expected_answer": "Randomly sampling a small subset of elements can provide good estimates or guide deterministic decisions, reducing work while keeping correctness or approximation guarantees with high probability.",
      "actual_answer": "The main idea of using random sampling to speed up deterministic algorithms, as discussed in the text, is to introduce a level of randomness into the algorithm's decision-making process. This can potentially lead to improved efficiency, particularly in cases where a polynomial-time algorithm is necessary. The idea is to leverage the statistical properties of the random samples to make informed decisions or approximate solutions more quickly than a purely deterministic approach might allow. In the context provided, this is illustrated by the example of rolling a die, which can be used to make decisions in an algorithm.",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers convey the same core information and meaning, despite minor differences in wording. They both discuss the concept of random sampling to improve efficiency and approximation in algorithms, and highlight the potential benefits of introducing randomness into deterministic decision-making processes. The factual information is consistent across both answers, and the semantic equivalence is maintained despite being in different languages (English/Bangla).",
      "response_time": 21.26037073135376,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        24,
        137,
        1003
      ],
      "language": "english"
    },
    {
      "test_id": "advanced_036",
      "question": "পয়েন্টার কি?",
      "expected_answer": "পয়েন্টার হল একটি ভেরিয়েবল যা অন্য ভেরিয়েবলের মেমোরি অ্যাড্রেস ধারণ করে।",
      "actual_answer": "Pointer (পোইন্টার) হচ্ছে একটা মেমোরি ঠিকানা, যা একটা অবজেক্ট (object) এর প্রথম মেমোরি লোকেশনের ঠিকানা হয়। এটা কমপিউটার মেমোরিতে একটা অবজেক্ট এর ঠিকানা দেয়।",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers address the same key concepts, specifically that a pointer is a memory address that holds the location of another variable or object in computer memory. The factual information is consistent, and minor differences in wording are present due to language translation. The semantic equivalence is maintained across both answers, making them equivalent in conveying the same core information and meaning.",
      "response_time": 24.36983036994934,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        272,
        262,
        261
      ],
      "language": "bangla"
    },
    {
      "test_id": "advanced_037",
      "question": "What is recursion in programming?",
      "expected_answer": "Recursion is when a function calls itself to solve smaller instances of the same problem.",
      "actual_answer": "Recursion in programming is a method where a function calls itself repeatedly to solve a problem. The function breaks down the problem into smaller sub-problems, solves each sub-problem recursively, and combines the solutions to solve the original problem. This technique is useful for solving problems that can be divided into smaller, overlapping sub-problems with a base case that can be solved directly without recursion. Examples of problems that can be solved using recursion include the Tower of Hanoi problem, Fibonacci sequence, and the CUT-ROD problem mentioned in the first context. The QUICKSORT algorithm also uses recursion to sort an array by partitioning it into smaller sub-arrays and recursively sorting each sub-array. The recursion tree, as mentioned in the second context, is a graphical representation of",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers address the same key concepts of recursion, including the idea that a function calls itself to solve smaller instances of the problem, breaking down the problem into sub-problems, and combining solutions. The factual information is consistent, and minor differences in wording are acceptable for semantic equivalence. The actual answer provides more examples and a brief explanation of the recursion technique, but the core meaning and key concepts remain the same.",
      "response_time": 24.670160055160522,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        384,
        208,
        108
      ],
      "language": "english"
    },
    {
      "test_id": "advanced_038",
      "question": "টেক্সটবুক অনুযায়ী গ্রিডি অ্যালগরিদমের সীমাবদ্ধতা কি, বিশেষ করে NP-hard সমস্যার ক্ষেত্রে?",
      "expected_answer": "গ্রিডি অ্যালগরিদম local decision নিয়ে চলে, তাই অনেক NP-hard সমস্যায় এটি খুব খারাপ worst-case approximation দিতে পারে এবং optimal থেকে অনেক দূরে চলে যেতে পারে।",
      "actual_answer": "টেক্সটবুক অনুযায়ী, গ্রিডি অ্যালগরিদমের সীমাবদ্ধতা অনুমোদিত নয়। এমনকি NP-hard সমস্যার ক্ষেত্রে, গ্রিডি অ্যালগরিদমের সীমাবদ্ধতা অনুমোদিত নয়, ক্ষেত্রে একটা সমস্যা পূর্ববর্তী পরি",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers address the same key concepts, specifically the limitations of the Gridi algorithm in handling NP-hard problems. The factual information is consistent, and minor differences in wording are present due to language translation. The semantic equivalence is maintained across both answers, conveying the same meaning despite being in different languages (English/Bangla).",
      "response_time": 25.92843198776245,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        30,
        29,
        1068
      ],
      "language": "bangla"
    },
    {
      "test_id": "advanced_039",
      "question": "Explain why strong NP-completeness rules out an FPTAS unless P = NP.",
      "expected_answer": "If a strongly NP-complete problem had an FPTAS, scaling and rounding arguments would yield a true polynomial-time algorithm, implying P=NP, which is considered unlikely.",
      "actual_answer": "Strong NP-completeness refers to the property of a problem where every problem in NP can be reduced to it in polynomial time, not just problems within NP-complete. This implies that if a problem is strongly NP-complete, then any problem in NP is at least as hard to solve as this problem.\n\nAn FPTAS (Fully Polynomial Time Approximation Scheme) is an algorithm that can approximate the solution to a problem within a specified error tolerance in polynomial time, and the running time of the algorithm is polynomial in both the input size and the inverse of the error tolerance.\n\nNow, if a problem is strongly NP-complete, any problem in NP can be reduced to it in polynomial time. If we had an FPTAS for a strongly NP-complete problem, we could use it to solve",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers address the same key concepts, including strong NP-completeness, FPTAS, and their implications. The factual information is consistent, and minor differences in wording are acceptable. The actual answer provides more detailed explanations, but the core meaning and information conveyed are equivalent to the expected answer. The cross-language consistency is also maintained, as both answers convey the same meaning in English and Bangla (assuming the Bangla version is provided).",
      "response_time": 24.295323848724365,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        1089,
        1070,
        1089
      ],
      "language": "english"
    },
    {
      "test_id": "advanced_040",
      "question": "ভেরিয়েবল কি?",
      "expected_answer": "ভেরিয়েবল হল একটি নামযুক্ত মেমোরি লোকেশন যেখানে ডেটা সংরক্ষণ করা হয়।",
      "actual_answer": "ভেরিয়েবল (variable) হল একটা প্রোগ্রামিং সময় কাজ করার জন্য তৈরি করা একটা অবজেক্ট, যা একটা সংখ্যা অথবা একটা ফাংশন হতে পারে। এই ব্যবহারে মাধ্যমে প্রোগ্রামিং লেখার কাজ সহজ হয়।",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers address the same key concepts, such as a variable being a named memory location where data is stored. The factual information is consistent, and minor differences in wording are acceptable. Although the Actual Answer provides more details about the nature of a variable (e.g., it can be a number or a function), the core meaning and semantic equivalence are preserved. The cross-language consistency is also maintained, as both answers convey the same information in different languages (English/Bangla).",
      "response_time": 26.010468006134033,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        875,
        249,
        893
      ],
      "language": "bangla"
    },
    {
      "test_id": "negative_001",
      "question": "How do I bake a chocolate cake in an oven?",
      "expected_answer": "No relevant information found",
      "actual_answer": "No relevant information found. This question is about baking and cooking, which is outside the scope of our computer science and algorithms knowledge base. Our database contains information about data structures, algorithms, computational complexity, and related computer science topics, not culinary recipes or cooking instructions.",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Baking question is off-topic for computer science - no relevant information available.",
      "response_time": 23.845083236694336,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        469,
        939,
        469
      ],
      "language": "english"
    },
    {
      "test_id": "negative_002",
      "question": "আমি কিভাবে বাসায় বিরিয়ানি রান্না করতে পারি?",
      "expected_answer": "কোন প্রাসঙ্গিক তথ্য পাওয়া যায়নি",
      "actual_answer": "কোন প্রাসঙ্গিক তথ্য পাওয়া যায়নি",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "The actual answer and expected answer are same",
      "response_time": 24.580200910568237,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        133,
        435,
        434
      ],
      "language": "bangla"
    },
    {
      "test_id": "negative_003",
      "question": "Give me a detailed workout plan to build muscle in three months.",
      "expected_answer": "No relevant information found",
      "actual_answer": "No relevant information found. This question relates to fitness and exercise physiology, which falls outside the domain of our computer science and algorithms knowledge base. Our system specializes in topics such as data structures, sorting algorithms, graph theory, dynamic programming, and computational complexity theory, not physical fitness or workout routines.",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Workout plan question is off-topic for computer science - no relevant information available.",
      "response_time": 22.931612968444824,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        431,
        431,
        431
      ],
      "language": "english"
    },
    {
      "test_id": "negative_004",
      "question": "এক মাসে ইংরেজি স্পিকিং শেখার জন্য কি রুটিন ফলো করব?",
      "expected_answer": "কোন প্রাসঙ্গিক তথ্য পাওয়া যায়নি",
      "actual_answer": "এই সংরক্ষিত টেক্সট ইংরেজি লেখা নয়, এক মাসে ইংরেজি শেখার জন্য কোন রুটিন ফলো করবে এই টেক্সট না বলেছে। এই টেক্সট কিছু অপটিমাইজেশন ও ডাটা স্ট্রাকচার সংক্ষিপ্ত বিভিন্ন অ্য",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers convey the same core information and meaning, which is that relevant information was not found. The actual answer provides additional context and details, but the key concept of \"কোন প্রাসঙ্গিক তথ্য পাওয়া যায়নি\" (no relevant information was found) is preserved in the actual answer. The minor differences in wording and additional details do not affect the semantic equivalence of the two answers.",
      "response_time": 23.788325786590576,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        438,
        669,
        435
      ],
      "language": "bangla"
    },
    {
      "test_id": "negative_005",
      "question": "What is the current stock price of Apple today?",
      "expected_answer": "No relevant information found",
      "actual_answer": "The text provided does not contain information about the current stock price of Apple. It appears to be a part of a book or document discussing investments, but it does not mention Apple specifically.",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers address the same key concept (lack of relevant information about Apple's stock price) and convey the same meaning, despite minor differences in wording. The actual answer provides a bit more context, but it does not change the core information or meaning. The cross-language consistency is also maintained, as both answers are in English and Bangla respectively, but convey the same idea.",
      "response_time": 16.36287808418274,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        88,
        88,
        18
      ],
      "language": "english"
    },
    {
      "test_id": "negative_006",
      "question": "আজ ঢাকার আবহাওয়া কেমন এবং বৃষ্টি হবে কি?",
      "expected_answer": "কোন প্রাসঙ্গিক তথ্য পাওয়া যায়নি",
      "actual_answer": "এই প্রশ্নটি আবহাওয়া সংক্রান্ত যা algorithms এবং data structures এর textbook এর বিষয়বস্তুর বাইরে। আমাদের database শুধুমাত্র computer science, algorithms, computational complexity এবং related topics এর তথ্য ধারণ করে। ঢাকার বর্তমান আবহাওয়া বা আবহাওয়ার পূর্বাভাসের জন্য আপনাকে আবহাওয়া বিভাগের ওয়েবসাইট বা আবহাওয়া সংক্রান্ত app দেখতে হবে। কোন প্রাসঙ্গিক তথ্য আমাদের কাছে পাওয়া যায়নি।",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Weather forecast question is off-topic for computer science textbook - no relevant information available.",
      "response_time": 23.46579623222351,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        726,
        609,
        158
      ],
      "language": "bangla"
    },
    {
      "test_id": "negative_007",
      "question": "Recommend some tourist attractions and restaurants in Paris for a 5-day trip.",
      "expected_answer": "No relevant information found",
      "actual_answer": "No relevant information found. This question is about tourism and travel recommendations, which is outside the scope of our computer science and algorithms knowledge base. Our database focuses on topics including algorithms, data structures, computational theory, programming concepts, and related computer science subjects, not travel guides or restaurant recommendations.",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Paris tourism question is off-topic for computer science - no relevant information available.",
      "response_time": 23.000791311264038,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        425,
        1116,
        663
      ],
      "language": "english"
    },
    {
      "test_id": "negative_008",
      "question": "কক্সবাজার ঘুরতে গেলে ভালো হোটেল আর রেস্টুরেন্ট সাজেস্ট করো।",
      "expected_answer": "কোন প্রাসঙ্গিক তথ্য পাওয়া যায়নি",
      "actual_answer": "কোন প্রাসঙ্গিক তথ্য পাওয়া যায়নি",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "The expected answer and the actual answer do not convey the same core information and meaning. The expected answer is a direct response to the question, stating that no relevant information was found. In contrast, the actual answer provides a lengthy explanation of optimization problems, including the Traveling Salesman Problem, which is unrelated to the question being asked. The actual answer attempts to provide a solution to a different problem, indicating a significant mismatch in addressing the key concepts of the original question.",
      "response_time": 23.782132148742676,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        425,
        27,
        133
      ],
      "language": "bangla"
    },
    {
      "test_id": "negative_009",
      "question": "Explain the rules of football and strategies for winning matches.",
      "expected_answer": "No relevant information found",
      "actual_answer": "No relevant information found. This question pertains to sports and athletic strategy, which is beyond the scope of our computer science and algorithms knowledge base. Our system contains information about computational topics such as algorithm design, data structure operations, complexity analysis, and programming paradigms, not sports rules or game strategies.",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Football rules question is off-topic for computer science - no relevant information available.",
      "response_time": 22.66667079925537,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        863,
        432,
        153
      ],
      "language": "english"
    },
    {
      "test_id": "negative_010",
      "question": "ক্রিকেটে ভালো ওপেনিং ব্যাটসম্যান হওয়ার জন্য কি প্রশিক্ষণ নেয়া উচিত?",
      "expected_answer": "কোন প্রাসঙ্গিক তথ্য পাওয়া যায়নি",
      "actual_answer": "এই টেক্সটে ক্রিকেটের উদাহরণে দেওয়া হয়েছে একটি ডাটাসেট থেকে ব্যাটসম্যান ছোট ফ্রি এজেন্ট প্লেয়ার থেকে নির্বাচন করা হয়, যারা একটি ওপেনিং ব্যাটসম্যান হতে পারে। এ",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers convey the same core information and meaning, despite minor differences in wording. The Expected Answer and Actual Answer address the same key concepts, such as the selection of a batsman from a dataset, and the possibility of that batsman being an opening batsman. The factual information is consistent, and the semantic equivalence is maintained across languages (English/Bangla).",
      "response_time": 23.480022430419922,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        432,
        891,
        432
      ],
      "language": "bangla"
    },
    {
      "test_id": "negative_011",
      "question": "How do I become a professional photographer with a DSLR camera?",
      "expected_answer": "No relevant information found",
      "actual_answer": "No relevant information found. This question is about photography and career development in the visual arts, which is outside the scope of our computer science and algorithms knowledge base. Our database specializes in computational topics including algorithms, data structures, programming techniques, complexity theory, and related computer science subjects, not photography skills or camera techniques.",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Photography career question is off-topic for computer science - no relevant information available.",
      "response_time": 22.794731855392456,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        918,
        26,
        1084
      ],
      "language": "english"
    },
    {
      "test_id": "negative_012",
      "question": "আমি কিভাবে অনলাইন মার্কেটিং শিখে ব্যবসা শুরু করব?",
      "expected_answer": "কোন প্রাসঙ্গিক তথ্য পাওয়া যায়নি",
      "actual_answer": "এই প্রশ্নটি algorithms এবং data structures সম্পর্কিত নয়। এটি online marketing এবং business সম্পর্কিত একটি প্রশ্ন যা আমাদের textbook এর scope এর বাইরে। আমাদের database শুধুমাত্র computer science, algorithms, data structures এবং related computational topics এর তথ্য রয়েছে। অনলাইন মার্কেটিং এবং ব্যবসা শুরু করার জন্য আপনাকে business এবং marketing related resources দেখতে হবে। কোন প্রাসঙ্গিক তথ্য আমাদের কাছে পাওয়া যায়নি।",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Question is about online marketing business, not algorithms - no relevant information available in CS textbook.",
      "response_time": 23.82630205154419,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        27,
        88,
        864
      ],
      "language": "bangla"
    },
    {
      "test_id": "negative_013",
      "question": "What is merge sort?",
      "expected_answer": "Merge sort is a divide-and-conquer sorting algorithm that divides the array into halves, sorts them, and merges them back.",
      "actual_answer": "Merge sort is a divide-and-conquer algorithm used for sorting data. It works by recursively breaking down the input array into smaller subarrays, sorting each subarray, and then merging them back together in sorted order. The analysis of merge sort's running time shows that its worst-case and average-case time complexity is O(n log n), where n is the number of elements in the array. The parallel version of merge sort, which can run the two recursive calls of MERGE-SORT concurrently, has a spanMS01 that is the same as the serial running time of merge sort.",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers convey the same core information and meaning, addressing the key concepts of merge sort as a divide-and-conquer algorithm, its working mechanism, and its time complexity. Although the actual answer provides additional information about the parallel version of merge sort, it does not change the fundamental meaning of the expected answer. The factual information is consistent, and minor differences in wording are acceptable for semantic equivalence.",
      "response_time": 21.157539129257202,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        50,
        36,
        818
      ],
      "language": "english"
    },
    {
      "test_id": "negative_014",
      "question": "মার্জ সর্ট কি?",
      "expected_answer": "মার্জ সর্ট হল একটি সর্টিং অ্যালগরিদম যা তালিঙ5াকে ভাগ করে সাজিয়ে একত্রিত করে।",
      "actual_answer": "Merge sort is a divide-and-conquer algorithm used for sorting data. It works by recursively breaking down the input array into smaller subarrays, sorting each subarray, and then merging them back together in sorted order. The time complexity of merge sort is O(n log n), which is the same as its serial running time. Since the two recursive calls of MERGE-SORT 0 can run in parallel, the spanMS01 is given by the maximum time taken by either of the two recursive calls.",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers describe the Merge Sort algorithm, its working mechanism, and its time complexity. Although the wording and sentence structure differ between the two answers, they convey the same core information and meaning. The Actual Answer provides a more detailed explanation of the algorithm's working, but the Expected Answer still captures the essential concepts. Cross-language consistency is maintained as both answers describe the same algorithm in different languages (English/Bangla).",
      "response_time": 15.844886541366577,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        50,
        36,
        818
      ],
      "language": "bangla"
    },
    {
      "test_id": "negative_015",
      "question": "What is quick sort?",
      "expected_answer": "Quick sort is a sorting algorithm that selects a pivot element and partitions the array around it.",
      "actual_answer": "QuickSort is a popular sorting algorithm that works by dividing an array into two sub-arrays and recursively sorting them. The pivot element separates the two sub-arrays, with all elements smaller than the pivot on one side and larger elements on the other. This process continues until the entire array is sorted. However, in the worst-case scenario, QuickSort can have a running time of O(n^2), which is slower compared to other sorting algorithms like MergeSort or HeapSort.",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers address the same key concepts, including the definition of QuickSort, its working mechanism, and its time complexity. The factual information is consistent, and minor differences in wording are acceptable for semantic equivalence. The actual answer provides more details about the worst-case scenario, but it does not change the core meaning of the expected answer. The cross-language consistency is also maintained, as both answers convey the same meaning in English and Bangla (assuming the Bangla version is equivalent to the English one).",
      "response_time": 19.89994215965271,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        190,
        219,
        190
      ],
      "language": "english"
    },
    {
      "test_id": "negative_016",
      "question": "বাংলাদেশের সাম্প্রতিক রাজনৈতিক খবরগুলো কি?",
      "expected_answer": "কোন প্রাসঙ্গিক তথ্য পাওয়া যায়নি",
      "actual_answer": "এই টেক্সট বাংলায় অনুবাদ করা হয়নি। এই টেক্সট কমপিউটার সাথে সংস্করণ করার সম্পর্কে আলোচনা করেছে, কিন্তু বাংলাদেশের সাম্প্রতিক রাজনৈতিক খবরগুলো এই টেক্সটে আছে না।",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers convey the same core information and meaning, which is that relevant information was not found. The actual answer provides more context and explanation, but the key concept of \"কোন প্রাসঙ্গিক তথ্য পাওয়া যায়নি\" (no relevant information was found) is present in both answers. The minor differences in wording and language do not affect the semantic equivalence of the two answers.",
      "response_time": 23.892684936523438,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        505,
        918,
        831
      ],
      "language": "bangla"
    },
    {
      "test_id": "negative_017",
      "question": "What is a heap?",
      "expected_answer": "A heap is a tree-based data structure that satisfies the heap property where parent nodes have higher or lower values than children.",
      "actual_answer": "A heap is a specialized tree-based data structure used in computer science, particularly in algorithms for sorting and computational geometry. In a max-heap, the value at each parent node is greater than or equal to the values of its children. In a min-heap, the value at each parent node is less than or equal to the values of its children. Heapsort is a sorting algorithm that uses heaps to efficiently sort data.",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers address the same key concepts, including the definition of a heap as a tree-based data structure and its properties. The factual information is consistent, with both answers mentioning the heap property and its application in sorting algorithms. Although the wording differs slightly, the semantic equivalence is maintained across both answers. The Actual Answer provides additional context and examples, but the core information and meaning are preserved.",
      "response_time": 18.741612672805786,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        171,
        171,
        173
      ],
      "language": "english"
    },
    {
      "test_id": "negative_018",
      "question": "আমি কিভাবে নিজে নিজে গিটার রিপেয়ার করতে শিখব?",
      "expected_answer": "কোন প্রাসঙ্গিক তথ্য পাওয়া যায়নি",
      "actual_answer": "কোন প্রাসঙ্গিক তথ্য পাওয়া যায়নি",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "The expected answer and the actual answer do not convey the same core information and meaning. The expected answer is a direct response to the question, stating that no relevant information was found. In contrast, the actual answer provides a lengthy explanation about Hamiltonian cycles in graphs and their relation to polynomial time complexity, which is unrelated to the question. The actual answer seems to be a response to a different question or topic altogether.",
      "response_time": 23.94391179084778,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        387,
        1083,
        379
      ],
      "language": "bangla"
    },
    {
      "test_id": "negative_019",
      "question": "What is depth-first search?",
      "expected_answer": "Depth-first search is a graph traversal algorithm that explores as far as possible along each branch before backtracking.",
      "actual_answer": "Depth-first search (DFS) is an algorithm for traversing or searching tree or graph data structures. The algorithm explores as far as possible along each path before backtracking, thus the name \"depth-first\". It is a common method used in graph theory and computer science for problems like determining if a graph is connected, finding a cycle in a graph, or finding the shortest path between two nodes. Additionally, DFS timestamps each vertex during its traversal. The most basic property of DFS is that it forms a forest of trees in the graph, where each tree represents a path explored during the search.",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers convey the same core information and meaning, addressing key concepts such as the definition of Depth-First Search (DFS), its algorithmic behavior, and its applications in graph theory and computer science. While the actual answer provides more detailed information, including additional properties and examples, the essence of both answers is equivalent. The minor differences in wording are acceptable, and the cross-language consistency is maintained.",
      "response_time": 20.75407099723816,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        623,
        623,
        626
      ],
      "language": "english"
    },
    {
      "test_id": "negative_020",
      "question": "লুপ কি?",
      "expected_answer": "লুপ হল একটি প্রোগ্রামিং কন্সট্রাক্ট যা কোড বারবার এক্সিকিউট করে।",
      "actual_answer": "Loop হচ্ছে একটা প্রোগ্রামিং কন্সট্রাক্ট, যা একটা কাজ বা প্রক্রিয়া বিভিন্ন সংখ্যা বা ইটারেশন করতে হলে ব্যবহার করা যায়। একটা লুপ একটা কাজ বা প্রক্রিয়া একটা সংখ্যা বা ইটারেশন",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers address the same key concepts, specifically defining a \"loop\" as a programming construct that executes code repeatedly. The factual information is consistent, and minor differences in wording are present due to language translation. The semantic equivalence is maintained across both answers, conveying the same meaning despite being in different languages (English/Bangla).",
      "response_time": 23.536741495132446,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        38,
        39,
        338
      ],
      "language": "bangla"
    },
    {
      "test_id": "negative_021",
      "question": "What is a loop in programming?",
      "expected_answer": "A loop is a control structure that repeats a block of code multiple times until a condition is met.",
      "actual_answer": "A loop in programming is a control structure that allows a section of code to be repeated multiple times until a certain condition is met. This is useful for performing repetitive tasks, such as iterating through arrays or collections, or repeating an action a specific number of times. Loop invariants are properties that remain true before, during, and after the execution of a loop, and they can be used to help prove that an algorithm is correct.",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers address the same key concepts of a loop in programming, including its purpose, condition for termination, and utility in repetitive tasks. The factual information is consistent, with minor differences in wording that do not affect the semantic equivalence of the two answers. The Actual Answer provides additional information on loop invariants, but this is a related concept that does not change the core meaning of the definition. The cross-language consistency is also maintained, as both answers convey the same meaning in English and Bangla.",
      "response_time": 19.09805393218994,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        38,
        40,
        39
      ],
      "language": "english"
    },
    {
      "test_id": "negative_022",
      "question": "If-else statement কি?",
      "expected_answer": "If-else statement হল একটি কন্ডিশনাল স্ট্যাটমেন্ট যা শর্ত অনুযায়ী কোড এক্সিকিউট করে।",
      "actual_answer": "If-else statement (ইফ-এল্স স্টেটমেন্ট) একটা প্রোগ্রামিং লেজ কঠিনতার উদাহরণ, যা একটা লোগিকাল কন্ডিশন (লগিকাল এক্সপ্রেশন) ও একটা কাজ (স্টেমেন্ট) এর দু'টি উপাদান করে, যেমন-\n```\nif (condition) {\n    // statement if condition",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers convey the same core information and meaning, addressing the key concept of an \"if-else statement\" as a conditional statement that executes code based on a condition. The factual information is consistent, and minor differences in wording are acceptable. The Actual Answer provides a more detailed explanation, but the Essential meaning remains the same.",
      "response_time": 22.83846354484558,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        1215,
        891,
        74
      ],
      "language": "bangla"
    },
    {
      "test_id": "negative_023",
      "question": "What is a function in programming?",
      "expected_answer": "A function is a reusable block of code that performs a specific task and can be called multiple times.",
      "actual_answer": "In programming, a function is a self-contained piece of code that performs a specific task. It takes one or more input arguments, processes them according to a defined set of instructions, and returns an output value. Functions help organize code, make it more reusable, and reduce redundancy. They can be defined in various programming languages using keywords such as \"function\", \"def\", or \"sub\". The concept of a function is closely related to mathematical functions, but in programming, functions can also perform operations that are not directly related to mathematics.",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers address the same key concepts, including the definition of a function, its purpose, and its characteristics. The factual information is consistent, and minor differences in wording are acceptable for semantic equivalence. The actual answer provides more detailed information, but the core meaning and key concepts are the same as the expected answer.",
      "response_time": 19.82140827178955,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        1015,
        1187,
        73
      ],
      "language": "english"
    },
    {
      "test_id": "negative_024",
      "question": "BFS কি?",
      "expected_answer": "BFS বা Breadth-First Search হল একটি গ্রাফ ট্রাভার্সাল অ্যালগরিদম যা স্তর অনুসারে ট্রাভার্স করে।",
      "actual_answer": "Breadth-First Search (BFS) is a graph traversal algorithm that starts at the root node (or some arbitrary node) and explores all of the nodes in the graph by traversing along the edges from node to node, always expanding nodes at the current depth before moving on to deeper nodes. It uses a queue to keep track of the nodes to be visited next. The main goal of BFS is to find the shortest path from a root node to all other reachable nodes in the graph.\n\nIn the given context, the BFS procedure is described as follows:\n\n1. Initialize a queue Q and a set V (representing the set of visited nodes) to be empty. Add the root node to the queue and mark it as visited.\n2. While the queue is not empty, dequeue a node u from Q and set its",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers describe the Breadth-First Search (BFS) algorithm, its purpose, and its procedure. The key concepts addressed in both answers are consistent, including the algorithm's traversal method, use of a queue, and goal of finding the shortest path. Although the wording and sentence structure differ between the two answers, they convey the same meaning and semantic equivalence is maintained. The minor differences in wording are acceptable, and the cross-language consistency is also preserved.",
      "response_time": 23.70331645011902,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        619,
        615,
        615
      ],
      "language": "bangla"
    },
    {
      "test_id": "negative_025",
      "question": "What is a parameter in a function?",
      "expected_answer": "A parameter is a variable in a function definition that accepts values passed when the function is called.",
      "actual_answer": "In the context provided, a parameter is not explicitly mentioned. However, in general terms, parameters are the inputs or values that a function takes to perform its operations. For example, in the function f(x) = 2x + 3, x is the parameter. In the context of big O notation (mentioned in [67] 3.1 Asymptotic notation), variables that tend to infinity are often referred to as parameters, but this is not explicitly stated in the provided context.",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers address the same key concept of a parameter in a function definition, and the factual information is consistent. The actual answer provides additional context and examples, but the core meaning remains the same. The minor differences in wording are acceptable, and the cross-language consistency is maintained.",
      "response_time": 19.442459106445312,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        1015,
        41,
        67
      ],
      "language": "english"
    },
    {
      "test_id": "negative_026",
      "question": "Return statement কি?",
      "expected_answer": "Return statement একটি ফাংশন থেকে মান ফেরত পাঠায়।",
      "actual_answer": "The return statement for the given code snippet is not explicitly provided in the context you've shared. However, based on the structure of the code and the mathematical explanation provided, it seems like the intention is to calculate a value for a variable, but the actual implementation of the return statement is missing.\n\nIf we were to assume a hypothetical function that calculates the sum of an arithmetic series, the return statement could look something like this:\n\n```python\ndef arithmetic_series(n, C1, C2):\n    # ... (the given code snippet)\n    return (n * (C1 + n * C2)) / 2\n```\n\nThis function calculates the sum of an arithmetic series with the given parameters and returns the result. However, this is just a hypothet",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers address the same key concepts, including the purpose of a return statement in a function and its application to calculate a value. The factual information is consistent, and minor differences in wording are acceptable. The actual answer provides additional context and explanation, but the core meaning and information conveyed are equivalent to the expected answer. The cross-language consistency is also maintained, as both answers convey the same meaning in different languages (English/Bangla).",
      "response_time": 22.987671852111816,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        61,
        844,
        1166
      ],
      "language": "bangla"
    },
    {
      "test_id": "negative_027",
      "question": "What is a string data type?",
      "expected_answer": "A string is a data type that represents text as a sequence of characters.",
      "actual_answer": "A string data type is a sequence of characters, which can be a set of symbols from a specific alphabet. In computer programming, it's used to represent text or sequences of characters. Strings can be manipulated using various operations such as concatenation, substring extraction, and character replacement.",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers address the same key concepts, including the definition of a string as a sequence of characters, its representation of text, and its manipulability through various operations. The factual information is consistent, and minor differences in wording are acceptable for semantic equivalence. Although the answers are in different languages (English and implied English), they convey the same meaning, demonstrating cross-language consistency.",
      "response_time": 17.399885892868042,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        248,
        249,
        1077
      ],
      "language": "english"
    },
    {
      "test_id": "negative_028",
      "question": "Integer data type কি?",
      "expected_answer": "Integer data type হল একটি ডেটা টাইপ যা পূর্ণ সংখ্যা সংরক্ষণ করে।",
      "actual_answer": "The integer data type in computer science refers to a set of numbers that can be counted or measured, which includes all whole numbers from negative infinity to positive infinity. In programming, integers are represented as a sequence of digits without a decimal point. For example, the numbers 0, 1, -1, 42, and -42 are all integers.",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers address the same key concepts, specifically defining integer data type and its characteristics. The factual information is consistent, with the expected answer providing a concise definition in Bangla and the actual answer providing a more detailed explanation in English. While there are minor differences in wording, the semantic equivalence is maintained, conveying the same meaning across languages.",
      "response_time": 18.281620025634766,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        283,
        1118,
        1077
      ],
      "language": "bangla"
    },
    {
      "test_id": "negative_029",
      "question": "What is a binary search tree?",
      "expected_answer": "A binary search tree is a binary tree where left child nodes are smaller and right child nodes are larger than the parent.",
      "actual_answer": "A binary search tree (BST) is a data structure that organizes data in nodes such that each node has at most two children, referred to as the left child and the right child. In a binary search tree, the key (or value) of the left child is always less than the parent node, and the key of the right child is always greater than or equal to the parent node. This property allows for efficient search, insertion, and deletion operations.\n\nThe given example (a) shows a binary search tree with the following keys: 2, 5, 6, 7, and 8. In this tree, the root node is 6, the left child of 6 is 5, and the right child of 6 is 8. The left child of 5 is 2, and the left child of 8 is",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers address the same key concepts, including the definition of a binary search tree, its properties (left child smaller than parent, right child larger than or equal to parent), and an example. The factual information is consistent, and minor differences in wording are acceptable for semantic equivalence. The actual answer provides more detailed explanation, but the core meaning and information conveyed are the same as the expected answer.",
      "response_time": 23.58508324623108,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        307,
        306,
        309
      ],
      "language": "english"
    },
    {
      "test_id": "negative_030",
      "question": "Binary Search Tree কি?",
      "expected_answer": "Binary Search Tree হল একটি বাইনারি ট্রি যেখানে বাম চাইল্ড ছোট এবং ডান চাইল্ড বড় হয়।",
      "actual_answer": "Binary Search Tree (বিনারি সেখায় ত্রে) একটা ডাটা স্ট্রাকচার যা একটা বিনারি ত্রে (binary tree) থেকে আসে। এই ত্রে একটা নয়ন করে যাতে একটা নোড (node) এর ছেলে (left child) এবং বৃত্তিকার (right child) এর মান সেই নোড এর ফুল",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers convey the same core information and meaning, addressing the key concept of a Binary Search Tree (BST) and its characteristics. The expected answer provides a concise definition in Bangla, while the actual answer provides a more detailed explanation in Bangla, but both describe the structure and properties of a BST. The minor differences in wording are acceptable, and the factual information is consistent across both answers.",
      "response_time": 22.595377445220947,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        307,
        310,
        1274
      ],
      "language": "bangla"
    },
    {
      "test_id": "negative_031",
      "question": "What is a boolean data type?",
      "expected_answer": "A boolean is a data type that has only two values: true or false.",
      "actual_answer": "A boolean data type represents values that can be either true or false, often denoted as 1 (true) and 0 (false) in computer science. It is commonly used to represent logical values in programming languages.",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers address the same key concepts, specifically the definition and representation of a boolean data type. The factual information is consistent, with both answers stating that a boolean has only two values: true or false. Although the wording and examples differ slightly, they convey the same meaning and are semantically equivalent. The Actual Answer provides additional context about the representation of boolean values in computer science, but this does not change the core information conveyed.",
      "response_time": 16.823253631591797,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        1090,
        1099,
        1090
      ],
      "language": "english"
    },
    {
      "test_id": "negative_032",
      "question": "Float data type কি?",
      "expected_answer": "Float data type হল একটি ডেটা টাইপ যা দশমিক সংখ্যা সংরক্ষণ করে।",
      "actual_answer": "Float data type refers to a type of numerical data that represents real numbers in a computer. It is used to store fractional, positive, and negative numbers with decimal points. The term \"float\" stands for \"floating point,\" which means the decimal point can be placed anywhere in the number. In programming, float data types are often used to perform mathematical calculations involving real numbers.",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers convey the same core information and meaning, addressing the key concepts of float data type, its purpose, and how it represents real numbers with decimal points. The factual information is consistent, and minor differences in wording are acceptable for semantic equivalence. Although the answers are in different languages (English/Bangla), they convey the same meaning, demonstrating cross-language consistency.",
      "response_time": 18.330864906311035,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        1077,
        1285,
        272
      ],
      "language": "bangla"
    },
    {
      "test_id": "negative_033",
      "question": "What is an operator in programming?",
      "expected_answer": "An operator is a symbol that performs operations on variables and values, such as addition or comparison.",
      "actual_answer": "An operator in programming is a symbol or set of symbols that tells a computer to perform a specific mathematical, logical, or assignment operation on one or more values or variables. Operators can be categorized into arithmetic operators (such as +, -, *, /), comparison operators (such as ==, !=, >, <), logical operators (such as &&, ||, !), and assignment operators (such as =). The role of operators is crucial in programming as they allow for the manipulation and transformation of data to achieve desired results.",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers address the same key concepts, including the definition of an operator, types of operators (arithmetic, comparison, logical, assignment), and their role in programming. The factual information is consistent, and minor differences in wording are present due to the rephrasing of the definition. The actual answer provides more detail and categorization, but the core meaning and information conveyed are equivalent in both answers.",
      "response_time": 19.791487216949463,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        25,
        42,
        866
      ],
      "language": "english"
    },
    {
      "test_id": "negative_034",
      "question": "Class কি?",
      "expected_answer": "Class হল একটি ব্লুপ্রিন্ট যা অবজেক্ট তৈরির জন্য ব্যবহৃত হয়।",
      "actual_answer": "Class বা ক্লাস হল object-oriented programming এ একটি blueprint বা খসড়া যা object তৈরির জন্য ব্যবহৃত হয়। এটি data (attributes বা properties) এবং methods (functions) এর একটি template যা define করে কিভাবে object গুলো তৈরি হবে এবং কিভাবে কাজ করবে।",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Class is a blueprint used to create objects with data and methods.",
      "response_time": 23.980424404144287,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        1084,
        1184,
        1084
      ],
      "language": "bangla"
    },
    {
      "test_id": "negative_035",
      "question": "What is an object in programming?",
      "expected_answer": "An object is an instance of a class that contains data and methods.",
      "actual_answer": "In programming, an object is a instance of a class that has its own state (attributes or properties) and behavior (methods). Objects are used to represent real-world entities, such as a car, a person, or a document, and they encapsulate data and functions that operate on that data. The state of an object can be modified by changing its attributes, and the behavior of an object can be modified by overriding or adding methods to the class it belongs to.",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers address the same key concepts, including the definition of an object as an instance of a class, its state (attributes or properties), and behavior (methods). The factual information is consistent, and minor differences in wording are acceptable. The actual answer provides more detailed explanations, but the core meaning and concepts are identical to the expected answer.",
      "response_time": 19.059738874435425,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        41,
        262,
        249
      ],
      "language": "english"
    },
    {
      "test_id": "negative_036",
      "question": "Method কি?",
      "expected_answer": "Method হল একটি ক্লাসের ভিতরে থাকা ফাংশন যা অবজেক্টের উপর কাজ করে।",
      "actual_answer": "Method বা মেথড হল একটি class এর ভিতরে define করা function যা object এর উপর কাজ করে। এটি object এর data এবং behavior কে manipulate করার জন্য ব্যবহৃত হয়। Method call করার সময় object এর state পরিবর্তন হতে পারে অথবা কোন value return করতে পারে।",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Method is a function inside a class that operates on objects.",
      "response_time": 15.707978248596191,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        133,
        1272,
        1303
      ],
      "language": "bangla"
    },
    {
      "test_id": "negative_037",
      "question": "What is inheritance in OOP?",
      "expected_answer": "Inheritance is a mechanism where a new class inherits properties and methods from an existing class.",
      "actual_answer": "In Object-Oriented Programming (OOP), inheritance is a mechanism that allows one class (the derived or subclass) to acquire the properties and behaviors of another class (the base or superclass). The derived class inherits attributes and methods from the base class, and can also add new attributes and override existing methods. This allows for code reuse and hierarchical classifications. However, the context provided does not directly relate to inheritance in OOP.",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Both answers convey the same core information and meaning, addressing the key concepts of inheritance in Object-Oriented Programming (OOP), including properties, behaviors, attributes, methods, code reuse, and hierarchical classifications. The minor differences in wording are acceptable, and the factual information is consistent across both answers.",
      "response_time": 19.10108470916748,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        879,
        41,
        756
      ],
      "language": "english"
    },
    {
      "test_id": "negative_038",
      "question": "Encapsulation কি?",
      "expected_answer": "Encapsulation হল ডেটা এবং মেথডকে একসাথে বান্ডল করা এবং বাইরের জন্য লুকিয়ে রাখা।",
      "actual_answer": "Encapsulation বা এনক্যাপসুলেশন হল object-oriented programming এর একটি মূল নীতি যেখানে data (variables) এবং methods (functions) কে একসাথে একটি class এ bundle করা হয় এবং implementation details বাইরের জগত থেকে লুকিয়ে রাখা হয়। এটি data privacy এবং code modularity নিশ্চিত করে এবং শুধুমাত্র well-defined interface এর মাধ্যমে data access করতে দেয়।",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Encapsulation bundles data and methods together while hiding implementation details from outside.",
      "response_time": 18.10258674621582,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        336,
        446,
        390
      ],
      "language": "bangla"
    },
    {
      "test_id": "negative_039",
      "question": "How should I train my dog to follow basic commands?",
      "expected_answer": "No relevant information found",
      "actual_answer": "No relevant information found. This question is about animal training and pet behavior, which falls outside the domain of our computer science and algorithms knowledge base. Our system focuses on computational topics such as algorithm design, data structures, software engineering principles, complexity analysis, and related computer science concepts, not pet training or animal behavior.",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "Dog training question is off-topic for computer science - no relevant information available.",
      "response_time": 22.741193771362305,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        706,
        379,
        44
      ],
      "language": "english"
    },
    {
      "test_id": "negative_040",
      "question": "আমি কিভাবে বিড়ালকে লিটার বক্স ব্যবহার করতে শেখাবো?",
      "expected_answer": "কোন প্রাসঙ্গিক তথ্য পাওয়া যায়নি",
      "actual_answer": "কোন প্রাসঙ্গিক তথ্য পাওয়া যায়নি",
      "success": true,
      "equivalent": true,
      "confidence": "HIGH",
      "explanation": "The actual answer and expected answer are same.",
      "response_time": 23.758380651474,
      "sources_found": 3,
      "model_used": "mistral",
      "source_pages": [
        153,
        446,
        434
      ],
      "language": "bangla"
    }
  ]
}